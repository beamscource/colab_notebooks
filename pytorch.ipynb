{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJ1xajqPGagHCHX5lsYX9Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beamscource/colab_notebooks/blob/main/pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary based on Programming PyTorch, NLP with PyTorch and https://deeplearning.cs.cmu.edu/F22/index.html."
      ],
      "metadata": {
        "id": "EugY5JiWDFQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional resources at https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial2/Introduction_to_PyTorch.html"
      ],
      "metadata": {
        "id": "D-avl5OESxGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "v5HyjCAhV6jT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch is a Python library which offers an **eager approach to differentiation** instead of defining static graphs, allowing for greater flexibility in the way networks are created, trained, and operated.\n",
        "\n",
        "Similar to DyNet and Chainer, and in contrast to static frameworks like TensorFlow/Theano/Caffe, models are not compiled before execution. \n",
        "\n",
        "PyTorch has two lineages. First, it derives many features and concepts from Torch, which was a Lua-based neural network library that dates back to 2002. Its other major parent is Chainer, created in Japan in 2015.\n",
        "\n",
        "The library also comes with modules that help with manipulating text, images, and audio (*torchtext*, *torchvision*, and *torchaudio*), along with built-in variants of popular architectures such as ResNet (with weights that can be downloaded to provide assistance with *transfer learning*).\n",
        "\n",
        "In 2022, about 85% of pre-trained models on HuggingFace are PyTorch models (https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2022/). Despite the fact that PyTorch is used by companies like Twitter, Salesforce, Tesla, Uber, and NVIDIA, the consensus seems to be that TF still offers better native deployment capabilities and that tf.keras might be better suited for a complete beginner."
      ],
      "metadata": {
        "id": "a-8pyuqhWl-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All code examples can be found at https://github.com/falloutdurham/beginners-pytorch-deep-learning For more infos and tutorials see https://pytorch.org/hub/"
      ],
      "metadata": {
        "id": "a2MaJ7GgWLft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensors and Matrix Algebra"
      ],
      "metadata": {
        "id": "iS_MyDcDREj8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensors are objects (\"multidimensional arrays\" or matrices) which hold numerical data of a single type used to propagate through the network. For example, a 1st-order tensor is a vector (one dimensional array) and 2nd-order tensor is a matrix. If you are coming from Matlab, this feels very familiar."
      ],
      "metadata": {
        "id": "wdU3bd-99zfb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iagD2RKaQzBM"
      },
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a tensor from Python lists\n",
        "x = torch.tensor([[0,0,1],[1,1,1],[0,0,0]])\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Li2WqHJND70Q",
        "outputId": "406c0f69-5ff6-42f3-f972-c2c4f9dcb6b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 1],\n",
              "        [1, 1, 1],\n",
              "        [0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.type()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-UpzAArLEMjh",
        "outputId": "ec3ff42f-7795-4518-fefe-44502ee9483c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'torch.FloatTensor'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# investigating the size of a tensor\n",
        "x.shape"
      ],
      "metadata": {
        "id": "8DZEpme-EMyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# or\n",
        "x.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkdDfCqkLY76",
        "outputId": "5f1fab8b-e269-46c5-94c0-11b815859e68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYmwWhUbRYP6"
      },
      "source": [
        "# helper function to investigate a tensor\n",
        "def describe(x):\n",
        "  print(f\"Type: {x.type()}\")\n",
        "  print(f\"Shape/size: {x.shape}\")\n",
        "  print(f\"Values: \\n{x}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO-u9AEyR6KC"
      },
      "source": [
        "describe(torch.Tensor(2, 3))\n",
        "describe(torch.rand(2, 3))   # uniform randomdescribe\n",
        "describe(torch.randn(2, 3))  # random normal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating tensors filled with ones or zeros (don't have tensor keyword)\n",
        "describe(torch.zeros(2, 3))\n",
        "describe(torch.ones(2, 3))"
      ],
      "metadata": {
        "id": "gfoX9AbSFkEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a tensor from NumPy array\n",
        "# the type of the created tensor is DoubleTensor which corresponds to NumPy\n",
        "# float64 matrix\n",
        "npy  =  np.random.rand(2,  3)\n",
        "describe(torch.from_numpy(npy))\n",
        "# or\n",
        "describe(torch.as_tensor(npy))\n",
        "npy"
      ],
      "metadata": {
        "id": "XZ8Vl4ozSI0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.tensor() always copies data. If you have a numpy array and want to avoid a copy, use torch.as_tensor()."
      ],
      "metadata": {
        "id": "2FTnuyj9L29m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Different types*\n",
        "\n",
        "The default  tensor  type  when  using  torch.Tensor  constructor  is  a torch.FloatTensor. But it's possible to convert it to float,  long,  double format  by  specifying  it  at the initialization  or  using  one  of  the typecasting  methods.\n",
        "\n",
        "See more infos at https://pytorch.org/docs/stable/tensors.html"
      ],
      "metadata": {
        "id": "jIuwscxYG3kA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using dtype at initialization\n",
        "torch.zeros([2, 4], dtype=torch.int32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghbEhd3QILqr",
        "outputId": "3c3a6766-fbb9-49ba-9263-9754290bd451"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0, 0],\n",
              "        [0, 0, 0, 0]], dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calling a specific constructor at initialization\n",
        "x = torch.FloatTensor([[1, 2, 3],\n",
        "                   [4,5,6]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9IulhcRKED0",
        "outputId": "24a5ad91-eb56-4986-83f4-9e19a4ab3da4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 2., 3.],\n",
              "        [4., 5., 6.]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# taypecasting\n",
        "x.long()"
      ],
      "metadata": {
        "id": "huSO6702K8O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Indexing and slicing*"
      ],
      "metadata": {
        "id": "nnLG-dwwMe6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a tensor with a short-cut\n",
        "x = torch.arange(6).view(2, 3)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuHqSI-SM7H2",
        "outputId": "b8204beb-c502-4190-c273-baae894d309e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 1, 2],\n",
              "        [3, 4, 5]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# indexing into a tensor works like in hierarchical lists (standard Python)\n",
        "x[0][1:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUxGfoVtFOu9",
        "outputId": "69170ffb-f73d-47c3-8e66-dfb92331ed82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# but also like in NumPy\n",
        "x[0,0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ec7JX0eNzzV",
        "outputId": "a477a336-ea56-4909-ad4e-e0abdb8cca5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# access the first two elements in the first row (indexing is starting at zero)\n",
        "# take from the row at index zero all elements until the element at index 2\n",
        "x[0, :2] "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtjQDmxVOxEc",
        "outputId": "2b8fbdde-a0ae-4cbd-fda0-9736f782dcba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[1,1:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DKMd9UaPPew",
        "outputId": "d874dd0e-2bf9-4e3f-b9d9-4fe1349b2828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# access scalar values from a single-element tensor\n",
        "torch.rand(1).item()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15Sbnb1OGSry",
        "outputId": "5a9b861a-f7c3-4986-f96c-25172fe868a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5620666146278381"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# replace all elements of a tensor\n",
        "x = torch.ones(4,8)\n",
        "x.fill_(5)\n",
        "x"
      ],
      "metadata": {
        "id": "A0-xiwgpEb9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Any PyTorch method with an underscore (_) refers to an inplace operation; that is, it modifies the content in place without creating a new object."
      ],
      "metadata": {
        "id": "HXLYLnOJJMYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# indexing using PyTorch functions\n",
        "# indices have to be of the type LongTensor\n",
        "print(x)\n",
        "indices = torch.LongTensor([0, 0])\n",
        "# joining first row into a new tensor\n",
        "describe(torch.index_select(x, dim=0, index=indices))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8GoGcEkYEhh",
        "outputId": "608ab873-8482-4656-e9bc-785957381b63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00,        nan, 4.7399e+16],\n",
            "        [4.4721e+21, 1.5956e+25, 4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00]])\n",
            "Type: torch.FloatTensor\n",
            "Shape/size: torch.Size([2, 6])\n",
            "Values: \n",
            "tensor([[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00,        nan, 4.7399e+16],\n",
            "        [1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00,        nan, 4.7399e+16]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting non-contogous elements by passing tensors as indices\n",
        "print(x)\n",
        "row_indices = torch.arange(2).long() # take from rows zero and one\n",
        "col_indices = torch.LongTensor([0, 1]) # take from colums zero and one\n",
        "print(row_indices)\n",
        "print(col_indices)\n",
        "describe(x[row_indices, col_indices])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI3aLn0TZIWF",
        "outputId": "18baccf5-37c1-4ba1-f2dc-6834b3b08642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00,        nan, 4.7399e+16],\n",
            "        [4.4721e+21, 1.5956e+25, 4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00]])\n",
            "tensor([0, 1])\n",
            "tensor([0, 1])\n",
            "Type: torch.FloatTensor\n",
            "Shape/size: torch.Size([2])\n",
            "Values: \n",
            "tensor([1.4068e-34, 1.5956e+25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Concatenating*"
      ],
      "metadata": {
        "id": "heXpDimsanKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# on columns\n",
        "print(x)\n",
        "describe(torch.cat([x, x], dim=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGWGBaYvau5j",
        "outputId": "67adf324-b0cc-47c5-ec8c-4669fd0c0886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00,        nan, 4.7399e+16],\n",
            "        [4.4721e+21, 1.5956e+25, 4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00]])\n",
            "Type: torch.FloatTensor\n",
            "Shape/size: torch.Size([4, 6])\n",
            "Values: \n",
            "tensor([[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00,        nan, 4.7399e+16],\n",
            "        [4.4721e+21, 1.5956e+25, 4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00],\n",
            "        [1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00,        nan, 4.7399e+16],\n",
            "        [4.4721e+21, 1.5956e+25, 4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# on rows\n",
        "print(y)\n",
        "describe(torch.cat([y, y], dim=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVghejRUbBFk",
        "outputId": "497e9fb2-2c44-4b59-f47c-dd59dcef03bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00],\n",
            "        [       nan, 4.7399e+16, 4.4721e+21, 1.5956e+25],\n",
            "        [4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00]])\n",
            "Type: torch.FloatTensor\n",
            "Shape/size: torch.Size([3, 8])\n",
            "Values: \n",
            "tensor([[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00, 1.4068e-34, 0.0000e+00,\n",
            "         3.3631e-44, 0.0000e+00],\n",
            "        [       nan, 4.7399e+16, 4.4721e+21, 1.5956e+25,        nan, 4.7399e+16,\n",
            "         4.4721e+21, 1.5956e+25],\n",
            "        [4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00, 4.7399e+16, 2.3868e-06,\n",
            "         1.4838e-41, 0.0000e+00]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to keep tensors as separated elements\n",
        "print(y)\n",
        "describe(torch.stack([y, y], dim=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-pOTutKbUDa",
        "outputId": "05083fdd-7312-40fe-cfa3-9ef10ae7228f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00],\n",
            "        [       nan, 4.7399e+16, 4.4721e+21, 1.5956e+25],\n",
            "        [4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00]])\n",
            "Type: torch.FloatTensor\n",
            "Shape/size: torch.Size([2, 3, 4])\n",
            "Values: \n",
            "tensor([[[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00],\n",
            "         [       nan, 4.7399e+16, 4.4721e+21, 1.5956e+25],\n",
            "         [4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00]],\n",
            "\n",
            "        [[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00],\n",
            "         [       nan, 4.7399e+16, 4.4721e+21, 1.5956e+25],\n",
            "         [4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)\n",
        "describe(torch.stack([y, y], dim=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GSuDfcab1Q8",
        "outputId": "9575e133-bb23-4497-aa66-dcb6cee1960d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00],\n",
            "        [       nan, 4.7399e+16, 4.4721e+21, 1.5956e+25],\n",
            "        [4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00]])\n",
            "Type: torch.FloatTensor\n",
            "Shape/size: torch.Size([3, 2, 4])\n",
            "Values: \n",
            "tensor([[[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00],\n",
            "         [1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00]],\n",
            "\n",
            "        [[       nan, 4.7399e+16, 4.4721e+21, 1.5956e+25],\n",
            "         [       nan, 4.7399e+16, 4.4721e+21, 1.5956e+25]],\n",
            "\n",
            "        [[4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00],\n",
            "         [4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Manipulating tensors' dimensions*"
      ],
      "metadata": {
        "id": "ia4HyJ5zUYJC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X44T7ZrxicpS",
        "outputId": "78290576-aaea-4a3f-da96-96b80b910886"
      },
      "source": [
        "# change dimensions of the tensor\n",
        "x = torch.Tensor(2,6)\n",
        "print(x)\n",
        "# view is not changing the original tensor\n",
        "# you have to assign it to a new tensor\n",
        "x.view(3, 4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.4062e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00,        nan, 1.5975e-43],\n",
            "        [4.4721e+21, 1.5956e+25, 4.7399e+16, 2.3868e-06, 1.4838e-41, 1.5695e-43]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.4062e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00],\n",
              "        [       nan, 1.5975e-43, 4.4721e+21, 1.5956e+25],\n",
              "        [4.7399e+16, 2.3868e-06, 1.4838e-41, 1.5695e-43]])"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)\n",
        "y = x.view(3, 4)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlaMJ32mUyqY",
        "outputId": "159544fe-bfa5-4222-a516-8bd8c8f316dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00,        nan, 4.7399e+16],\n",
            "        [4.4721e+21, 1.5956e+25, 4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00]])\n",
            "tensor([[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00],\n",
            "        [       nan, 4.7399e+16, 4.4721e+21, 1.5956e+25],\n",
            "        [4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# or if you want to operate on non-contigous tensors\n",
        "print(x)\n",
        "y = x.reshape(3, 4)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7ymH2Kok74I",
        "outputId": "61766dfb-98e4-4914-fb6d-d1b6bd906374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.4062e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00,        nan, 1.5975e-43],\n",
            "        [4.4721e+21, 1.5956e+25, 4.7399e+16, 2.3868e-06, 1.4838e-41, 1.5695e-43]])\n",
            "tensor([[1.4062e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00,        nan, 1.5975e-43],\n",
            "        [4.4721e+21, 1.5956e+25, 4.7399e+16, 2.3868e-06, 1.4838e-41, 1.5695e-43]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iH9FdmzfkADq",
        "outputId": "65a50b4f-55db-459f-8ae5-2ee3d41f88c6"
      },
      "source": [
        "# transposing a tensor (columns become rows)\n",
        "torch.transpose(y, 0, 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.4068e-34,        nan, 4.7399e+16],\n",
              "        [0.0000e+00, 4.7399e+16, 2.3868e-06],\n",
              "        [3.3631e-44, 4.4721e+21, 1.4838e-41],\n",
              "        [0.0000e+00, 1.5956e+25, 0.0000e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# re-arrange dimensions of a tensor\n",
        "x = torch.rand(640, 480, 3)\n",
        "y = x.permute(2,0,1)\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEPbXNgPmQrf",
        "outputId": "bc76ad37-34e2-496f-9da4-cc556fd058e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 640, 480])"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Operations on tensors*"
      ],
      "metadata": {
        "id": "HlLgKe9_To_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# element-wise additon with mathematical symbols\n",
        "torch.ones(1,2) + torch.ones(1,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0iRmesUFwiv",
        "outputId": "0543a393-9fc1-4541-dafe-3b2645e58976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2., 2.]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# or with built-in methods\n",
        "torch.add(torch.ones(1,2), torch.ones(1,2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1BJmrv9T1cs",
        "outputId": "3970b32b-a010-4fab-bb39-d0619be0cedf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2., 2.]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# summing alog the colums\n",
        "print(y)\n",
        "describe(torch.sum(y, dim=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "li9wj5SlUKFa",
        "outputId": "b152593b-9ac4-4cba-eed8-a4acea5c2921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00],\n",
            "        [       nan, 4.7399e+16, 4.4721e+21, 1.5956e+25],\n",
            "        [4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00]])\n",
            "Type: torch.FloatTensor\n",
            "Shape/size: torch.Size([4])\n",
            "Values: \n",
            "tensor([       nan, 4.7399e+16, 4.4721e+21, 1.5956e+25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# or rows\n",
        "describe(torch.sum(y, dim=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ug9viDYVheC",
        "outputId": "a2e2846b-c6ac-4d9f-b616-7e29a6d1308d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type: torch.FloatTensor\n",
            "Shape/size: torch.Size([3])\n",
            "Values: \n",
            "tensor([1.4068e-34,        nan, 4.7399e+16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# matrix multiplication\n",
        "describe(torch.mm())"
      ],
      "metadata": {
        "id": "me3OuZ88cRVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# access the max value\n",
        "x.max().item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAgM44ZpiU4G",
        "outputId": "47c7c1f8-648d-484f-dccd-1c718e3e5c7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Broadcasting*"
      ],
      "metadata": {
        "id": "FZJ_UZfdm2yq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Borrowed from NumPy, broadcasting allows to perform operations between a tensor and a smaller tensor. You can broadcast across two tensors if, starting backward from their trailing dimensions: \n",
        "- the two dimensions are equal\n",
        "- one of the dimensions is 1"
      ],
      "metadata": {
        "id": "N3QJP7a_nGRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*GPU vs CPU tensors*"
      ],
      "metadata": {
        "id": "OcobxzOsgmMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, PyTorch tensors are created to be used by a CPU."
      ],
      "metadata": {
        "id": "PyfwFzjfg46b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cpu_tensor = torch.rand(2)\n",
        "cpu_tensor.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOjCCW_JhTMI",
        "outputId": "82319306-593a-41f0-ac5f-493e08da35c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When doing linear algebra operations it make sense to utilize a GPU. To use a GPU, you need to first allocate the tensor on the GPU’s memory. Access to GPUs is provided via CUDA API that was created by NVIDIA and is limited to use only NVIDIA GPUs."
      ],
      "metadata": {
        "id": "gEZ77o7Jhtu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# in colab you need to change runtime environment for this to work\n",
        "# transfer a tensor to a GPU\n",
        "gpu_tensor = cpu_tensor.to(\"cuda\")\n",
        "gpu_tensor.device"
      ],
      "metadata": {
        "id": "5Z1syqE6hu_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be device agnostic and write code that works whether it’s on the GPU or the CPU:"
      ],
      "metadata": {
        "id": "ctVdpI5Uiynf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "x = torch.rand(3, 3).to(device)\n",
        "describe(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMb0t0hIi8L7",
        "outputId": "640d0c89-311a-4184-a9d0-d41c8a9d3d7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "Type: torch.FloatTensor\n",
            "Shape/size: torch.Size([3, 3])\n",
            "Values: \n",
            "tensor([[0.6116, 0.3273, 0.7642],\n",
            "        [0.8197, 0.4571, 0.1784],\n",
            "        [0.9317, 0.1341, 0.0010]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computations will break if both tensors involved are not used on the same device. It's computationally expensive to move data back and forth and therefore typical procedure involves doing parallelizable operations on the GPU and transfer the final results to the CPU.\n",
        "\n",
        "In case you have multiple GPUs, the best practice is to use the CUDA_VISIBLE_DEVICES as environment variable when executing the Python training script."
      ],
      "metadata": {
        "id": "690qwbr5jofb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CUDA_VISIBLE_DEVICES=0,1,2,3 python main.py"
      ],
      "metadata": {
        "id": "evc_7qPkkqmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Tensors and computations within a network*\n",
        "\n"
      ],
      "metadata": {
        "id": "M2IOumsgcqSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apart from storing the data itself, PyTorch tensors handle the intermediate results of gradient computation by setting *requires_grad* flag to True at instantiation time. This is required for model training.\n",
        "\n",
        "At the end of a forward pass through the network, a single scalar (*loss*) is used to compute the backward pass which is initiated by using the *backward()* method. During the backward propagation, gradient vectors are computed for all tensors which where involved during the forward pass.  \n",
        "\n",
        "It's possible to access the gradients for all nodes of the computational graph by using the *.grad* variable of a tensor. The network optimizer uses this variable to update the values of the parameters (model weights). \n",
        "\n"
      ],
      "metadata": {
        "id": "bOJFl35yc_o8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-qxIBeglG5p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d6877a6-6ecc-4781-fd8a-e86dab1b6b6c"
      },
      "source": [
        "x = torch.ones(2, 2, requires_grad=True)\n",
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we require a scalar to apply the backward method to it\n",
        "x = x.mean()\n",
        "x.backward()"
      ],
      "metadata": {
        "id": "5buOeXPNfJLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model objects"
      ],
      "metadata": {
        "id": "wwfaPUBERmK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data sets and data loaders**"
      ],
      "metadata": {
        "id": "pxJChYeUoHLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch has developed standard conventions of interacting with training data that make it fairly consistent to work with, whether you’re working with images, text, or audio. Those convetions include *datasets* and *data loaders*.\n",
        "\n",
        "A Dataset is a PyTorch class which allows to \"pre-package\" training data into the right format. We might apply any manipulation to the data here and implement different methods such as get.length and get.label."
      ],
      "metadata": {
        "id": "106XH8NzIEB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to give a better idea how to define a custom set\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# define your own dataset class by inheriting from PyTorch Dataset \n",
        "class dataset(Dataset):\n",
        "\n",
        "  def __init__(self, path_to_data):\n",
        "    raise NotImplementedError\n",
        "    '''Load data from disk, pre-process it, and compile it\n",
        "    to feature tensors and labels\n",
        "    \n",
        "    self.features = []\n",
        "    self.labels = []\n",
        "    \n",
        "    file_list = dir(path_to_data)\n",
        "    \n",
        "    for i, file in enumerate(file_list)\n",
        "    \n",
        "      normalize data\n",
        "      extract features here\n",
        "    \n",
        "      self.features = append(feature_from_file)\n",
        "      self.labels = append(flabel_from_file)'''\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    raise NotImplementedError\n",
        "    ''' Extract a single item (label + feature tensor) from the\n",
        "    dataset \n",
        "    \n",
        "    return self.features[index], self.labels[index]\n",
        "    \n",
        "    '''\n",
        "  \n",
        "  def __len__(self):\n",
        "    raise NotImplementedError\n",
        "    ''' Returns the length of all training features \n",
        "    \n",
        "    return len(self.features)\n",
        "    \n",
        "    '''\n",
        "\n",
        "# load your training data from disc into the dataset class\n",
        "train_data = dataset(path_to_data)"
      ],
      "metadata": {
        "id": "pptP4-56p04-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A data loader is there to load *batches* of training data into the training pipeline. For that the loader uses the *\\__getitem\\__* method from the Dataset class. The loader also controls the *number of worker* pocesses and whether the training data should be shuffeled. By default, data loaders set the batch size to 1."
      ],
      "metadata": {
        "id": "tTNYdo7SqUNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a DataLoader for the training data (before a training loop)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, num_workers=4, shuffle=True)"
      ],
      "metadata": {
        "id": "Oh4Sw1LVwnTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same procedure has to be performed for the *validation* and *test* data sets."
      ],
      "metadata": {
        "id": "EDdl0uBYx5Pc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The network**\n"
      ],
      "metadata": {
        "id": "jwGEVtRpoLmk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create a network, we inherit from a class called *torch.nn.Network* and fill out the \\__init\\__ and forward methods:"
      ],
      "metadata": {
        "id": "2-izSE3uyksq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# in this example the activations functions are included in the forward method\n",
        "\n",
        "class SimpleNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(12288, 84) # called Dense in Keras\n",
        "    self.fc2 = nn.Linear(84, 50)\n",
        "    self.fc3 = nn.Linear(50,2)\n",
        "  \n",
        "  def forward(self):\n",
        "    x = x.view(-1, 12288)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "# initialize the model\n",
        "simplenet = SimpleNet()"
      ],
      "metadata": {
        "id": "Urq4_-3Zy3M5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in this example the activation functions are included in the layers object and\n",
        "# the forward method just calls it - seems to be more clean\n",
        "\n",
        "class SimpleConvNet(nn.Module):\n",
        "  '''\n",
        "    Simple Convolutional Neural Network\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Conv2d(1, 10, kernel_size=3),\n",
        "      nn.ReLU(),\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(1094500, 50),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(50, 20),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(20, 10)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''Forward pass'''\n",
        "    return self.layers(x)\n",
        "\n",
        "# initialize the model\n",
        "simpleconvnet = SimpleConvNet()"
      ],
      "metadata": {
        "id": "0FBb1c4kz4U6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train the network, we need an optimizer:"
      ],
      "metadata": {
        "id": "ZRmN57l-3EMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize optimizer (before a training loop)\n",
        "optimizer = torch.optim.Adam(simpleconvnet.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "lEUfq3mw3JTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training loop"
      ],
      "metadata": {
        "id": "s4Lb_Tm44aYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to define a training loop for the network. Here are the required pieces:"
      ],
      "metadata": {
        "id": "lnnCTSWg1oX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# configuration options for the training\n",
        "epochs = 1\n",
        "batch_size = 16\n",
        "learning_rate = 1e-4\n",
        "loss_function = nn.CrossEntropyLoss() # includes the softmax activation\n",
        "\n",
        "# initialize data loaders for train, test data\n",
        "# initialize the optimizer\n",
        "\n",
        "# initialize the neural network\n",
        "\n",
        "# set fixed random number seed\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# outer loop for the epoch numbers\n",
        "for epoch in range(epochs):\n",
        "  # inner loop for the mini-batches\n",
        "  for batch in train_loader:\n",
        "    optimizer.zero_grad() # reset gradients to zero before new batch\n",
        "    features, labels = batch\n",
        "    output = simpleconvnet(features)\n",
        "    loss = loss_function(output, labels)\n",
        "    loss.backward() # compute gradients\n",
        "    optimizer.step() # adjust the weights based on the gradients"
      ],
      "metadata": {
        "id": "DYhpp-Ku1xzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to train on a GPU\n",
        "simpleconvnet.to(device)"
      ],
      "metadata": {
        "id": "iiMZu_Bn7GgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving a model\n",
        "torch.save(simplenet, \"path\")\n",
        "\n",
        "# loading a model\n",
        "simplenet = torch.load(\"path\")"
      ],
      "metadata": {
        "id": "OMq56BAvAw2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This stores both the parameters and the structure of the model to a file. This might be a problem if you change the structure of the model at a later point. For this reason, it’s more common to save a model’s state_dict instead. This is a standard Python dict that contains the maps of each layer’s parameters in the model."
      ],
      "metadata": {
        "id": "cRNx1QEHBGwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"path\")\n",
        "\n",
        "# load\n",
        "simplenet = SimpleNet()\n",
        "simplenet_state_dict = torch.load(\"/tmp/simplenet\")\n",
        "simplenet.load_state_dict(simplenet_state_dict)"
      ],
      "metadata": {
        "id": "JtajsgJFBNzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The benefit here is that if you extend the model in some fashion, you can supply a strict=False parameter to load_state_dict that assigns parameters to layers in the model that do exist in the state_dict, but does not fail if the loaded state_dict has layers missing or added from the model’s current structure.\n",
        "\n",
        "Models can be saved to a disk during a training run and reloaded at another point so that training can continue where you left off."
      ],
      "metadata": {
        "id": "iVyDE4-EBkWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Lightning"
      ],
      "metadata": {
        "id": "2CaTppULPKze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch Lightning is a wrapper library which allows to re-pack the training loop inside a single module. This does't really makes you write less code, but apparently you get out a better structured code. For a gentle introduction see https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09\n",
        "\n",
        "Altough, the author suggests to use LightningModule for model and update fuctions, LightningDataModule for DataLoaders, and the Lightning's trainer function to train the model, there are examples where DataLoaders are put inside the LightningModule, which makes it kind of more slick."
      ],
      "metadata": {
        "id": "dXdOBhDvPmXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- TO_DO: intro LIghtning"
      ],
      "metadata": {
        "id": "QB_ZZEIKR6_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment logging"
      ],
      "metadata": {
        "id": "kwmbSyOovlfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensorboard"
      ],
      "metadata": {
        "id": "42prP8dAvqpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO_DO"
      ],
      "metadata": {
        "id": "LNt4sjlBvvh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Examples"
      ],
      "metadata": {
        "id": "GulMyhDsR2rd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feed forward network for image classification**"
      ],
      "metadata": {
        "id": "pQWENpp-Pn07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save images.csv that contains an URL list from https://github.com/falloutdurham/beginners-pytorch-deep-learning/tree/master/chapter2 and copy it in the Colab session.\n",
        "\n",
        "Use then the following script to download the images (1394) into the Colab session (takes over 20 minutes!):"
      ],
      "metadata": {
        "id": "S7vTYyNwQcC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import urllib3\n",
        "from urllib.parse import urlparse\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import shutil\n",
        "\n",
        "from urllib3.util import Retry\n",
        "\n",
        "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
        "\n",
        "classes = [\"cat\", \"fish\"]\n",
        "set_types = [\"train\", \"test\", \"val\"]\n",
        "\n",
        "def download_image(url, klass, data_type):\n",
        "    basename = os.path.basename(urlparse(url).path)\n",
        "    filename = \"{}/{}/{}\".format(data_type, klass, basename)\n",
        "    if not os.path.exists(filename):\n",
        "        try: \n",
        "            http = urllib3.PoolManager(retries=Retry(connect=1, read=1, redirect=2))\n",
        "            with http.request(\"GET\", url, preload_content=False) as resp, open(\n",
        "                filename, \"wb\"\n",
        "            ) as out_file:\n",
        "                if resp.status == 200:\n",
        "                    shutil.copyfileobj(resp, out_file)\n",
        "                else:\n",
        "                    print(\"Error downloading {}\".format(url))\n",
        "            resp.release_conn()\n",
        "        except:\n",
        "            print(\"Error downloading {}\".format(url))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not os.path.exists(\"images.csv\"):\n",
        "        print(\"Error: can't find images.csv!\")\n",
        "        sys.exit(0)\n",
        "\n",
        "    # get args and create output directory\n",
        "    imagesDF = pd.read_csv(\"images.csv\")\n",
        "\n",
        "    for set_type, klass in list(itertools.product(set_types, classes)):\n",
        "        path = \"./{}/{}\".format(set_type, klass)\n",
        "        if not os.path.exists(path):\n",
        "            print(\"Creating directory {}\".format(path))\n",
        "            os.makedirs(path)\n",
        "\n",
        "    print(\"Downloading {} images\".format(len(imagesDF)))\n",
        "\n",
        "    result = [\n",
        "        download_image(url, klass, data_type)\n",
        "        for url, klass, data_type in zip(\n",
        "            imagesDF[\"url\"], imagesDF[\"class\"], imagesDF[\"type\"]\n",
        "        )\n",
        "    ]\n",
        "    sys.exit(0)"
      ],
      "metadata": {
        "id": "FEVpLYV1QOeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating training, validation, and test sets for the image data\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "train_data_path = \"train\"\n",
        "\n",
        "transforms = transforms.Compose([\n",
        "    transforms.Resize(64),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225] )\n",
        "    ])\n",
        "\n",
        "train_data = torchvision.datasets.ImageFolder(root=train_data_path,\n",
        "                                              transform=transforms)\n",
        "\n",
        "val_data_path = \"val\"\n",
        "val_data = torchvision.datasets.ImageFolder(root=val_data_path,\n",
        "                                            transform=transforms)\n",
        "\n",
        "test_data_path = \"test\"\n",
        "test_data = torchvision.datasets.ImageFolder(root=test_data_path,\n",
        "                                            transform=transforms)"
      ],
      "metadata": {
        "id": "UbY_REywWpWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating DataLoaders\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size=64\n",
        "train_data_loader = DataLoader(train_data, batch_size=batch_size)\n",
        "val_data_loader  = DataLoader(val_data, batch_size=batch_size)\n",
        "test_data_loader  = DataLoader(test_data, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "o5m7pzZGXtlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# network\n",
        "class SimpleNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(12288, 84) # called Dense in Keras\n",
        "    self.fc2 = nn.Linear(84, 50)\n",
        "    self.fc3 = nn.Linear(50,2)\n",
        "  \n",
        "  def forward(self):\n",
        "    x = x.view(-1, 12288)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "# initialize the model\n",
        "simplenet = SimpleNet()"
      ],
      "metadata": {
        "id": "1jLgpGbXY3Pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(simplenet.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "Hy1GkTs5Zib7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training loop in a single training function:"
      ],
      "metadata": {
        "id": "1sR0TA_07UAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device=0):\n",
        "  for epoch in range(epochs):\n",
        "    training_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      inputs, target = batch\n",
        "      inputs = inputs.to(device)\n",
        "      target = targets.to(device)\n",
        "      output = model(inputs)\n",
        "      loss = loss_fn(output, target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      training_loss += loss.data.item()\n",
        "    training_loss /= len(train_iterator)\n",
        "      \n",
        "    model.eval()\n",
        "    num_correct = 0\n",
        "    num_examples = 0\n",
        "    for batch in val_loader:\n",
        "      inputs, targets = batch\n",
        "      inputs = inputs.to(device)\n",
        "      output = model(inputs)\n",
        "      targets = targets.to(device)\n",
        "      loss = loss_fn(output,targets)\n",
        "      valid_loss += loss.data.item()\n",
        "      correct = torch.eq(torch.max(F.softmax(output), dim=1)[1],\n",
        "                         target).view(-1)\n",
        "      num_correct += torch.sum(correct).item()\n",
        "      num_examples += correct.shape[0]\n",
        "    valid_loss /= len(valid_iterator)\n",
        "    \n",
        "    print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}'.format(epoch, training_loss,\n",
        "    valid_loss, num_correct / num_examples))"
      ],
      "metadata": {
        "id": "aZzASgJTIE0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(simplenet, optimizer, torch.nn.CrossEntropyLoss(), train_data_loader, test_data_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "g3JJcU2JBk9B",
        "outputId": "47c485cd-afde-4bae-d422-bb8d4f09ff15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnidentifiedImageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-133-e229ee12f99e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimplenet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-126-ad414c61fd48>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, loss_fn, train_loader, val_loader, epochs, device)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \"\"\"\n\u001b[1;32m    229\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2894\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2895\u001b[0m     raise UnidentifiedImageError(\n\u001b[0;32m-> 2896\u001b[0;31m         \u001b[0;34m\"cannot identify image file %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2897\u001b[0m     )\n\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BufferedReader name='train/cat/052cat.jpg'>"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example with prebuilt data set**"
      ],
      "metadata": {
        "id": "fzLHK0AobsMB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aYJLSkQZbw56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face with PyTorch\n",
        "\n"
      ],
      "metadata": {
        "id": "1NI78k428mml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging Face transformers is a Python library which allows to use pre-trained large language models and fine-tune them on your own data set using its Trainer API (see https://huggingface.co/course/chapter3/1?fw=pt).\n",
        "\n",
        "Following this approach, tuning/training a PyTorch model becomes as easy as using Keras' model.fit(). See the following example:"
      ],
      "metadata": {
        "id": "tM-eclpnSOC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning\n",
        "!pip install tensorboard\n",
        "!pip install nlp\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "Ko2w1CY_-gvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "import nlp\n",
        "import transformers"
      ],
      "metadata": {
        "id": "tHSmBOV4-uMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBSentimentClassifier(pl.LightningModule):\n",
        "    \n",
        "    # initilize the model and model loss\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # load a re-trained BERT model from HF transfomers\n",
        "        self.model = transformers.BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "        # cross-entropy loss from PyTorch\n",
        "        self.loss = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    def prepare_data(self):\n",
        "        \n",
        "        # load BERT tokenizer from HF transformers\n",
        "        tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # convert the text field to token ids and add to the data set items\n",
        "        # text sequences also get normalized here\n",
        "        def _tokenize(x):\n",
        "            x['token_ids'] = tokenizer.batch_encode_plus(\n",
        "                    x['text'], \n",
        "                    max_length=32,\n",
        "                    truncation=True, \n",
        "                    padding=True)\n",
        "            return x\n",
        "\n",
        "        # load IMDB data set from HF nlp and split it\n",
        "        def _prepare_ds(split):\n",
        "            ds = nlp.load_dataset('imdb', split='train[:10%]')\n",
        "            ds = ds.map(_tokenize, batched=True)\n",
        "            ds.set_format(type='torch', columns=['token_ids', 'label'])\n",
        "            return ds\n",
        "\n",
        "        self.train_ds, self.test_ds = map(_prepare_ds, ('train', 'test'))\n",
        "\n",
        "    def forward(self, token_ids):\n",
        "        mask = (token_ids != 0).float()\n",
        "        logits, = self.model(token_ids, mask)\n",
        "        return logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        logits = self.forward(batch['token_ids'])\n",
        "        loss = self.loss(logits, batch['label']).mean()\n",
        "        return {'loss': loss, 'log': {'train_loss': loss}}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        logits = self.forward(batch['token_ids'])\n",
        "        loss = self.loss(logits, batch['label'])\n",
        "        acc = (logits.argmax(-1) == batch['label']).float()\n",
        "        return {'loss': loss, 'acc': acc}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        loss = torch.cat([o['loss'] for o in outputs], 0).mean()\n",
        "        acc = torch.cat([o['acc'] for o in outputs], 0).mean()\n",
        "        out = {'val_loss': loss, 'val_acc': acc}\n",
        "        return {**out, 'log': out}\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "                self.train_ds,\n",
        "                batch_size=8,\n",
        "                drop_last=True,\n",
        "                shuffle=True,\n",
        "                )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "                self.test_ds,\n",
        "                batch_size=8,\n",
        "                drop_last=False,\n",
        "                shuffle=True,\n",
        "                )\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.SGD(\n",
        "            self.parameters(),\n",
        "            lr=1e-2,\n",
        "            momentum=0.9,\n",
        "        )\n",
        "    \n",
        "def main(_):\n",
        "    model = IMDBSentimentClassifier()\n",
        "    trainer = pl.Trainer(\n",
        "        default_root_dir='root/logs',\n",
        "        gpus=(1 if torch.cuda.is_available() else 0),\n",
        "        max_epochs=10,\n",
        "        logger=pl.loggers.TensorBoardLogger('root/logs/', name='imdb', version=0),\n",
        "    )\n",
        "    trainer.fit(model)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main(_)"
      ],
      "metadata": {
        "id": "3HMXSEXd-xp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, in case you need any additional customizations for your training you can still utilize all of the underlying PyTorch functionality and implement your own training loop (see https://huggingface.co/course/chapter3/4?fw=pt)."
      ],
      "metadata": {
        "id": "rKMrIKe9_tA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using AWS Sagemaker"
      ],
      "metadata": {
        "id": "WhNvJaLMwqzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO_DO"
      ],
      "metadata": {
        "id": "kFCzJc1Gwurl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hfHuNeC9wt6p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}