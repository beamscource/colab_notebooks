{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "v5HyjCAhV6jT",
        "iS_MyDcDREj8",
        "6q4DL8Z4JP9K",
        "jIuwscxYG3kA",
        "nnLG-dwwMe6w",
        "heXpDimsanKL",
        "ia4HyJ5zUYJC",
        "HlLgKe9_To_a",
        "FZJ_UZfdm2yq",
        "OcobxzOsgmMe",
        "M2IOumsgcqSk",
        "wwfaPUBERmK_",
        "jwGEVtRpoLmk",
        "pxJChYeUoHLv",
        "s4Lb_Tm44aYA",
        "Sq6zK1yaVXZd",
        "s3ht_4gk_J6f",
        "kwmbSyOovlfM",
        "pQWENpp-Pn07",
        "sGCGjWAYMkkt",
        "TxsHX6fagkGx",
        "9GoLcs2rRiFt",
        "1NI78k428mml"
      ],
      "authorship_tag": "ABX9TyNUUjTuHgNXHzhfxKsdsULt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d8f8236f31ef404db00cfd9cc33cf5be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43ccd0a4d3b64662892b98c72915601f",
              "IPY_MODEL_e9d7e34efe5d4fa4b7a3089be534d228",
              "IPY_MODEL_2a08d3341ffa4ebe9604a7390ec40cab"
            ],
            "layout": "IPY_MODEL_89c8d98e9d0546a8a99a640ba0f7d36a"
          }
        },
        "43ccd0a4d3b64662892b98c72915601f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b109dcd3af546c08c5a6f0c3bb9b982",
            "placeholder": "​",
            "style": "IPY_MODEL_b5498f5609ac4ae5ab8438e177738522",
            "value": "100%"
          }
        },
        "e9d7e34efe5d4fa4b7a3089be534d228": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5edd651cab504470bf6076f12aa24f04",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a9b9804e055c42a1b156b00c66ce7617",
            "value": 100
          }
        },
        "2a08d3341ffa4ebe9604a7390ec40cab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0c235fb107d4d98a4d2a42289f6c311",
            "placeholder": "​",
            "style": "IPY_MODEL_2acda8a104784d149d23ab78a8573123",
            "value": " 100/100 [00:07&lt;00:00, 16.31it/s]"
          }
        },
        "89c8d98e9d0546a8a99a640ba0f7d36a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b109dcd3af546c08c5a6f0c3bb9b982": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5498f5609ac4ae5ab8438e177738522": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5edd651cab504470bf6076f12aa24f04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9b9804e055c42a1b156b00c66ce7617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0c235fb107d4d98a4d2a42289f6c311": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2acda8a104784d149d23ab78a8573123": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e312058d534143f88ad373782dfcf8c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b5c1d388fe247e586f8ea7e6ac1acc7",
              "IPY_MODEL_268b70eb9a0d4b9585629ea098b2bccd",
              "IPY_MODEL_bea98cd8f34f46dfa3f05bded41f47c7"
            ],
            "layout": "IPY_MODEL_890f7094f8f14175b41a7839a2652266"
          }
        },
        "8b5c1d388fe247e586f8ea7e6ac1acc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8aac15fea6b3450593f3de0566ce17f7",
            "placeholder": "​",
            "style": "IPY_MODEL_5c976a6f49b04215a1026a5eb5431869",
            "value": "100%"
          }
        },
        "268b70eb9a0d4b9585629ea098b2bccd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_895135a9ecb7484991c33ae32e7d9433",
            "max": 244408911,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38e29e1e5a494e61a18d9db76dc32a83",
            "value": 244408911
          }
        },
        "bea98cd8f34f46dfa3f05bded41f47c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5d4ee258443403a9f3260641581aab5",
            "placeholder": "​",
            "style": "IPY_MODEL_c9d76c976755483bac7349c7e02cc124",
            "value": " 233M/233M [00:01&lt;00:00, 219MB/s]"
          }
        },
        "890f7094f8f14175b41a7839a2652266": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aac15fea6b3450593f3de0566ce17f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c976a6f49b04215a1026a5eb5431869": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "895135a9ecb7484991c33ae32e7d9433": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38e29e1e5a494e61a18d9db76dc32a83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a5d4ee258443403a9f3260641581aab5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9d76c976755483bac7349c7e02cc124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ad446cb2a9940eaba5c21cf8ee07e79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_053903b76f7e4cd1becf0f18a5d2eb1f",
              "IPY_MODEL_6452972c24884da199a37166f39a40d0",
              "IPY_MODEL_43184973c86846babec4e264640098e0"
            ],
            "layout": "IPY_MODEL_17462a9249c043058f31f9ae429082ab"
          }
        },
        "053903b76f7e4cd1becf0f18a5d2eb1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bf61653db854a5184a0405ddfce6c72",
            "placeholder": "​",
            "style": "IPY_MODEL_6c878b76530e454489efdba6fed3ed69",
            "value": "  0%"
          }
        },
        "6452972c24884da199a37166f39a40d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e07ee736341c4fd580d968d9c69bb3b3",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f32d1163d5114e44876fc689e31f50c3",
            "value": 0
          }
        },
        "43184973c86846babec4e264640098e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c405faadf9634776aecbb8e6ea4c214e",
            "placeholder": "​",
            "style": "IPY_MODEL_a4c8d1201f904830830d68b514f47aee",
            "value": " 0/100 [00:00&lt;?, ?it/s]"
          }
        },
        "17462a9249c043058f31f9ae429082ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bf61653db854a5184a0405ddfce6c72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c878b76530e454489efdba6fed3ed69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e07ee736341c4fd580d968d9c69bb3b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f32d1163d5114e44876fc689e31f50c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c405faadf9634776aecbb8e6ea4c214e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4c8d1201f904830830d68b514f47aee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c58cb25edbb4d958826a9a13c96e003": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc1917bf1da04472a28956e3f4db121d",
              "IPY_MODEL_502666c085e34ec1b6b2c83b725c20c4",
              "IPY_MODEL_8a72abd599d244c3a764345dd907545d"
            ],
            "layout": "IPY_MODEL_6c81461c1e864e62beb634638839fc2f"
          }
        },
        "cc1917bf1da04472a28956e3f4db121d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ee74950374c439b970ad764f532faf4",
            "placeholder": "​",
            "style": "IPY_MODEL_84c1e5133d5043de9824db22788fa509",
            "value": " 45%"
          }
        },
        "502666c085e34ec1b6b2c83b725c20c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3dded5b149fd4584829746387b6b0d08",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f51f0c9db16b4cd38d0fb82cb72ed8b9",
            "value": 9
          }
        },
        "8a72abd599d244c3a764345dd907545d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a611a7e2b9d847ad827aac12938ca65d",
            "placeholder": "​",
            "style": "IPY_MODEL_584931f17c7245f9ae2d9c53fc4073a4",
            "value": " 9/20 [15:30&lt;18:18, 99.82s/it]"
          }
        },
        "6c81461c1e864e62beb634638839fc2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ee74950374c439b970ad764f532faf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84c1e5133d5043de9824db22788fa509": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3dded5b149fd4584829746387b6b0d08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f51f0c9db16b4cd38d0fb82cb72ed8b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a611a7e2b9d847ad827aac12938ca65d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "584931f17c7245f9ae2d9c53fc4073a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41ca62de845e4a6e97fc18addfb4785b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_841ad15f068e4f6aa05b6db04446147d",
              "IPY_MODEL_8ce4b0fdc8c34b57b63f34a1a6425ee8",
              "IPY_MODEL_1434f96a632e411f95efb18a22949dcd"
            ],
            "layout": "IPY_MODEL_3005a789363445f0b88dccb584a5fa09"
          }
        },
        "841ad15f068e4f6aa05b6db04446147d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d972a22ac1a42d8af0babf4a753b1cb",
            "placeholder": "​",
            "style": "IPY_MODEL_9280137bde34443780539267b6c7dddc",
            "value": " 30%"
          }
        },
        "8ce4b0fdc8c34b57b63f34a1a6425ee8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e24b8065302461a823f63ee4e520714",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_21c9ddc0e696412981bfe9bb5e15bab2",
            "value": 6
          }
        },
        "1434f96a632e411f95efb18a22949dcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81d9a85df32b4468a410d8bfa28267b1",
            "placeholder": "​",
            "style": "IPY_MODEL_b9072a5c5ece49ffbe300ba2ae702900",
            "value": " 6/20 [1:14:27&lt;2:57:07, 759.08s/it]"
          }
        },
        "3005a789363445f0b88dccb584a5fa09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d972a22ac1a42d8af0babf4a753b1cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9280137bde34443780539267b6c7dddc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e24b8065302461a823f63ee4e520714": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21c9ddc0e696412981bfe9bb5e15bab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "81d9a85df32b4468a410d8bfa28267b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9072a5c5ece49ffbe300ba2ae702900": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beamscource/colab_notebooks/blob/main/pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch"
      ],
      "metadata": {
        "id": "DylM7tW-zerv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary based on https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial2/Introduction_to_PyTorch.html \n",
        "\n",
        "Additional resources taken from Programming PyTorch (https://github.com/falloutdurham/beginners-pytorch-deep-learning), NLP with PyTorch and https://deeplearning.cs.cmu.edu/F22/index.html."
      ],
      "metadata": {
        "id": "EugY5JiWDFQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction"
      ],
      "metadata": {
        "id": "v5HyjCAhV6jT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch is a Python library which offers an **eager approach to differentiation** instead of defining static graphs, allowing for greater flexibility in the way networks are created, trained, and operated.\n",
        "\n",
        "Similar to DyNet and Chainer, and in contrast to static frameworks like TensorFlow/Theano/Caffe, models are not compiled before execution. \n",
        "\n",
        "PyTorch has two lineages. First, it derives many features and concepts from Torch, which was a Lua-based neural network library that dates back to 2002. Its other major parent is Chainer, created in Japan in 2015.\n",
        "\n",
        "The library also comes with modules that help with manipulating text, images, and audio (*torchtext*, *torchvision*, and *torchaudio*), along with built-in variants of popular architectures such as ResNet (with weights that can be downloaded to provide assistance with *transfer learning*).\n",
        "\n",
        "In 2022, about 85% of pre-trained models on HuggingFace are PyTorch models (https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2022/). Despite the fact that PyTorch is used by companies like Twitter, Salesforce, Tesla, Uber, and NVIDIA, the consensus seems to be that TF still offers better native deployment capabilities and that tf.keras might be better suited for a complete beginner."
      ],
      "metadata": {
        "id": "a-8pyuqhWl-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All code examples can be found at https://github.com/falloutdurham/beginners-pytorch-deep-learning For more infos and tutorials see https://pytorch.org/hub/"
      ],
      "metadata": {
        "id": "a2MaJ7GgWLft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "GnsgtHqiHBPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "cNwx3pipHEtq",
        "outputId": "ebceaeea-5e49-4abe-a183-3af5c82faacc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.12.1+cu113'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensors"
      ],
      "metadata": {
        "id": "iS_MyDcDREj8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Basics"
      ],
      "metadata": {
        "id": "6q4DL8Z4JP9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensors are objects (\"multidimensional arrays\" or matrices) which hold numerical data of a single type used to propagate through the network. For example, a 1st-order tensor is a vector (one dimensional array) and 2nd-order tensor is a matrix. If you are coming from Matlab or NUmPy, this feels very familiar."
      ],
      "metadata": {
        "id": "wdU3bd-99zfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a tensor from Python lists\n",
        "x = torch.tensor([[0,0,1],[1,1,1],[0,0,0]])\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Li2WqHJND70Q",
        "outputId": "ecf6f4c1-8c25-47f1-9ed0-6d0a042fd99c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 1],\n",
              "        [1, 1, 1],\n",
              "        [0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.type()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-UpzAArLEMjh",
        "outputId": "793da8c6-893b-4082-e241-2005c4e6e104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'torch.LongTensor'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# investigating the size of a tensor\n",
        "x.shape"
      ],
      "metadata": {
        "id": "8DZEpme-EMyF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daf52f46-7e2c-4a99-ef6e-a06b9638010f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# or\n",
        "x.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkdDfCqkLY76",
        "outputId": "0de23fa6-6bba-40c8-bac6-a61f15de742a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYmwWhUbRYP6"
      },
      "source": [
        "# helper function to investigate a tensor\n",
        "def describe(x):\n",
        "  print(f\"Type: {x.type()}\")\n",
        "  print(f\"Shape/size: {x.shape}\")\n",
        "  print(f\"Values: \\n{x}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO-u9AEyR6KC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bf3252d-27e1-4759-e4a8-823ba89c79d5"
      },
      "source": [
        "describe(torch.Tensor(2, 3))\n",
        "describe(torch.rand(2, 3))   # uniform randomdescribe\n",
        "describe(torch.randn(2, 3))  # random normal"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type: torch.FloatTensor\n",
            "Shape/size: torch.Size([2, 3])\n",
            "Values: \n",
            "tensor([[1.9839e-35, 0.0000e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 0.0000e+00, 0.0000e+00]])\n",
            "Type: torch.FloatTensor\n",
            "Shape/size: torch.Size([2, 3])\n",
            "Values: \n",
            "tensor([[0.3545, 0.2646, 0.6202],\n",
            "        [0.7330, 0.5283, 0.9949]])\n",
            "Type: torch.FloatTensor\n",
            "Shape/size: torch.Size([2, 3])\n",
            "Values: \n",
            "tensor([[ 1.2141,  1.3971, -0.0657],\n",
            "        [-0.0966, -0.0245,  0.8139]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# special tensors\n",
        "describe(torch.eye(3))\n",
        "describe(torch.ones(2,3))\n",
        "describe(torch.zeros(2,3))\n",
        "describe(torch.arange(0,3))"
      ],
      "metadata": {
        "id": "gfoX9AbSFkEW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c56910b2-85ea-43e2-9ab6-8508558ef650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type: torch.FloatTensor\n",
            "Shape/size: torch.Size([3, 3])\n",
            "Values: \n",
            "tensor([[1., 0., 0.],\n",
            "        [0., 1., 0.],\n",
            "        [0., 0., 1.]])\n",
            "Type: torch.FloatTensor\n",
            "Shape/size: torch.Size([2, 3])\n",
            "Values: \n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "Type: torch.FloatTensor\n",
            "Shape/size: torch.Size([2, 3])\n",
            "Values: \n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "Type: torch.LongTensor\n",
            "Shape/size: torch.Size([3])\n",
            "Values: \n",
            "tensor([0, 1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a tensor from NumPy array\n",
        "# the type of the created tensor is DoubleTensor which corresponds to NumPy\n",
        "# float64 matrix\n",
        "npy  =  np.random.rand(2,  3)\n",
        "describe(torch.from_numpy(npy))\n",
        "# or\n",
        "describe(torch.as_tensor(npy))\n",
        "npy"
      ],
      "metadata": {
        "id": "XZ8Vl4ozSI0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# export to numpy array\n",
        "x_np = x.numpy()\n",
        "print(x_np)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rneq8s6SItMd",
        "outputId": "ebf7bd5e-db6f-4698-ec00-e9cc525cdac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 1]\n",
            " [1 1 1]\n",
            " [0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.tensor() always copies data. If you have a numpy array and want to avoid a copy, use torch.as_tensor()."
      ],
      "metadata": {
        "id": "2FTnuyj9L29m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create uninitialized tensor\n",
        "x = torch.FloatTensor(2,3)\n",
        "print(x)\n",
        "# initialize/set to zeros\n",
        "x.zero_()\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Z3w_rGnJq73",
        "outputId": "49d8f5ce-4bc6-4f83-8c8a-5d14f2350967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.9839e-35, 0.0000e+00, 1.4013e-45],\n",
            "        [0.0000e+00, 2.8026e-45, 0.0000e+00]])\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create random tensor (seed for repeatability)\n",
        "torch.manual_seed(123)\n",
        "x=torch.randn(2,3)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "sS5TZpibKAuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Different types"
      ],
      "metadata": {
        "id": "jIuwscxYG3kA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The default  tensor  type  when  using  torch.Tensor  constructor  is  a torch.FloatTensor. But it's possible to convert it to float,  long,  double format  by  specifying  it  at the initialization  or  using  one  of  the typecasting  methods. The most common types you will use are `IntTensor` and `FloatTensor`.\n",
        "\n",
        "See more infos at https://pytorch.org/docs/stable/tensors.html"
      ],
      "metadata": {
        "id": "0GzDJ94-NvOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using dtype at initialization\n",
        "torch.zeros([2, 4], dtype=torch.int32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghbEhd3QILqr",
        "outputId": "3c3a6766-fbb9-49ba-9263-9754290bd451"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0, 0],\n",
              "        [0, 0, 0, 0]], dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calling a specific constructor at initialization\n",
        "x = torch.FloatTensor([[1, 2, 3],\n",
        "                   [4,5,6]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9IulhcRKED0",
        "outputId": "24a5ad91-eb56-4986-83f4-9e19a4ab3da4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 2., 3.],\n",
              "        [4., 5., 6.]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# typecasting\n",
        "x.long()"
      ],
      "metadata": {
        "id": "huSO6702K8O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Indexing and slicing"
      ],
      "metadata": {
        "id": "nnLG-dwwMe6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a tensor with a short-cut\n",
        "x = torch.arange(6).view(2, 3)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuHqSI-SM7H2",
        "outputId": "9a1e75a0-7667-4a83-c2a5-24877c0dbe5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 1, 2],\n",
              "        [3, 4, 5]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# indexing into a tensor works like in hierarchical lists (standard Python)\n",
        "x[0][1:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUxGfoVtFOu9",
        "outputId": "e3374ec2-dc94-4a79-8831-add569a38bbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# but also like in NumPy\n",
        "x[0,0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ec7JX0eNzzV",
        "outputId": "b5c49913-6684-41c0-8cce-ca35e5a37972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# access the first two elements in the first row (indexing is starting at zero)\n",
        "# take from the row at index zero all elements until the element at index 2\n",
        "x[0, :2] "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtjQDmxVOxEc",
        "outputId": "2b8fbdde-a0ae-4cbd-fda0-9736f782dcba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[1,1:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DKMd9UaPPew",
        "outputId": "d874dd0e-2bf9-4e3f-b9d9-4fe1349b2828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# access scalar values from a single-element tensor\n",
        "torch.rand(1).item()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15Sbnb1OGSry",
        "outputId": "5a9b861a-f7c3-4986-f96c-25172fe868a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5620666146278381"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# replace all elements of a tensor\n",
        "x = torch.ones(4,8)\n",
        "x.fill_(5)\n",
        "x"
      ],
      "metadata": {
        "id": "A0-xiwgpEb9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Any PyTorch method with an underscore (_) refers to an inplace operation; that is, it modifies the content in place without creating a new object."
      ],
      "metadata": {
        "id": "HXLYLnOJJMYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# indexing using PyTorch functions\n",
        "# indices have to be of the type LongTensor\n",
        "print(x)\n",
        "indices = torch.LongTensor([0, 0])\n",
        "# joining first row into a new tensor\n",
        "describe(torch.index_select(x, dim=0, index=indices))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8GoGcEkYEhh",
        "outputId": "608ab873-8482-4656-e9bc-785957381b63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00,        nan, 4.7399e+16],\n",
            "        [4.4721e+21, 1.5956e+25, 4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00]])\n",
            "Type: torch.FloatTensor\n",
            "Shape/size: torch.Size([2, 6])\n",
            "Values: \n",
            "tensor([[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00,        nan, 4.7399e+16],\n",
            "        [1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00,        nan, 4.7399e+16]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting non-contogous elements by passing tensors as indices\n",
        "print(x)\n",
        "row_indices = torch.arange(2).long() # take from rows zero and one\n",
        "col_indices = torch.LongTensor([0, 1]) # take from colums zero and one\n",
        "print(row_indices)\n",
        "print(col_indices)\n",
        "describe(x[row_indices, col_indices])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI3aLn0TZIWF",
        "outputId": "18baccf5-37c1-4ba1-f2dc-6834b3b08642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00,        nan, 4.7399e+16],\n",
            "        [4.4721e+21, 1.5956e+25, 4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00]])\n",
            "tensor([0, 1])\n",
            "tensor([0, 1])\n",
            "Type: torch.FloatTensor\n",
            "Shape/size: torch.Size([2])\n",
            "Values: \n",
            "tensor([1.4068e-34, 1.5956e+25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Concatenating"
      ],
      "metadata": {
        "id": "heXpDimsanKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# on columns\n",
        "print(x)\n",
        "describe(torch.cat([x, x], dim=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGWGBaYvau5j",
        "outputId": "67adf324-b0cc-47c5-ec8c-4669fd0c0886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00,        nan, 4.7399e+16],\n",
            "        [4.4721e+21, 1.5956e+25, 4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00]])\n",
            "Type: torch.FloatTensor\n",
            "Shape/size: torch.Size([4, 6])\n",
            "Values: \n",
            "tensor([[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00,        nan, 4.7399e+16],\n",
            "        [4.4721e+21, 1.5956e+25, 4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00],\n",
            "        [1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00,        nan, 4.7399e+16],\n",
            "        [4.4721e+21, 1.5956e+25, 4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# on rows\n",
        "print(y)\n",
        "describe(torch.cat([y, y], dim=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVghejRUbBFk",
        "outputId": "497e9fb2-2c44-4b59-f47c-dd59dcef03bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00],\n",
            "        [       nan, 4.7399e+16, 4.4721e+21, 1.5956e+25],\n",
            "        [4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00]])\n",
            "Type: torch.FloatTensor\n",
            "Shape/size: torch.Size([3, 8])\n",
            "Values: \n",
            "tensor([[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00, 1.4068e-34, 0.0000e+00,\n",
            "         3.3631e-44, 0.0000e+00],\n",
            "        [       nan, 4.7399e+16, 4.4721e+21, 1.5956e+25,        nan, 4.7399e+16,\n",
            "         4.4721e+21, 1.5956e+25],\n",
            "        [4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00, 4.7399e+16, 2.3868e-06,\n",
            "         1.4838e-41, 0.0000e+00]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to keep tensors as separated elements\n",
        "print(y)\n",
        "describe(torch.stack([y, y], dim=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-pOTutKbUDa",
        "outputId": "05083fdd-7312-40fe-cfa3-9ef10ae7228f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00],\n",
            "        [       nan, 4.7399e+16, 4.4721e+21, 1.5956e+25],\n",
            "        [4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00]])\n",
            "Type: torch.FloatTensor\n",
            "Shape/size: torch.Size([2, 3, 4])\n",
            "Values: \n",
            "tensor([[[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00],\n",
            "         [       nan, 4.7399e+16, 4.4721e+21, 1.5956e+25],\n",
            "         [4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00]],\n",
            "\n",
            "        [[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00],\n",
            "         [       nan, 4.7399e+16, 4.4721e+21, 1.5956e+25],\n",
            "         [4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)\n",
        "describe(torch.stack([y, y], dim=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GSuDfcab1Q8",
        "outputId": "9575e133-bb23-4497-aa66-dcb6cee1960d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00],\n",
            "        [       nan, 4.7399e+16, 4.4721e+21, 1.5956e+25],\n",
            "        [4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00]])\n",
            "Type: torch.FloatTensor\n",
            "Shape/size: torch.Size([3, 2, 4])\n",
            "Values: \n",
            "tensor([[[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00],\n",
            "         [1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00]],\n",
            "\n",
            "        [[       nan, 4.7399e+16, 4.4721e+21, 1.5956e+25],\n",
            "         [       nan, 4.7399e+16, 4.4721e+21, 1.5956e+25]],\n",
            "\n",
            "        [[4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00],\n",
            "         [4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Manipulating tensors' dimensions"
      ],
      "metadata": {
        "id": "ia4HyJ5zUYJC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X44T7ZrxicpS",
        "outputId": "78290576-aaea-4a3f-da96-96b80b910886"
      },
      "source": [
        "# change dimensions of the tensor\n",
        "x = torch.Tensor(2,6)\n",
        "print(x)\n",
        "# view is not changing the original tensor\n",
        "# you have to assign it to a new tensor\n",
        "x.view(3, 4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.4062e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00,        nan, 1.5975e-43],\n",
            "        [4.4721e+21, 1.5956e+25, 4.7399e+16, 2.3868e-06, 1.4838e-41, 1.5695e-43]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.4062e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00],\n",
              "        [       nan, 1.5975e-43, 4.4721e+21, 1.5956e+25],\n",
              "        [4.7399e+16, 2.3868e-06, 1.4838e-41, 1.5695e-43]])"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)\n",
        "y = x.view(3, 4)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlaMJ32mUyqY",
        "outputId": "159544fe-bfa5-4222-a516-8bd8c8f316dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00,        nan, 4.7399e+16],\n",
            "        [4.4721e+21, 1.5956e+25, 4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00]])\n",
            "tensor([[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00],\n",
            "        [       nan, 4.7399e+16, 4.4721e+21, 1.5956e+25],\n",
            "        [4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# or if you want to operate on non-contigous tensors\n",
        "print(x)\n",
        "y = x.reshape(3, 4)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7ymH2Kok74I",
        "outputId": "61766dfb-98e4-4914-fb6d-d1b6bd906374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.4062e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00,        nan, 1.5975e-43],\n",
            "        [4.4721e+21, 1.5956e+25, 4.7399e+16, 2.3868e-06, 1.4838e-41, 1.5695e-43]])\n",
            "tensor([[1.4062e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00,        nan, 1.5975e-43],\n",
            "        [4.4721e+21, 1.5956e+25, 4.7399e+16, 2.3868e-06, 1.4838e-41, 1.5695e-43]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iH9FdmzfkADq",
        "outputId": "65a50b4f-55db-459f-8ae5-2ee3d41f88c6"
      },
      "source": [
        "# transposing a tensor (columns become rows)\n",
        "torch.transpose(y, 0, 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.4068e-34,        nan, 4.7399e+16],\n",
              "        [0.0000e+00, 4.7399e+16, 2.3868e-06],\n",
              "        [3.3631e-44, 4.4721e+21, 1.4838e-41],\n",
              "        [0.0000e+00, 1.5956e+25, 0.0000e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# re-arrange dimensions of a tensor\n",
        "x = torch.rand(640, 480, 3)\n",
        "y = x.permute(2,0,1)\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEPbXNgPmQrf",
        "outputId": "bc76ad37-34e2-496f-9da4-cc556fd058e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 640, 480])"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Operations on tensors (matrix algebra)"
      ],
      "metadata": {
        "id": "HlLgKe9_To_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch math and linear algebra is also similar to Numpy. Operators are overridden so you can use standard math operators (`+`,`-`, etc.) and expect a tensor as a result. See pytorch documentation for a complete list of available functions."
      ],
      "metadata": {
        "id": "uvBojFuVLUpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(0.,5.)\n",
        "print(torch.sum(x))\n",
        "print(torch.sum(torch.exp(x)))\n",
        "print(torch.mean(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLTHfaTWLfQ0",
        "outputId": "15f6f057-6418-42b5-cb70-b5f6353e0ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(10.)\n",
            "tensor(85.7910)\n",
            "tensor(2.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# element-wise additon with mathematical symbols\n",
        "torch.ones(1,2) + torch.ones(1,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0iRmesUFwiv",
        "outputId": "0543a393-9fc1-4541-dafe-3b2645e58976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2., 2.]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# or with built-in methods\n",
        "torch.add(torch.ones(1,2), torch.ones(1,2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1BJmrv9T1cs",
        "outputId": "3970b32b-a010-4fab-bb39-d0619be0cedf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2., 2.]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# summing alog the colums\n",
        "print(y)\n",
        "describe(torch.sum(y, dim=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "li9wj5SlUKFa",
        "outputId": "b152593b-9ac4-4cba-eed8-a4acea5c2921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.4068e-34, 0.0000e+00, 3.3631e-44, 0.0000e+00],\n",
            "        [       nan, 4.7399e+16, 4.4721e+21, 1.5956e+25],\n",
            "        [4.7399e+16, 2.3868e-06, 1.4838e-41, 0.0000e+00]])\n",
            "Type: torch.FloatTensor\n",
            "Shape/size: torch.Size([4])\n",
            "Values: \n",
            "tensor([       nan, 4.7399e+16, 4.4721e+21, 1.5956e+25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# or rows\n",
        "describe(torch.sum(y, dim=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ug9viDYVheC",
        "outputId": "a2e2846b-c6ac-4d9f-b616-7e29a6d1308d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type: torch.FloatTensor\n",
            "Shape/size: torch.Size([3])\n",
            "Values: \n",
            "tensor([1.4068e-34,        nan, 4.7399e+16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# matrix multiplication\n",
        "describe(torch.mm())"
      ],
      "metadata": {
        "id": "me3OuZ88cRVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# access the max value\n",
        "x.max().item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAgM44ZpiU4G",
        "outputId": "5ca2111c-4266-4d6b-8ce1-ae3897de215c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.0"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Broadcasting"
      ],
      "metadata": {
        "id": "FZJ_UZfdm2yq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Borrowed from NumPy, broadcasting allows to perform operations between a tensor and a smaller tensor. You can broadcast across two tensors if, starting backward from their trailing dimensions: \n",
        "- the two dimensions are equal\n",
        "- one of the dimensions is 1"
      ],
      "metadata": {
        "id": "N3QJP7a_nGRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GPU vs CPU tensors"
      ],
      "metadata": {
        "id": "OcobxzOsgmMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensors can be copied between CPU and GPU. It is important that everything involved in a calculation is on the same device. \n",
        "\n",
        "By default, PyTorch tensors are created to be used by a CPU."
      ],
      "metadata": {
        "id": "PyfwFzjfg46b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cpu_tensor = torch.rand(2)\n",
        "cpu_tensor.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOjCCW_JhTMI",
        "outputId": "82319306-593a-41f0-ac5f-493e08da35c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When doing linear algebra operations it make sense to utilize a GPU. To use a GPU, you need to first allocate the tensor on the GPU’s memory. Access to GPUs is provided via CUDA API that was created by NVIDIA and is limited to use only NVIDIA GPUs."
      ],
      "metadata": {
        "id": "gEZ77o7Jhtu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# in colab you need to change runtime environment for this to work\n",
        "# transfer a tensor to a GPU\n",
        "gpu_tensor = cpu_tensor.to(\"cuda\")\n",
        "gpu_tensor.device"
      ],
      "metadata": {
        "id": "5Z1syqE6hu_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(3,2)\n",
        "# copy to GPU\n",
        "y = x.cuda()\n",
        "# copy back to CPU\n",
        "z = y.cpu()\n",
        "# get CPU tensor as numpy array\n",
        "# cannot get GPU tensor as numpy array directly\n",
        "try:\n",
        "    y.numpy()\n",
        "except RuntimeError as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "rxw6RSLSMMvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Operations between GPU and CPU tensors will fail. Operations require all arguments to be on the same device."
      ],
      "metadata": {
        "id": "Ku6Gsm-OMSs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(3,5)  # CPU tensor\n",
        "y = torch.rand(5,4).cuda()  # GPU tensor\n",
        "try:\n",
        "    torch.mm(x,y)  # Operation between CPU and GPU fails\n",
        "except TypeError as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "X-yiu2toMT2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy to CPU if on GPU\n",
        "if y.is_cuda:\n",
        "    y = y.cpu()\n",
        "    print(y, y.dtype)"
      ],
      "metadata": {
        "id": "FGPmQaUUMf-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A convenient method is `new`, which creates a new tensor on the same device as another tensor. It should be used for creating tensors whenever possible."
      ],
      "metadata": {
        "id": "-NCKUovuNN41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = torch.rand(3,2)\n",
        "x2 = x1.new(1,2)  # create cpu tensor\n",
        "print(x2)\n",
        "x1 = torch.rand(3,2).cuda()\n",
        "x2 = x1.new(1,2)  # create cuda tensor\n",
        "print(x2)"
      ],
      "metadata": {
        "id": "InPBfti-NTsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be device agnostic and write code that works whether it’s on the GPU or the CPU:"
      ],
      "metadata": {
        "id": "ctVdpI5Uiynf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "x = torch.rand(3, 3).to(device)\n",
        "describe(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMb0t0hIi8L7",
        "outputId": "640d0c89-311a-4184-a9d0-d41c8a9d3d7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "Type: torch.FloatTensor\n",
            "Shape/size: torch.Size([3, 3])\n",
            "Values: \n",
            "tensor([[0.6116, 0.3273, 0.7642],\n",
            "        [0.8197, 0.4571, 0.1784],\n",
            "        [0.9317, 0.1341, 0.0010]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computations will break if both tensors involved are not used on the same device. It's computationally expensive to move data back and forth and therefore typical procedure involves doing parallelizable operations on the GPU and transfer the final results to the CPU.\n",
        "\n",
        "In case you have multiple GPUs, the best practice is to use the CUDA_VISIBLE_DEVICES as environment variable when executing the Python training script."
      ],
      "metadata": {
        "id": "690qwbr5jofb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CUDA_VISIBLE_DEVICES=0,1,2,3 python main.py"
      ],
      "metadata": {
        "id": "evc_7qPkkqmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculations executed on the GPU can be many times faster than numpy. However, numpy is still optimized for the CPU and many times faster than python `for loops`. Numpy calculations may be faster than GPU calculations for small arrays due to the cost of interfacing with the GPU."
      ],
      "metadata": {
        "id": "HkiFEDcxNbdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import timeit\n",
        "# Create random data\n",
        "x = torch.rand(1000,64)\n",
        "y = torch.rand(64,32)\n",
        "number = 10000  # number of iterations\n",
        "\n",
        "def square():\n",
        "    z=torch.mm(x, y) # dot product (mm=matrix multiplication)\n",
        "\n",
        "# Time CPU\n",
        "print('CPU: {}ms'.format(timeit(square, number=number)*1000))\n",
        "# Time GPU\n",
        "x, y = x.cuda(), y.cuda()\n",
        "print('GPU: {}ms'.format(timeit(square, number=number)*1000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSZPHMIXNctB",
        "outputId": "b00f80e9-fdde-44ef-c438-f997dad46c1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU: 547.9166890000045ms\n",
            "GPU: 2637.6892470000257ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Differentiation within a network"
      ],
      "metadata": {
        "id": "M2IOumsgcqSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apart from storing the data itself, PyTorch tensors handle the intermediate results of gradient computation by setting *requires_grad* flag to True at instantiation time. This is required for model training.\n",
        "\n",
        "At the end of a forward pass through the network, a single scalar (*loss*) is used to compute the backward pass which is initiated by using the *backward()* method. During the backward propagation, gradient vectors are computed for all tensors which where involved during the forward pass.  \n",
        "\n",
        "It's possible to access the gradients for all nodes of the computational graph by using the *.grad* variable of a tensor. The network optimizer uses this variable to update the values of the parameters (model weights). \n",
        "\n",
        "What you need to remember :\n",
        "\n",
        "- Tensors you are differentiating with respect to must have `requires_grad=True`\n",
        "- Call `.backward()` on scalar variables you are differentiating\n",
        "- To differentiate a vector, sum it first"
      ],
      "metadata": {
        "id": "bOJFl35yc_o8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-qxIBeglG5p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "c6a99130-14e7-4e16-f596-be8ddf2cc12e"
      },
      "source": [
        "x = torch.ones(2, 2, requires_grad=True)\n",
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f2999c0034b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we require a scalar to apply the backward method to it\n",
        "x = x.mean()\n",
        "x.backward()"
      ],
      "metadata": {
        "id": "5buOeXPNfJLA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "91972ded-7c30-4a3d-e759-d299b226c788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e49f60657ae3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# we require a scalar to apply the backward method to it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Differentiation accumulates gradients. This is sometimes what you want and sometimes not.\n",
        "\n",
        "**Make sure to zero gradients between batches if performing gradient descent or you will get strange results!**"
      ],
      "metadata": {
        "id": "r81jqyTlOuaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a variable\n",
        "x=torch.tensor(torch.arange(0.,4.), requires_grad=True)\n",
        "# Differentiate\n",
        "torch.sum(x**2).backward()\n",
        "print(x.grad)\n",
        "# Differentiate again (accumulates gradient)\n",
        "torch.sum(x**2).backward()\n",
        "print(x.grad)\n",
        "# Zero gradient before differentiating\n",
        "x.grad.data.zero_()\n",
        "torch.sum(x**2).backward()\n",
        "print(x.grad)"
      ],
      "metadata": {
        "id": "QXbUM3JaO8nZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that a Tensor with gradient cannot be exported to numpy directly:"
      ],
      "metadata": {
        "id": "sv4aW5JVPZA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.tensor(torch.arange(0.,4.), requires_grad=True)\n",
        "x.numpy() # raises an exception"
      ],
      "metadata": {
        "id": "Xio95tV5PdA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reason is that pytorch remembers the graph of all computations to perform differenciation. To be integrated to this graph the raw data is wrapped internally to the Tensor class.\n",
        "\n",
        "You can detach the tensor from the graph using the .detach() method, which returns a tensor with the same data but requires_grad set to False."
      ],
      "metadata": {
        "id": "U4sPF9NZP_Ay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.detach().numpy()"
      ],
      "metadata": {
        "id": "rHEmD4nuQInW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another reason to use this method is that updating the graph can use a lot of memory. If you are in a context where you have a differentiable tensor that you don't need to differentiate, think of detaching it from the graph."
      ],
      "metadata": {
        "id": "gGRv_7oGQRJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network modules"
      ],
      "metadata": {
        "id": "wwfaPUBERmK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The network"
      ],
      "metadata": {
        "id": "jwGEVtRpoLmk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use a tiny neural network with two input neurons and four hidden neurons. As we perform binary classification, we will use a single output neuron."
      ],
      "metadata": {
        "id": "soFN2tNWp-8k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAXwAAAERCAYAAAB4jRxOAAAgAElEQVR4nOydd1gU1/eH39nZXZamgqBUNWCLJTasGAtYorH33jX2WGLE3hNLjMYWNRq7Jhr9GtRoVKzYexcVo0izIZ1l6+8PsvsDg4pK1Xmfh+RxZ+bOnTt3PnPm3HPPFXQ6nREJCQkJiQ8eWU5XQEJCQkIie5AEX0JCQuIjQRJ8CQkJiY8ESfAlJCQkPhIkwZeQkJD4SJAEX0JCQuIjQRJ8CQkJiY8ESfAlJCQkPhIkwZeQkJD4SJAEX0JCQuIjQRJ8CQkJiY8EeU5XQEIiNyCKYob20+v1WVwTCYmsQxJ8iY+SlwU+IiKCGzdvERoaytNnUcTHxwEClpaWFCrkiLurK8VLePJJsWJpjpNeABJ5CUnwJT4aZDIZgiAA8M8//3Di5Gl2+u/izJlzqJOTAVAoFOSztcXS0gKjETRaLdHRMWi1WgCUSiXVq3nh61Ofxo0aUtzTAwCj0YjBYMiZC5OQyCCClB5Z4mNAFEUMBgPnzl3gpyVLuX79BvHxibi5OlGmbGmqVCxPhfKf4uzshEKpQBRlYASDwYhGqyEy4gm37wZz6vR5bt4MIjQ8AgsLFeXKfEqP7l1p2qQxgiBIFr9ErkYSfIkPGpPr5uTJ0yxYtJjAwFNYWqlo17YFLZs2wsurAhaWBQAtGHUY9AYwGkn9UAiCgEyUgSAHLFAnPef27Xts3b6Lbdv9ef48Cu9aNRk1Yhj169UFJFePRO5EEnyJDxZRFAkLC2fytBls37GTMp+WpluXNgweNACQg1GNTqt9a1eMTCZDLpeDTAUY2LH9f8xbsIzg+w+oUqkiG9atxsHBAYPBgNEoPV4SuQdJ8CU+OARBQCaTsf/AQaZOm0Xk48d81a8H/fp0oaBjIXQadab52wVBQGFhTVxMFKvWbGblqvVYWKgY7zeGDu3bSqIvkauQBF/ig8I0MLvyl9VMmTYLNzdn5n0/mXr1G6PXxmSZq0UURUSFiof/3Kd91wFEhD9m0MB+jPf7VhrQlcg1SIIv8cFgEvtv/SawctUaunVpx+KFMxFkSjTqxGypg9LCAgSBocPG8tu2nXTp1IFFC+cDkl9fIueRwjIlPggEQUAQBMb6TWDrH9vp3qUdixfNw6BTo9Vkj9gDaJKTkcvl/PTjTNzcXViydDUqlYq5s2chCILk3pHIUSTBl/ggkMlkrP51LStWraFb57YsXjQbvVadI1a1TqdDJpPh960fjyOfsvm3rdjbFcBv7BjJypfIUSSXjkSeRxRFjhw9StdufWjZsgnLlszPMbFPjUwmIFfaMuzr0WzYtJVfli+lfbs2OV4viY8XSfAl8jSiKPL48WPaduhCQkIC507uQyaTodPpsr0ucrmIIMgQZAIIAgZdirAnJCbQrddQHjwIZd+enTg7O0uiL5EjSIIvkacRRZFhw0exe+8+Vi6dR8NGvtkyQCuXy5HJVYDS/FvU0xCio6OJjU8gNjYWp8KFKF7cE5lczoN/HlLRy5dWLb9k7a+/SOGaEjmClB5ZIs8iiiIXL19mw6Yt9OzWnoaNvsxysVeqVChV9jx//pwlixbTrFkTnAoXRhAEChYqimfJClSqXIu69b5g9twfkcmt0ajVFPukFNMmf8uRY4H8+eduZDLp0ZPIfiQLXyJPIggCMkGgW6++BAUFsWfnRhwcCmaZKyclzt6GwGOHmDptFkePnTCfy9ramqJF3KhTxxt3dzdsbGywsLCgWjUvKlYoj1arRRRFEhMSad6mB1qdgRPHAiS3jkS2I0XpSORJZDIZV69d4/CR44wc1p9CTs5ZZt0rVSrAyJBBA1m2fBUAlpaWTJ08jsZfNKTMp6WwsikEGAE9YACMGPUac5ZNvV6PbQEHundrzxi/aRwLPEGd2t6S6EtkK5KFL5EnEUWRvv0Hsf9AACH3zmMwGtDrM382q1JlxaOHD/jEsxx6vZ5KFT9j2ZIF1PD2AXRgTEav02bo3DKZDI1GQ6OmnXF3c2XzpnUYjUbJly+RbUiORIk8hyiKREREcvnyFb5s2gBBbpslYq9QKIiLiaaYZ1n0ej1+Y0dx8dJZanjXRKOOQqOORZOcnOFzGwwGrGzy8UXjepy7cIGw8HDJly+RrUi9TSJPcvb8OZ6/iKJ1iy+ApEwvXxAEBNGCNm27YNAbmP39NL6fPR+9NgmN+j3OZ9DRukUTomNiCTh0JPMqLCGRASTBl8iT7NmzD6fChfHyqoT+Xz95ZqKwsGbzpo0cDDjMkEEDGOs3GW3yi/f+ktBoNJQt74VDwYLs2fNXJtVWQiJjSIIvkSc5dfosRYu4UdDBJdMHPlOWQTQy1m8yAEuW/oheG5OJvnY99ep68yg0jJiYmAwvoC4h8b5Igi+RpzDNoo2IiKBSxXKkRMVkLgqFgqtXLhIaGsaXTb8AwTqTXypaqlQuz4sX0YQ8epSJ5UpIvB5J8CXyFIIgcP3GTWQykXJlSgNZEHcvUxJw4CgAAwf2IdPHCIx6Shb3IFmTTGhoeOaWLSHxGqQ4fIls52UXxttaz+HhERgBNzdnyJKFReTcuRcMQJlPS2PUZ/JLxWjAzq4AFhYWRERGZm7ZEhKvQRJ8iWxFFEVu377NrVu38PHxASB//vxvVUZMbCyiTMCuQIEsWklKxqPQR8gVcgpkwTkMBiM2NtaolBZERUVlatkSEq9DEnyJbMVoNOLh4cHo0aNp06ZNmm358uVDr9dTo0YN9Ho9pUqVwsrKitKlSyOTyXB3d8fGxprbt29hZWWNXJ51g5337z/Axsoa+4J2aDWZHQVkxEKpQJSLJCZmfkiphMSrkARfIlsxGAwolUr27NnDkCFDWLZsmXlbbGwsAAEBAQAcOZJ+nHqJkqVRqawwknUzVB+FhpHPxgYEG4zGzLbCTStfGZHJhEwuW0Li1UiDthLZjl6vx2g0snTpUhYtWvRWx7q7uzNs+DBiYmPQ6QwIWaSX8XHxODkVzpKyBQHUGi06nR4rK6ssOYeERHpIgi+RIxgMBgwGA8OGDeP8+fMZOsba2pqrV6/gVaUKGo2WJ0+eIGRyagIh1RukSBH3TC37/88hIzY6lqQkNYUKFcqSc0hIpIck+BI5hmkiU5UqVdi8eXMasU2PEydOUKCAHQXt7RGABw9DQchcP77CQsVfu3cAUK1qFSDzZ/Eik/H0eRQ6nR4X56z5ipCQSA9J8CWyHVEUzX9Hjx6lWbNmdOnS5bUzWdesWUOFChUwGo2ULFkCvV7HraC7ZP4wlIrft/4BQLt2rcCQnMnlA4jcC/4HlUqFk5NTFpQvIZE+0qCtRLYgk8nMFnxYWBgbN27Ez8/PvL1Xr16UKFGCCRMm/OfYhQsX0qtXL3O8viiKlCxVknNnL2VqHZUqFcF3r7J+wxYqVfoMT08Pcz77zEVJYOAZCha0o7inp5QeWSLbkCx8iSxFEATzRKvLly/TuHFj3Nzc8PPzw8nJienTpxMaGsqaNWsoW7bsf44fOXIkX3/99X9i4et87k145GNCHt5HLn9/uyUlTbGKHr0GArB08Y/I5PJMF+OUd56RE6fPUbp0KZRKZRbNJZCQ+C+S4EtkCSaXjUwm4+eff6ZcuXJUqlSJ/fv3U7lyZX7//XciIiKYNGkSrq6uAFy6lNZir1mzJj/++GO6i4TUr1uHqBfRXLx0DZlc8V51lcvlyJUFaN26JSdPnqZGjarU9PZFo1a/V7npoVBacP7cSRITE2nw78QzCYnsQhJ8iUwjtW/+/Pnz9OzZE0EQGDx4MCEhIQwdOpTk5GQuXLhAhw4dMBqN6PV6s6sm9Xq01apV4+TJkwDpWsB1anvj5FSYNeu3AJbvVF+5XESpsiMuLo6mTRqxc+duqlevyqnAQ+i1Me9U5hsRZKzftA1np8J88UWDrDmHhMQrkARf4r0xiXxiYiLbtm2jYsWKVK1alfXr11OyZEl27NhBZGQkixcvRqlUmkX+ZSG/desWAC4uLuzZswdIP8+OXq/Hytqahg18OHP2IuFhwSgUGXPriKKIUmWNUmVNQkIi83/4HmfX4uzddwAfn7oc3O8PsrfP75MR5HI5EeHhHD12ivr16pI/X37JnSORrUiCL/FOyGQys9DHxcUxfvx4HB0d6dChA1euXKF79+6cPn2aoKAgWrdujbW1dRprPj00Gg2Q4ut3cHB4o+j27NYVlYUFK35ZjyAq02xTWligVNmhVNmn+dNoNWz7fSutWrbBrqA734yZQFKSmgXzZxMQcBhra2s0yZr3b6B0kMlt+P0Pf0IehdKndw8AacBWIluRonQkMowgCGnWYD18+DCjRo3i8uXLAJQqVYrhw4czePBg8z4GgyHDonb58mXOnTuHo6PjG8XeYDBQvLgn7du1YeOWP+jcsRWlS5dAk6xBaWFBQMBhli37BZ1eT2JCAiEhody5ey9NGbVr1+Sbb76mZcuOgA5tcnSWCbBCISfsUTCrf91E/bp1qFihQpZ8RUhIvA5Bp9NJJobEGzFF2sTFxbFgwQKWLl3KkydPAGjWrBkTJkygRo0aQIrV+rauCpPf38vLK8NCKIoijyMfU6VGbb5o5MOqlYvRJiegsLBi/rwf+ObbiSiVSiwslNjY2FCp0mdUrVqFz2vX4tPSJXFxKwIIaJMTs9zSVqoKMGTocAIOB7Jpw69UqVxZEnyJbEcSfIlXkjpv/eXLl5kxYwY7duww/+bn58fw4cNxdnYG3k3oXz7f24qgKIpMnjqDNWvX88OcqXTs2BGdJp4XL6JJSkrE3t4em3y2gPW/R+gADQadzpzTJ6tRqqy5dOkSPo3a0rNHVxbOnyuJvUSOIAm+RBpSu21evHjBpk2bmDFjhtmar1atGj/88AOff/65+ZicFC/ThK427Tpx8vRZLp89gJOLMzqNBplMZn4J5ZSvXKlS8eD+Q1q27YmdfQEOH/wbQRAkwZfIESTBlwDSCv29e/eYP38+y5cvN28bPXo0Q4YMoVixYkDOivzLiKLI/eD7tGrXETd3VzasXkTBgvbmQeCcQi6Xk5CYSJfug7ly7QZ7/txO+fLlclXbSXxcSIL/kZPabbNv3z78/Py4cuUKAB4eHkydOpU2bdpgbZ3iEslJa/l1iKLIgQMBdOvZFx+fz9m4ZgmiKEOT6YuXZAyFQo4g5qNz125cvX6baVMm0q5NK0nsJXIUKSzzIyR1SOWDBw8YOnQogiDQpEkTrl69Svfu3bl79y7BwcF07949TUhlbhN703UANGzoS8vmTTh67CQ+jdujTk5GqVJle52UKhWxsfF07NSVk6fPk5wYRz4bS3N9JSRyCknwPyJM4mg0GgkICKBBgwZ88sknLF26FFdXV+bPn09sbCzr16+nePHiZpHPjVap6VqSkpIICAigc+fO5MuXj19WLmf40IGEh0fQul1vbt68jVKVfYuMKFW23L1zj47dBnLh0hW6de5ARHgojRt/weTJk9HpdJLoS+QYkkvnA+fl2PkffviBZcuW8c8//wBQu3ZtZs+ejbe3t3mf3CrwqTl06BCrV6/myJEjhIeHm38vUaIEd+7c4fCRo3Tr0YcCBfLzzchB9O7dHwzxWebXV1ooQbDk6JEABg8fR0JCAt/PmkHnTu25f/8+zZo149atW3z22WcEBgZia2uba91jEh8ukoX/gZI6ednRo0dp164dgiAwZswYoqOjmTNnDsnJyRw/fhxvb+//5LXJDaTOzaPT6fjzzz/p1KkTgiDg6+vL5s2b04g9QLdu3QCoX68u1y6fp2yZTxn17VRatmnHxUtXUapsUSqVb1xsJSMIgoBCoUCpsuf69Zt07d6Hlu168cknRTn49246d2qPXq/Hw8ODmzdvMnnyZK5evUq+fPlYtWqV2bUmIZFdSBb+B4ZJQKKjo/H392fKlCk8ePAAgBo1ajBhwgQaNGiA6l/fdm4SeBOma0hKSuLkyZOsWrWKvXv3EhPz5oRmjx49ws3NDb1ejyiKJCdr2LhpM/MXLAKMeNeqxsivv6Kk5yfILazQa9XvFPsvKizQa9UE3w/h17Vb2LFzDxYqFW1bt2Tq5JSc/i+XK4oiZ8+epX379oSEhNCmTRtWrlxJwYIFc+V9kPjwyLOC/7Kr4mXedxJQXiL14iIxMTGMGjWKDRs2mBfvGDRoEAMHDuSzzz4zH5NbBUYURUJCQujZsycnT558KxeMSqUiKSkpzbWZ2ubRo0csWLSELb9tQ6FQUKNaFRr61qV922YUsHcF9KRMytIDRjC7WoR/k9iL//7JiX4RxsGA42zb4c+Zs5eIjo6mW5fODB86iJIlS7y275leZvXr1+fIkSMIgsDdu3fx/HchlI+lz0rkDHlG8FOLmoknT54S9eIFSYmJaLVaBJkMlUqFjY0NhQsVwsoqbdrcD8ln+vILb/369Xz33XcEBQUBULlyZYYPH07Pnj3N++RWkX8Z02DsunXr2Lx5M8ePH8/QcVOmTGHq1KnpXqdJaLVaLQt+WsLBg4e4fuMGGo2WIkVcqVK5AhUrlMXN1ZV8+WxQqSzACBqtltiYOB6FhREUFMyxE2cIC4tAr9NRseJnNGzoS99ePXF0dAAy1samuuzatYsWLVoA8NVXX5nnPeSV+ySR98j1gm96OJKTkwkLD+fEiVMcOnKERyGPiI+PJzExAa1Gi8GgBwTkCjkWFimiX6CAHZUqVaChrw9lypTB3t4OyPsPlKlNHjx4wIoVK1ixYgUvXrxAEATatm3L9OnT+fTTT4G8+5JL7duOiIjg8OHDzJkzhwcPHhAbG5vuMefOnXtjLh5TuWq1mvv/PGD3nr2cOXOWyMePiY6JRZ2kBlK1l5DyT5VKha2tDU5OTtSsUQ3f+vUoW7YMlpYpRsW79ClRFImKisLX15fLly9TqlQpDhw4gLu7e57voxK5k1wr+KYHMyExkQ0bN7PLfxfXrl9Hq0nGysqKUuUqU8SjJM5uxbApYI+FhQUGg56E+Diinz/h4f073L91nUcPgzEawc3dncqVKzHwqwFUrlQRyFtimFoAT5w4wdy5c/H39wdSrP2FCxfSuXNnHB0dgQ/HpfXyl0ytWrU4derUf/ZzcXEhLCwsw/f05S/G+Ph4QsPCiYp6QVx8HMnqlMXLlRZKbG1sKGhvT+HChbCzszMfkxltbLqvo0eP5scffwRgy5YtdOrUCcj7xolE7iLXCb7pAYiIjGTmzO/x37ULo8FAhWq18KrlQ9XavpSt/Ak6Heh1YNCnuFtNz7gggEwGohwUCoiLgZOHDnD22EEunz1OaMgDSpUuzdfDh9GubWsg9z5UqUUpIiKCLVu2MHr0aPP2tm3bMnbsWKpWrWr+Lbdey/tg6hNFixYlJCSEb775hh9++CHNPpMnT2batGnvfP3puQzTIyva13TuyMhIPDw8SEpKokqVKpw9exaZTPZB3lOJnCFXCb4oimg0Gpb+vIK1a9eh0WipXseX5p37U6zEp1haglYLureYLS/IQGkBRj3Ex6k5+OdvHNj1G6EP/6GKlxfjxo6hcqWKucraT23VXr16lQkTJrB7924AHBwcGDRoEP3798fd3R34MEXeREqkTTINGzbk+PHjjBo1ivnz5+Pi4kJERIR5v7t375oni+VVTKuGDRw4kA0bNlC4cGE2bdqEr69vruqfEnmXXCH4JoF7+vQp7Tt25e6d25T9rApfT1uIZ6miaDQpIv++/V0mgqUVxLzQEuD/O7/Mn4JMrmDQwK/4ZvRIIGfFM7XbZtWqVSxevJirV68C8Nlnn+Hn50fnzp3N+3zoImBqjzp16nD8+HFGjBjBggULAJgwYQLfffcdACVLluTatWsoFIo878YyXfOWLVvo0qULAN988w3z5s0DPuyXu0TWk+OCb/qcXbd+I1OmTiN/ATsG+31Pw5a+xMemWPSZf06wtIb4OFgwcRiH9/+Jl1c1Vq5YhlPhQtn6UKUW+YsXL7J06VJ+/fVXACwtLenRowcLFiwwDw5+KL75N2FqF3d3d0JDQ81ibzAYkMlk3Lhxg/Lly2M0Gpk2bRqTJ0/+YMTQZADpdDpq167NmTNncHNz48CBA5QuXfqDf9FLZB05KvgmsZ86fSYbN27Co2QZ/OaspJBzQZLVWX9+8d8FHg/++Qc/z53IJ598worlS/H08Mhy8Ug9uWjv3r3MmjWLixcvAlC8eHFmzpxJ06ZNsbW1BT4uy04URdRqNY0aNeL48eOMHj2aH374wSx0JkEsVqwYDx8+TDPZ6kPC1Ed++OEHxowZA8DSpUvNS0h+aNcrkfXkmOCbHtqFi5Yyc+YMfJu0Yty85chkoM3GNOaCAPntYN+OfSycNgobW1sCjx4yZ4jMTFIPDCYmJjJ37lx+/PFH4uLiAOjcuTNDhw6lVq1a5mM+tof6ZTeOyWf/8peNKIr4+fkxZ84cc1qIDxFTnzlw4ADNmzcnOTmZLl26sGnTJuDj6x8S70eOCb4oisz7YQFz5syhfY8BDJ8yneQkyKn+a5sfLp+5yrSve2BlbcOuP7fjVLhwpjxQqd02x44dY/To0Zw/fx5ISfY1dOhQhg8fbt7nY/1kf9mNYxL79NrDlNp51apVzJw584MXPlPb9O/fn1WrVgHg7+9P8+bNAUn4JTJGjgi+KIocOBjAsOEj+LSCF98tX4M6CXLaNa2yhIunzjF5aBeqVa/B71vWYzTyzj7z1At/L1myhEWLFhEZGQnAF198wYQJE/D29kYQhI/GN/8qTG6chg0bEhgYaB6ofN3LT6vVotPpsLKy+ihekKb+tHHjRnr16oVer2fKlClMnDgRuVwuib7EG8l2wTeFXn5W0Qu7gg4s3Pg3KksFOl121uLV5LeHTT//ytolsxkyeBDfjB75Vg9Samv+xo0bTJ8+na1bt5p/GzVqFCNHjsTNzQ34eAZhX4epzT7//HMCAwNf6cZJD9PL8mPB5Aq9f/8+TZs2JSgoiAoVKhAYGIiNjc1H+3UokTGyNT2yyX/ds3d/jEYDY79fjnW+3CP2ADFR0LFfH2rWb8x338/m2vUbb0xhKwiCOY1vTEwMv/zyC25ubpQrV46tW7dSpUoVAgICMBqNzJ8/3zzAqNfrJbH/t23d3NwIDAxk9OjRZjdORtrmYxM303iFh4cHt2/fZuLEiVy5cgVbW1tWr14tpVyWeC3ZKvgymYzde/7iROBxOvYbTsnyJUlOys4aZIzkJBgwejoeJUoyddoMdFpdupk5TUIvk8kIDg5m+PDhFChQgAEDBhAWFsbXX3/N/fv3OX/+PD4+PhgMhlyXcz4nMblxateuTVhYGGPGjEkTjSPxakz9aMaMGZw6dYoiRYrQr18/2rdvT1RUlCT6EumSbS4dmUwGAjT9shWhYWH8dvg8yclgzKUGrsoKdm7YwOLvx7F69Wq+bNLYLNSpH6aDBw8yduxYc0ilu7s7U6dOpX379uaQSknA/oupDWvXrs2JEyfMPnvJxfX2mNqybt26HDt2DJlMxt27d/Hw8JDaUyIN2WbhC4LA779vIyjoFn1HTEIQcq/YA6iToMtX3XF2dmXOnHlgNJrdNo8ePWLkyJEIgkDDhg25ePEiHTt25ObNm4SEhNCnTx9sbW1z7cLfOY1JoFxdXdOIfUbdOFmNIAhp/nI7JkPk6NGj+Pv7YzAY8PT0ZNCgQeavUAkJyEYLXxRF6vs2Qo/I/HW7kMvlOR6V8yYsreDPzZtYs+g7Fi/6CbsC+Zg0aRL79+8HwMnJiREjRjBkyBBsbGwAKTzuTZhy3Tds2JATJ07w7bffMmfOnBz9CkpPEE0vnvRcebn5HouiyPPnz/Hx8eHq1auUKlWKgwcPfpAT0yTenmwR/JSY6YdUq16TfiMn0nPIYOLjMlA5GVioQCaAVg3adPqrhSWIMtBrITmTJ2yJIsS8iMGvf2uSEuK4cikldr569erMmzePzz//3Lyv9DC9mdzixnk5M2Z4eASXr1zh2vUbREY+Ji4+HnWSOmXNWqWCfLa2ODkVpmSJElSr6oWbm2ua8nLbvTe188iRI1m4cCEAv//+Ox06dAByX30lso9sE/wZM75j3YYNrPxfIPaOBdG/ITJHkIE64gp/7T+B3mCkatshlLSD5FS5dazzw/F1SwiOEilYtiFNPi+e6aKf3w7mjZ/E0b//pHnTRsz67juUSiUghVS+DSYRMmW5HDNmDHPnzs1Wy968+ElSEidPnWH3X3v5c9ceYmNiMQIF7e1wdCxIvny2WFqoQDCi0WiJjonlydNnREVFg9FIvvy21Kv7OR3atcO7Vo1c+XVneqlFRETg4eGBWq2matWqnD59Wkq5/BEjz+oTyGQy1Opkzl+8QJkKVSlgVxBDBvqa0Qj5ChXk0rKh/BVipPDfMez8YzwqAfRGUFrBo90zGTBkEpCfuYd6IWSBbiQlQo36Tdi1bR2t27ZHqVRKD8tbYnLjNGjQgIiIiGx345hi16Oiotj6xw7WrtvAkydPyV8gH9W8KlGvTk2qVP6MIu6uKJQKlAqF+eVgMBjQaLQkazQ8f/acE6cucOLUWa5fv8HBgCO4u7nxRaMGDBs6iPz58+caI8BUB2dnZ54+fcrgwYPZsGEDrq6ubNq0yRw1Jo0vfVxkuYUviiLPnj2jRq3Pad11AH1GjCQxIYPHqkBzdx+NajQhBui5IZhpLT2IUoNl/E3alSzLDR20XHSb+f1KEf0i8+svEyEuJpZB7erTpnULZkybIgn+W2ASTtNKVSbLPruE0XT+pcuWs3b9JoKD71PVqxI9urenTu2auBcpCSSDQYtBr8eI8d803MaUlQ4FAYF/XxqiDGRKQOT5s3AOHznBug1bOXHqHK4uzgzo15thQ3NfYrPUM3S7d+8OYL4PkLvqKpG1ZIvgHww4TPfu3Zm76n9UrF4VTXLGj1flh/C1I2k0dCFQibUPLtKgMPj55GfjqViKd1/Ln8t6khwHhiy4EkFI+dqY9nUPlDItWzauRy6KGCTL6I287MYZO7AUiLwAACAASURBVHYss2fPzhbLUiYTEAQZZ89dYNCQYYSFRVC/Xm1GDOtH9Rp1gSQMupTUDO+CKIqICiVgyf3g64yf+D3HT54hf758LF20gPr164LRiD4XWPuQNuWyt7c3Z8+epUiRIuzfv59SpUpJ1v5HQraEZQYF3UEQBD4p8elbz6pVx4DngAV0rWoJXGL6lN85s2YoG0/Fgk09fl7cE31C1og9pIi9yhIKu7gTHhZGQmIiQjqRGxJpMblxatasSUREBH5+ftkm9qIoIggyvp/zA7369Eej1bL+18VsXLuE6jWqolFHoVEnvbPYQ4pVrFEnoVFH4eFRjN82r2DVz/NxcS5Mr74DmDvvR8hFIZGmGbpyuZzTp08zd+5cQkJCKF26NMuXL5dm6H4kZItyPXj4EJkg4ORukyH//cvExIDfqj9xB+6v60v3MUsBGLbyDz4BNFn8RSqTQWFXN548eWpe3Fri1ZiEw9fXl9OnTzN27Fi+//57jEZjtog9gN/4ScyZ9yOVK5UnYO82GjVulOKPV2f+QguaZA1ajZYmTb/gL/9NVK9ehQWLltChc7c0dcoNmNw3Y8aMYf/+/SiVSgYNGkS3brmvrhKZT7YIfkRkJAUdC/OuhrFBA0LxhixbPxlIIDEBao4PZGzngkS/aTxAAPl79mG9ARwLuxEfF0dcRuJJP2JMguHs7MypU6fSuHGy2mdvOnenrj1Y8cuvTJ4wmo3rV+PoWBCNOj5LXzZGoxGNOgFRFNm6ZROzZ07k7NnzfF6vAc+e5a5UB6YUHw0bNiQ5OZk+ffqwadMmBEFg9+7d5gmGEh8e2SL48fHxWFrbvNfMWpkBHj98aP73wyN7eRgH4usmQgqgJJEHjyLe60qNRrC0skYQBOLjMjji/BGS2o0TGRmZrW4c0wSpLt16ce7cBSaNH8WokcPRaeLQZsU6ma9Ar9ejTY6mZ8+uLPlpNg9DHtG7b3/i4xNynYiarP3Vq1ezbt06RFGkefPmTJ8+Hb1en+vqK/H+ZIvg63U6RPm7R4DKLCD59jb8Jq0DCuHqriD85CxmLD6HY4FXH2djB5dnt6dV+x4ky1MmcL0TRpDJTWF6UkRDepjEwcfHh9OnT+Pn55dtbhxTCoSp02by1779fNW/B6NGDkebHI8hqwZ3XoPJ2m/WrBXTJo3hyrXrDBqSssBNbkvUYMrY2qNHD4KCgihZsiRTpkzBy8uLhISEf8dDclutJd6VbBF8K0srkpMS3623y8DBBgY37sAzwPu7/Zzw34IdsG9CLf73AKwsXjpGAFt7uPn7OHos+gsXd9d3F3tSInU0SUkYAQuV6t0L+kAxib2Tk1Masc+u3DgymYyVv6xm4eKlfDt6CN+OGYk2OWtdOBlBo46id+9+jP/2a/b+vZ8FCxcjy4VWs2lA19PTk6CgICZMmMDly5exsbGRUi5/YGSL4NvZFyA66vk7ia7KCvbOaMapKLAq2pOp/SugK92WbwfXAHSM6zaExxqQm65EAGsrOLZ4CH2HLkQugvCeWdoEGcTGRKFQKHB2dgaQLJ9/EUWRxMREatSowePHjxk/frxZ7LNDcEVR5HZQED+v+IXa3jUYP+5bdJrEbBd7uVyOUqVCqbJCqbJGqbJBaaFEp3lB/z5daNW8KQsXLeHMmXO5VjxNyf5mzpzJiRMncHd3p1+/fnTo0IEXL17k2npLZJxsEXwXFxcSE+OJj0+xljOKaAnqa38yZtYeQGDoLwsoJoeYWGgxZiOVbCH+4jJmrjqFrX3KMVZyPcsGVaX/t38xd1cwzet4oE5+vxVWZDJ4GhmObT5b1q1by+HDh3n8+LHZ8nndX3rJtz4UUrtxzpw5w7hx45g1a1a2uHFSM2bsBBKT1MyZNRGMmiz/qhAEAaWFBUpVAZQqe5Qqe54/jyLg4GF2bN/B+rXr+GnhQo4eDUSuUCATZfy8dA4ymch3c+YB6Sdlyy3o9Xpq1apFSEgItWvXZtu2bTg6OvLgwYMPvk9/6GR5agWA0qVLAQL/3AmmqKcnuoyMocnAQh1K/w6tiAW8hwbQu6YdsbEpEyC1dp6s2b2cinUHEjC5Pmu8ntG1ug2axCQ8Wy/g9vra2CYb2R6f+F51FwRIVsPTiEdo1MmMGjnyP/sULVqUEiVKUKpUKcqVK4ebmxtOTk64urri5OT0RssoL056MV1T4cKFefLkCePHj2fWrFnZnhvH3383gSdOMe/7yZQtVwmNOjrLzicIAgoLG8DIubOnWLt2Ezv/3EV4eGS6+/fq1Z269Rqi1cagVNnw/czxjB0/g1Wr19Cvb+8sq2dmYBq0PX78OP7+/rRs2ZJPPvmEQYMGsWzZMkRRlGbo5kGyZabtzZu3adCoMd9M+5HGbduizoAGK6zg7E99GfvjdhIc6rDvkj/2iaBNZbzls4el3b34dX8wQuWhHNszA300yJWg00I+pYahTYpynvrs3rsZC83bT9CSyUCdlIxf/zYU/8SN0SOHc+vWba5du8bdu3e5desWDx48ID4+Pt1oEFtbW+zt7SlXrhylSpWiRIkS5peClZUVNjY2WFlZpXvu3PoiMLlxTJZ9Toi9TCZDq9PRum0H1ElJ7PxjDVZWluj1WWPdKy0sQJCzds1aJk2eQWhoWMrvSiUuzk58XsebmjWq4u7ujq2NDZZWKoq4u+Ho6Iher0cmE5DJRJq37k7w/YfcvnE5197flzGlR/Hx8eHatWuULl2agIAAXFxcJNHPY2SL4EdHx9D4i6aUr/Y5I6fOJqNzXxRWYGuTEpIZ/RzSq6l1AbBUpsTqv4hOsf4BECB/Jgi+XA6RYeF81aYOY8Z8w5DBA9PdLzo6msjISMLCwggPD+f+/ftcvXqVO3fuEBQU9MrQQCcnJ1xcXHBxcaF06dKUKlWKkiVLUrZsWQoWLPjG+mV3si6TZV+9enXOnj3LhAkTmDlz5jvXQxTFdzpWFEWOHj1O+07dmDN7Er17dkOTEUviHVCq8hMeep/uPftx6NAxAGrXrsnXwwZTp04tCjl5/LunBtCTkofHiF6nSyOISpUVe/bso3vvoaxavpQ2bVrmGcFML+Xy1q1bad++PSDl48krZLngm3J4fDVoKJevXmfBxr1YWlpm/eInmST4VjawddVqls6dzLkzJylatOh/OrfpGt9EUlISd+7c4fr169y5c4c7d+7w8OFDwsPDCQ8Pf+VLwcPDg1KlSlGqVCnKlCmTxmVUqFChN543syzJzHbjiKLIxo0b6dq1K4IgvJVoiKJI+45duXT5KveCLqFNjssSa1mpKsC+vf40adoagHbtWrNy+SLsCroBagw6TYZTNMhkMgwGA63a9UYmiuz443cUcnmuyK6ZEUwpl8PDw/H09EStVlOtWjVOnz791vdPImfIch++6SFs1LABe3bv4s71S1SpVeutEqjlFIIAeh2cOLSHIkWKpCv28P9hbW/C0tKSChUqUKFChTS/q9VqkpKSSEhI4PHjx9y9e5cbN24QFBTErVu3CA0NZf/+/ezdu/el+gnY2tri4ODAZ599RsmSJSlRogTly5fH2dkZKysrrK2tsbS0TLc+byuwCQkJ+Pj48OTJEyZOnMiMGTPe+2USExODra0tV65cwdPTM0PWviiK3Au+z+2gO3Tq0AIgi8TegqBbV8xi/9uWtXTs1B0MCWjUUW9dnsFgQKmyolnTBsyavZDg4GA+LV06s6udZZjui4uLC0+ePGHw4MFs3LgRV1dXNm/eTL169fKMm+pjJVsGbY1GI23btGLIsOHs37mZ2r55Q/Dlcrh78w4Pg+/So3u39y4vPYEVBAELCwtUKhV2dna4ublRpUqV/+z3/Plzs8soIiKCu3fvcvXqVe7evcvt27e5f/9+uud0dXXFxcUFZ2dnypQpYx5HKFu2LAUKvGbW2r8YDAbz14uPj4/ZjTNjxoxMicbp1KkTQ4cOpXjx4nz33XeMGzcuQwOCZ86eIyExgSZf+IIx8zuTTCYDg5FmzdsBsHnjajp26ok2+cV7XbNRp6FF88ZMnjaXAwcP5SnBN6HX67G1tWXDhg00atSIHj16UL9+fSnlch4gW9e09Rs/kZ07/ZmzagfFShTPWLTOu5IJLh27gjBuwEBC7t5g9+4/KWhvnyMdOaMuo9jYWO7cucONGzfMLqOQkBDzS+JVlrNp3KBUqVKULVsWZ2dnnJyccHNzM48jmNw4s2fPZuzYseZjTaL/LiJouq6WLVvi7+8PpAxyX758GQ+PFL94eu0tiiKdu/bkUWgo+/dsQalUZrpbRKkqwKyZU5k4aQaDBvVn2bIVaNUvyIyHRamyp1rNz3F0cGTPrh15VhxTp1yuWbMm58+fp2jRovz9999SyuVcSrYK/u1bt2nXqQuVqtdh8k8Lic2CBUvMCJBfoaZvHXvO0IBDx/yx1GZc8BUKiAyPZHgnXxRKC5YuXkidOnWA3Gu9vCr8MykpicTERBITEwkPD+fu3bvmcYRbt24RHh5OfHz8f0RTLpfj4OCAwWDgyZMnVKxYkfHjx+Pp6Ymjo6PZZaR6xezjjLSTKIrs2bOHZs2apfl90qRJTJ8+Pd1yRFHEs1Q56njXYM2vK9CoY954nrfBJGKFnT2JjY3FaNSi1yZk2n1XqvIx+hs/jp84w64/t1O4UKFc26cygmngfe7cufj5+SEIAsuXL2fAgAFA7n1ePkayTfAhpWMMGzGK7Vu3svav8zi7O2Wpa0cpGji9fyvPcKZBo7rIMtjvBAGUKvhh/DdcPXuMSxfPAdC/f39WrlwJ5N6QyZfJ6NfB06dPiYyMJDQ0lIiICO7fv8+dO3fYtm3ba48rUqSI2WWUOvS0bNmyWFtbv/G8RqPRnAsnvbIvX76MnZ2d2bcviiIxsXF4lizDpHGj+PrrwWjUmZvQTqmy4MTxk9Su04ghgwewZOmKd/LZv7p8SzZt2srUGT/w2+b1VKlcKc+LomlA9++//6Z58+ZotVq6d+/O+vXrAUn0cwvZPmVu1vSpOLm48f2Yvmg0KUsIZhUavYwaTTvRomnGxR7A2hYO+vuz93+b6Ne/Hzqdjnbt2vHLL78gCAK//PJLnskvYhpQft2f0WjE0dGR8uXL06RJE/r06cPMmTO5evUqAJMnT8ZoNPL8+XNOnTrFqlWrGDNmDK1atcLR0ZGHDx/yv//9jxkzZtCtWzeqV6+OjY2NWcjLly9PmzZtGDduHBs3buTgwYNcv36dFy9emIV+yJAh/6l7SEgI9vb2fPvttwipFhO5cycImSCjTJmSwPvNok4fJQH/hl+2bt0CyORwT6OeYkXd0Rv0hIVFZG7ZOYQp5XLjxo3RaDT06tWLDRs2IJPJ2LNnj5RyOZeQrRY+pFj5f/rvYvCQYXQdMILew0dkeI3b7EChhLCHoUwZ3hWFKHLi2CHztu3bt9O3b19iYmLw9vZmy5YtuLu7f1DWiyiKxMfH4+Pjw7lz55g8eTLTpk0zW+LpYXIXJSQkEBoammYc4caNGzx9+pS4uP+uIyCXy7G1taVIkSLY2toSGBj4ynoVLVqUgIAAPD092bX7L/r0H8iRA/+jbJmSaDSZOxikVNnRvVsXNm76jUcht3F1cUarzbwXi1Kh4PqtIDp0GcCIYUMY0L/vB9eHANauXUvfvn0xGAxMnz6d8ePHSzN0c5hsF3yTi6Fdxy5cuHCeUVMX8EWbL4nNuhnxGUYUU+Lu+zX/gojQEDZsWEuN6tXQ6/XmesfHxzN27FiWLVsGwPTp05k0aRKQ9z9bTQ9q1apVOX/+vNmP/rpQyYy6jCIjI81RRqGhoQQFBZlDTx+mWufgTSz66Sec3YowfMRozgTuxdXF6b2WKkwPpcoeX996nDhxmmePH2BpZZmp91YulxPyKJRW7XvTuWMHxo4Znef7zsuY+sW9e/do0qQJ9+7do1KlSpw4ceLfeTh5wyX6oZHtgg8pwmIwGGjbrhOnTgWyYN1uKlSvTFIOWvqiPCV785xxQzl7PICfFi7gy6ZfpDtgCHDjxg3atWvH7du3cXBw4M8//6RWrVpA3hR+03U5Ojry7Nkzs2WfGQ+myb/7OkqUKMG9e/cyVF5hJxccC7uwd9dmXJwLZ4ngF/f0IOrFC6KiwtEmqzNVnORykYiIx7Rs24sWLZoxeeL4PNlnMoKpX02cOJFZs2YB8Ouvv9K7d0ouoQ/1unMrOZL2LiW3iIyff15M2fIV+O7bAZw5ehLLN4/xZQlyORj0sHDqGLauX0HbNm34sukX6Vq1Jr932bJluXr1KgsWLODZs2d4e3vTpUsXIO+tC2py43h5efHs2TMmTZqUaWIP/+/fTe8P4Lvvvnuj2CsUCurUqcPhw4eZPHkyMTGxCFloIYaFRfyb48gyCyxRAb3BgMFoRBSzZSpMjpE65XJgYCCurq706dOHTp06ER0dneeelbxOjuU51ev1OBUuzPZtv6HTaZnj9xX7d/pj78g7r337LlhYgiDC5KFd+Nv/d/Q6HSuWLyUqKuq1rgq9Xo9CoWDEiBHcuXMHLy8vtmzZgr29Pfv3788zg1SmOtarV48LFy4wefJksxsnOxYcv3fvHhMmTHjlPhUrVmTt2rUEBwdz9OhR6tWrh6OjI2q1mmSt9q3Sbb8N6mQ1Lq7OWVK2IIBarUGr02Fjk0NWTjaj1+vx9vYmNDQUb29vfv/9d/OAv5RyOfvI0VbW6/UUyJ+fS+fP8GmZMswa058lM39AbzBikX42gExDJgObfHD9/GV6Na7B7asX+XnZMu7du0dMTAwlS5ZEq9W+VrRN1kuJEiU4d+4c+/bt48WLFzRu3JhKlSoRExOTqzuz6docHBzMYm+y7LM6v4upTRo0aPCfbdWrV2f+/PkYDAYuXbpEz5490wyOOxUuhBEID49EyOQwL5lMwJSCr4SnJ2TKVKu0CIKM6OgY1ElJuDg5ZXr5uRXT/QsMDGTHjh3odDqKFSvG0KFD00RhSWQdOa5Eer0elUrF+jWr6dylG//b9Atj+7TixsWrWKhSJkBlJoIAFiowGGDxjBnMGNUbrVbL+vVradmiGZ6enixZsoTnz59TqVIl4M0uGpPwN27cmMePH9O9e3cuX75MgQIFWLBgQa7szKIoEhcXh5eXF8+fP2fKlCmZ6sZ5E4IgMGrUKPOAbZEiRRgzZgzBwcGcPn2aUaNGmRNypXb/mPYVgH8ePEr5PMtE5Eortv+xGYCWLb8EsmCiiEzG06fP0BsMFHYqnPnl52JM97J169Y8efKEcuXKsXTpUsqWLUtERESue04+NHJc8MGUm8OGRQvnM3PmdP4JDuLbPq1YOG08EaHh2DumiPS7GnOCAHJFSny9pRXs/98OhndpxG+/LqVataoEHNxHrZo1zAs6DxkyhOHDh3Pjxo238svr9XoKFSrE+vXrOXnyJACjRo3Cw8PD7K/MDR3aVIf69etz4cIFpkyZwtSpU7NtpaqUNRJusmDBAvr27UtgYCAPHz5k7ty5eHh4pPH5v4zBYKBo0SLo9Dpu3goCMtkiQMGmLVuBlBTIRp0mk8sHkHMr6C42VjYflYWfGr1ej6OjI9euXWP48OHcvHkTFxcXtm/fnmuekw+RHInSeR2iKKLVapk770fWbVhPYnw85StXp1XXr/i0YjWcXG3RaUGnSxloNemT6SKEf/8jE1IibxRK0CTDvZtBnD22nx0bVxAfH0/ZcuX4buZ0vKpUBtJGC5iiSoYNG8aSJUvo2LEjv/3221vlbTd12JkzZ5rDNr/66iuWL18O5NxMXVO9ChYsSFRUFFOnTmXKlCnZvlLV7du3KZ0qcdjbnF8URRo1aY5Oq+XQwe1okzWZUnelyoqLFy5Qxas2n9euxbGjf6PVajO9XZQqe75s0YqEBDVHAvZ91CGKpmctLCwMDw8PNBoN1atX59SpU7k65bIp6ix19Fl2L+35LuQ6wYf/F6UHDx6ydv0G/v57P48fP6agY2GKepTEq7YvnqU/w6GwMwqlElFMWTfUaDRi0OvRabUkJsTxMPg2QVfOc/7UEZ49icSg11K+fAV69+pOo4YNkMvlr+xQpjrUqVOH48eP88033zBv3ry36oCmznzv3j3atGnDtWvXyJ8/P/7+/jmSl8fkxqlXrx4XL17MEbFPXZd3vXZRFPl+zjw2b/md7b+vpkRxj1euJfA2ZYoKBWXLVOHmrdvcuHaWMuXKoFFnrktHJhOQiwqci1WgR7dOzPk+JVQxtwpbdmHqmwMHDmTz5s04OzuzZcsW6tatm2teiC9/dej0+pSQ4H8nJcrl8v/sk9vua64UfBOmxouLiyfg0CH27P2bgIAAEhMSkAkC1jY22DsUxtLKGrlCicFoQJOURHxcDM+eRqLX69Hq9FSsWJEmjRvSoEEDKnxWHsjYSlGm85cuXZqgoCD+/vtvGjVq9NY30VTOokWL+PrrrwFo06YN27dvB7LH2jfVoUqVKly8eJFp06aZUybkpgU4Mvopv2/ffoZ8PYqpE0fTvXvn91rtKmX5QmuqVavCuXMXadnyS3bu3J2p+XPM51JZcuDgYXr0GsqkCWNxdLCnVatWKNIZrMptYpHVmO79+vXr6dmzJwBjx45l9uzZQMaT8WV03zfx8qTC8PAIzl24yKVLl4l8/JgXMTEkJCRiNBiQiTJsrG2wK5AfZ6fCVK5ciepVvXBwcDAfnxteXLla8E28LAIPHj7k+o2b3L17j7CwMJKSktBqtMhEAQsLFfnz58fTw5MyZUpRuVKlNMe/baObjnV3dyc0NJSDBw/i6+v71h0qdedp3bo1O3fuBGD16tX06dMHyLoH3HQN9vb2vHjxwiz2OdUBXzc7V61WEx4eTlRUFBEREURGRhIUFERQUBAPHjzg1q1b6PV6nj59StcefUiIjyfw2KG3FmeZTEAU5QiiLcF3r9KseXtuB92hefMm+PvvRKdJzPQXoSAIyEU5PfsO51lUDBZyo7kfAHh7e9OiRQu8vLwoXbo0Li4u/ykjN4hGVmLqG1qtlpo1a3LhwgWKFSvG33//TcmSJV97/aIosmbNGnx8fF65WFFGEEUZ/zqHOXHyFHv27mPb9p28iInBKAi4ODjgbG+HXT5bbCwtEWUydHo98UlJRMXGEf7sOU9evACDgUIOBenYvh1NmzSmqlfKOhc5aWTlCcF/mbcZ0MmMxhVFkadPn1KoUCFkMhlnz56lSpUq79ShTHX39/dn4MCBRERE4O3tzcqVKylTpkymdwbTp3LdunW5dOlSrhB7rVbLuXPnCAkJ4f79+wQFBfHPP//wzz//kJycTHx8PMnJr3alDBkyhCVLlvDT4qXMmDWHowd3ULZsaTTJrx9gFUUZokIJWKDXxvP8eRTfjp3IuvUpUTm9e3Vj9aplGI3GTJ+9C6BQyHn4MBTfJh3o3rUTXw8bQmBgIHv27OHw4cPcv38/zXmLFStG3bp18fHxwcfHh/z582Nra2ventu+zjIT02z8OXPmMH78eARBYOXKlfTr1w949ToJI0aMIDw8nK1bt77z85mUmMS+AwdYvORnHoWHY6lQUuaTYvhWqkCtsmVwc3RApVCiVMgR5P9OnDMaMer1JGu1JGu03I+M5OSNmxy+cpVb/zxAazBS3KMYI4YNoX69uigUipxZWyMvCn56mAZPskrERFHk0KFD+Pr6Av8/W/hdblpqC7dbt25s2rQJgO+//x4/Pz9z+ZlRZ4DKlStz6dIlc96fnBQKmUxGXFwcy5YtY9y4cW99vK2tLS9evEAURcIjIvBt2BRfn89Zsmg22uRk8/1XqqyAtHn69doYzpw5z649f7H3rwNcuXoNgMKFCrFyxSJatOqIXhuTZQ+iUmXPuPHj+HnlWm5fv4TTSxE6MTExXLlyhQsXLnD48GF27dqVZrudnR1ly5bF19eX+vXrU7du3XTP86G4gkxjYPv27aNZs2bo9frXplwWRZEWLVqwa9cunj17RsGCBTPcFqZnZfeev1i05GfOX7lKURdnBrVoTpOqXrh7FAO9HnRa0BtIWaie/48aMQ3emv4viqCQgyDj/r1g/E+fYYX/Hp5FRVGrelW+HTUSb++a6V5HVvLBCH52IIoiBw8epGHDhqhUKsLCwrB/j1Ww/j/d7x26devGuXPnsLOzY9OmTTRp0gR4985gKtvOzo7o6Giz2OcGl0Dq3DqjRo1iwYIFGT5227ZttGvXDr1ejyiKzJn3IwsXLWHnH+uoXt0LjToJpcqKJYuXMWz4aCyUShCE/3wxODg40KiRDxPHf8unZasA6vcaB3gTSgsLLly4QqeuX9H4i4Ys+enHV+ZpSo3BYODo0aPs2rWLs2fPcu3aNWJjY83bbWxsaNCgAb6+vlSrVo3SpUuTL1++NGXk9S8BU7v07t2btWvXIggCu3fvpmnTpkBa48vLy4srV65Qt25djhw5kmG/f2hoKGPHTWTPgQCqlvmUwS1b0Lrll5CYBBpNiti/K3I5KJWgkLP+j//xy569XL1zh+4d2zN18kQcHByyTfQlwX9LRFFky5YtdOnSBS8vL86dS1kc5X1umKlDz5o1i4kTJwIwYMAAZs+ejZ2d3TsNEsfGxlKvXr1c4cZ5FabrfvToEW3btjW35avw9vYmMDAwzWIoer0ez1Jl+bR0Sfy3r0UQZMiVKn5ZuYqJk6bj7OyEtZUVdnYFqFSpAl5elalQoTz2dgXIV6AwGNVoXuM+ygwEQUBhkY827Trzz4MQtm/bjMcnHm81CGkiLi6OiIgITpw4weHDh9m3bx9Pnz41b1coFHh6evLll1/i6+tLxYoVsbOzS7MqWW7rBxnB1A6//vor/fv3x2AwMHPmTPz8/MzboqOjKVGiBM+ePQPg4sWLVKr06sVlTF/aR44eZ9ykKTx+8oRRHdvTr3FjVPltQa3O3IsQBFBZEB/1gkU7/Vm5aw/F3N34cd5sKr+mnplaBUnw3x6Tr/Cnn37iyy+/ZPfu3e/9mKnXSgAAIABJREFUEJms3uDgYHr16kVgYCD58uVjxYoVdOrUCXi7KAWTG2fGjBlMnDgxV1t5pjofOnSIJk2aoNGk74u/d+8enp6eadpBFEU2bNrMxEnTGDG0PyNHfYNGHZWuSydlsRQtRr3OPLkrO1CqbPj9t218NXQMkyeMZdTIr9/bFZiaR48eERQUxJEjRzh06BCnTp1Ks71o0aKUL1+eOnXq8OWXX1KmTJn/lJGb+4cJ0/XfvXuXJk2aEBwcTKVKlTh16hQWFhaEh4fj6upq3t/Hx4eAgP9j77zDmry+OP5JXghhg6gouLeA4sCiOHCLinvXvWcdrVq31Wrde++90Tqoe+86cQCioqLgBpkhjCS/P9I3P1BQwIDY8nkentYk7773e8977rnnnErxXkslEiRSKXu89jFs5K/ky5ObJT8N0rpZIqO0S/EzC6kULC04duI0Py1ZRnx8PJvXrqZWrRqZPhjnCH4GEQSBvn37snbtWgYOHMjy5cv1NkEM4OXlRd++fQkPD6datWps2bKF4sWLf/YY4rZWVlZEREToxD47W3TiOR84cICePXvy4UPKhY5//PFHtm3b9knnFUWgT/9B7PH6k4NeG3Gv456p7pn0IJObcvPGTdp37ktNNzc2blij1+eR2iAQGhrKmTNnOHHiBFeuXOHevXvJvs+bNy/Nmzendu3aODo6UqFChU/2kV0HAbHNjB07VheyuWvXLgoUKED16tWT/fb69eu4uLgkazfiPZs2YzZzFi6iXf26rP15OMjl+rfqP4eJCdHv39N99nzO377NjCmT6dO7Z6YaIjmCn0HEKIIGDRpw+vRpnbjq62EJgkB4eDjjxo1jxYoVwOeLrYhuHHd3d3x8fL4bsY+MjGTUqFGsXr0aQRDYs2cP7du3TxatYmxszNOnT7G1tU01OuPDh3A6dOpCaFgYOzavoFSZUsQrY7Pycj5BJpfzJPApPfoMI+TlK25cvYS1tVWWvFl8HIocHh5OSEgIR48e5eTJk1y7do3w8P9XHTI1NaVixYrUqVOH5s2bU7hwYaysrJKtD8huk8GCIHDx4kU6duxISEgIpUuXJiAgINlvRF9+0n4gCALzFy1h8bKVuFd0ZtOk8ZCQ8HV++ozyT5RPq/G/4fv0CVMnTaBjh3aZdq9zBP8rEDuVpaUlkZGRbNy4ke7du+tV9AGOHj1K586dCQsLo3LlymzdulWXlkCcvARtKuE7d+4wffp0xo0bl+0ttOPHj9O5c2fev39PrVq12LJlC4UKFdKteRAR5yA+d1/FibcfqrlTqlRx9nttxMra+puJvkwuJzEuDrfazXn/PpS1q1dQt477NxHN1ArQ+Pv74+/vz8WLFzl48CCBgYHJvi9ZsiSurq7UrVuXOnXqUKRIkU/2kZUGhWiZi/cw6duNWKUtJU6dOkXdunV1feXWbR8aebaknmsVdk6eoI20+ZaDmYEBmoQEmoybxL3Hj7l09iSFCxXKlLaSI/hfiWjpFy1alOfPn3P+/Hlq1qyp14clCuTcuXMZNWoUgC6/j4g46EybNo3x48dnS8te7KChoaEMHDiQPXv2IJfLWbNmDV26dNH9rkmTJhw5cgSAsmXL4ufnl+aV0Tdv3aJ7z34o4+I4dmg7xUuWIiFOkaX3QiY35+EDX37sPoTIyEjWrl5OrZo1spWF/Ln5gEOHDnH27Fnu3LnDw4cPk33v5OREvXr1aNCgAaVLl6ZEiRKf7CMz2p5EIiE2Npa9e/fSrVs33ee3b99mzJgxHD9+PNVtCxYsyPPnzwEIDHyCZ6u2FLLNy7HZM7QTqdnhuRgaEhURQb3R44hPSOCo937y5cun9zaTI/h6QBAEXrx4QZkyZVAoFPj6+uLg4KDXhyV20AcPHtCvXz8uXLiAnZ0dCxcuZP78+Vy9ejVbu3HEQWvXrl306tULhUJB69atWblyJXny5NHdK0EQGDJkCMuWLQPg7NmzuLun3TIWBIG/r13jp2EjUcQqmDJxJG3atEKVEItKlfk5/g1kxpw+fZZxE//g/ftQNq9fg1v1atlK7D9HUldQQkICHz58wM/Pj3PnzrF//34CAgKIjf3/W5OFhYXuDaBu3boUKFAAKyurZPvU5xuvRCJh+vTp/PTTTzRv3pyzZ8+madtdu3bRvn172nfszNWbt7mwaB6FC9iBHovTfzUyGT7+D2g6dgItmzRm2ZKFeu/LOYKvJ8QMkGXLlsXGxoYXL15gbKzf4tficUCbb6RHjx66xjBnzhxGjhwJZC9fqzhQqdVqOnXqxO7d2tTDmzZt0llqH0fdLFy4kBEjRlChQgVu376dobDU96GhNGrcnLdv39G5U2tmzpgMSP9JmaB/69PQyBgwYPaceSxaspq8efOyctkiXF1/yFbPIz2k5ApKSEjAz89Ptzjs5MmTvH79Wve9TCbDyckJFxcXPD09qVu3Lqamn1b1yugq2A4dOujaUHqwsLBgz94/6TtwCGM6d2Lgjx0gKird+8l0rK0YM2cB246fZNWyRTTxaKRfwzFH8PWHIAgcOnSI5s2bY2BgoMvgmFmi7+DggL+/v+7zBQsWMHz48Ew5ZkYQz3P58uUMHjwYgN69e7N27Vog9eXxXl5etGvXjqdPn1KkSJGvSmEx+tfx7Ni9B3Nzc8aPHUandi2RGhihSlB+9T3SpmqQA2r27j3IzDlLeBz4lL69ejJzxu8ZXomdnUltPuDmzZucPXuWo0eP4uvry6tXr5J9X6dOHerUqYOrqyvOzs7Y2n5a+OVL1qwgCIwbN44ZM2boPrOxsSE0NDRN5+5cpRol8tvi9fsUbX71bPYWDGjzuiOh0agxhISGcf/2df1GdeUIvn4RBIFFixYxfPhwfvjhB86fP4+RkZFeX2sjIiJwd3fnzp07zJo1i/r161O/fn0+fPiAk5MT+/bto2TJkt900lYQBF69ekXHjh05f/481tbW7N69W1fS8HNpqQ8cOMDp06dZtGiRXha0/f33NUaPm0hQ0HOKFS3MkIG9cK9VFZvceQANifHxab5PWreNISAQGf6BM+cusXHzLm7fuU/RIkUYPnQQLZo3++w1/tv4eD5AoVAQGRnJ5cuXOX36NCdOnODx48fJ7rG9vT3u7u54enpStWpVbGxskq0QTqntCoLAkydPKFu2rG6thpmZGXPmzMHNzY0DBw7g5+eni0BSKBQo/wmzzG9fEOu8+VkzfDBulSpBJi+2+yqMjTl4+gx9Z89nybzZtG/XRm9tKUfw9YzY+EVLpHnz5hw4cEAvo7QoYM7Ozty9ezdZ7h2AQYMGpSmEMzMRz1Ec9AAGDhzI3LlzMTEx+eJ9kEqlvH79Gmtra4yMjL56wErqUtq0ZSsLFi0lJOQlxYsVpVrVSrRs1oTqbi7I5DaA6p+/f3KlIFqyUkAABDSJUZy7eIWTpy9w4uQ5nj4LwsrKkjGjR9KhXRtMTU2z5RxKVvG5dQEPHjzAx8eHQ4cOceLEiWTP1tzcnEqVKlG9enXq1q2ry1mVEnnz5k22uhhg3759tGrVSvdvMevqgwcPeBEczJIVa3AqUlgblaNWZ0/rXkQiQa1S03DMOIzMzDmwdzeCINWL8ZYj+JmA2OjF3B9iRM3XCIEopBYWFkRFRenEXtyn+H1ERASVK1fWhdhduHCBGjVqAJkr/OKrfkBAAO3atePevXsULlwYLy8vXFxc0nV8qVSq9+pBSV0RJ06eYt/+A1y9ep0XwcFIpQIF7PNTqUI5Cha0J0+e3Jibm6LRaFDGKnnz7j1Bz4J58PAx/g8eIpVKyGVtTY3qbrRt05LGHo10x/mvWPXpJbUMt8eOHePMmTNcuXKFGzduoFD8f8GcTCajQYMGeHp64uzsTJkyZbC2tmbYsGEsXrz4k30lrTGRlKPHT9CtzwD2TZ1EDZfK2du6FzE2Zu/xk/SdNZeDe3ZSo4abfnL85wh+5iAKjIeHB8eOHWPFihUMGDAgw/7o8PBwatWqxb179/jjjz8YO3ZsigNI0mIrI0aMQK1W07lzZ+bNm5fqwqWvJaVyjqNHj2by5MmYmJhkOxHUDY6RkVy/fpMrV//m9u3bvHsfikIRS1xcnG7hlyAIyOVGGJuYYGVpQRWXylSrVpVKFZx1xS3+yxb91/DxfEBERAShoaFcvnyZQ4cOcfHiRV6+fKn7XhAEnJycsLW1TTUM087OjsePH+sCJgRBoEv3Xly9foPHB7y0aRO+B6QSUINd+060b96MhQvm5gh+dkcQBOLi4ihSpAivX7/mzz//pGXLlul6cKm5cT7nnxc7UmRkJHXq1OHWrVsAHDlyBA8PD0A/lqh4nODgYDp16sTFixexsbFh586dX/TVf2tScj3EKpWEhYUREx2DMk4JSJDJDDE3M8M6Vy5MjI2T/T5H6PVHaq6gV69ecefOHf7++29OnTrFhQsXvrgvQ0NDrl27RoUKFXjz5i1NWramhkNZFo0eCdHRmXH6mYOlJR1G/sqLD+GcOuKN3Fj+9S7OHMHPXETBFi0Z0deYnkRo5ubmREdHf+LGSev2J0+epEOHDoSFhVGpUiW2b99O6dKlga9Pv5w0akKskfs1+/2WpBaBAtk3r8y/mdQGAVtbW96+ffvF7adMmUKjxk1p/2NXNowZSd0fqmhTHX8vyOXsPnacsWs3sG3DWqrqIcQ35TpzOegN8QH5+fkhl8tp3bo1jx8//mLVLtGNU758eaKjo/njjz/SJfbisVUqFfXr1yc0NJQuXbpw69YtypQpw7x580hISEhX9TDQdkJBELh79y6Ojo7MmDGD0qVLc+vWLSZPnoxGo/kuxR7QZdBM6S9H7LMesS0lbU8lSpRIk9gDTJ48mQkTJpIvd25K2OfPHitq00NiIg6FCmMglXDf11cvu8wR/CxApVJRtmxZTp8+DWhz3kRGRqYqtuLnos9+5syZjB07NsMTmWKH2bJlC1euXMHW1paRI0fi5OTEvXv3EAQhTcIvCAJSqZSRI0fi7OyMn58f06dP58GDB7q84znCmIO+Ed+6evbs+Um+n5Rwd3dn0aJF+PreJ0/evJgaySiUJ2/mpjzODNRqCubNjbW5Bffv++lllwZ62UsOX0SlUlGtWjV8fHyoUKECZcqUITg4WFfEQ+RjN87s2bMZNWrUV/uLxWNUrVqV169f6yZYy5cvz8CBA5k1axbm5uafnQi+ePEiHh4exMTE4OLiwtGjR7Gxsclxd+SQqUilUmbMmMHGjRs/+S5Xrlw4OzvTsGFDmjdv/kmuf1//AMoWKgCWFhAR+cn22Rq1Gss8ebA0NcU/4OGXf58Gciz8LESlUuHs7MycOXN49eoVlStrq9iLgiq6ccqVK0d0dDQzZ87Ui9h/fA4qlYoJEybw4sULatWqxYoVKyhQoAC7d+9GKpUms/bF/+/evTs1a9YkJiaGNWvWcP36dV3N0ByxzyGzEASB/fv3M27cOEBb68HV1ZUpU6bg6+vLkydPOH36NGPGjNHlr0rqBnr1+jUF8+T5/tw5IgYCua0skqWv+BpyBD+LUavVjBw5kv79++Pj40P37t2B/wtrzZo1uX//PrNnz+bXX3/Vezy6iEqlokCBApw7d441a9YQGRlJhw4daN26NTExMTo3z5UrV7Czs2Pz5s1UrFiRwMBA+vTp81376nP4PhBXa48aNYqRI0dy+PBhXr16xdWrV5k0aRIODg5YWlomm3sREd1AGo0GazNT0HP+pCxDrcHGwpKwVAoDpZccl04WIwr4ypUrAVi1ahVSqZQNGzbo3DizZs3Su2WfEiqVColEQp8+fejTpw8DBgxg1apVmJmZsXjxYi5dusSuXbuA5CsZc4Q+h6xApVKRP39+Hj169MnnX0IikegW0xoZyjLj9LIIDXKZDGWcfqKLciz8b4DoAlm5ciU1atRg48aN2NjY6Nw4o0ePzrIY76SW+sqVK7lx4waGhoYMHTqUXbt24ezszNu3b2nVqlWW1oHNIQcgxYiptKDRaBAjbBNU2SgFcgZISEzE0FA/tnmO4H8jxIZ79OhRZDIZYWFhDBo0KFPdOKmRNPZ86dKluiyfgC5BG3w+Tj2HHLITYv9Rq9VEKWLhe223EikR0dFYmlt8+bdpIEfwvxGiz97Ozo74+HhMTU1Zvnw5586dy1JRFUMt9+3bh0QiYePGjXh6eqLRaAgPD6dq1arMmzcPiUTC4cOHP5nUzSGHzEacTxL/0tM/8tnaEvT2DXyvbVaj5mVoGPb2dnrZXY7gfwO0Rbe1qYwjIyOZP38+QUFBANSvX5+7d+9miagKgkB8fDz16tWjTZs2gDb9wqFDhwBt2cQrV67oCk40bdqUhg0bEh8fn+6Ol0MOGUEQBNasWcOhQ4cICQkhMjJSZ3SkZQAoUawIb0I/QOx3aOVLpcSGfSA6NpZiKdQTztAu9bKXHNKMKOQ1atTA19eXuXPnMmLECGxsbDh27BiJiYk4Ozsn+21mnIMgCOzduxdjY2NOnz6Np6cnUVFReHh4JIt60Gg0tGvXjnfv3tGoUSNOnDiBXC5n69atOdZ+DllCvXr1aN68OQUKFKBMmTLUrl2bnj17smnTJoKCgj4ZAJIuJCxbpgwfoqN59/49pJCmIVsjlfLyXSgfoiIpW7a0XnaZk0snCxEboampKQqFgrlz5/LLL78kS3F85MgRmjRpgpmZGSEhIVhYWOhtolT0wUdERODq6kpAQACGhoZcv35dN8h8rjAJaH361atXJyYmhnz58nHt2jUKFiz42W2/FyQSic5aTBrWJ/43J1Hat0EQBN68eUORIkV0BU0+xs7ODmdnZ+rVq4eDgwPFixenVKlSXPn7Oj9268mmsaOoVbnS95VLx8iI/WfOMmLZKrasX00Nt6+vjfydDXnfL6Ibx9HREYVCwezZs5OJPWgFs3HjxmzatIno6GiaNm0KkGICqYwcXyKRsHTpUqytrQkICKBfv36EhYXh7Oz8xQgI8XtnZ2fCwsL45ZdfeP36NYUKFWLixInJcvJ/TyS1CKVSKfHx8cTEKIiIiCAiIpKYmBji4+M/sSJzyDpUKhW2trYEBASk+puXL19y5MgRRo4cSZMmTShXrhx58+alR/dumJgYc+G+Lxh8Z1HoMhknbtzCJlcuSpcsqZdd5lj4WYAoEI6Ojvj5+TFv3jx+/vnnVFMSCIJA//79Wb16NW3atMHLyyvDYZqiVR8XF0fVqlXx8fEB4OrVq7i6ugLpt8zF6wkMDKRixYpERUVhbW3NtWvXKFGiRIb2mZUkFewPH8K5fOUqt27f5umzIEJDw4iKjNKmR9aAzEiGhYUFNja5KGBvRxUXF6q7aUvyiajVqmxdQOnfgiAIHD58WGcIpYWffvqJoODXvHjxglt7d8FHlbKyNWbmFG3RGrcfqrBt8wb9pDTXw2nl8BmSunGSir1arU41JYFarWbVqlV06dKFvXv3Mnz4cKRSabotfdGqnzp1KnK5HB8fH13Yp6ura7rimpMible8eHEiIyNZvXo1Hz58oGTJkrRq1UqXGC47TeqKWT4FQSDg4UMWLlpCrToNKFHaiZ59BrB9xy5ev3qFpYUpDg4lqe5WBTc3F5zLlcXa2pw3r19z8NBhevUdQGnHCjhXcmXOvIX4+fkjlaY/eiSHtCM+O4AmTZpw/vz5NG03ZMgQFi9eTLu2rXj28hW3Ll0B2XeyCEsu58iZM0RGRdG9S2e97TbHws9ERDdOjRo18PPzY86cOYwcOTJN1rqYVK1u3bqcP3+emTNn8uuvv6ZJoEWr/smTJzRr1gw/Pz/s7e05ePAglSpVAvRngYs5y8PDw2nRooWuMx44cIDmzZvr9VgZRRQLnzt3WLp8FZcuXUGjUWNvn58mHvWp4VaFYkULY2JijJGRka5IuRY1qoQElEolSmUcz4KCOX/xCmfPX8Hf/yEGBgY4ly/HqJEjqFSxAvDtr/ffQtI3saioKB49esSGDRv466+/ePr06We3XbhwIcOGDQPg3bt31GvcjFpODiz9eTgkWWeSLZFIQCKh8/QZPHsXxlHv/Zibm+VUvMrOiI3VwcEBf39/5s+fz4gRI9KVWfLjSd5t27bx448/fvbBi9vMnz+fX375BYDBgwezdOlSIPPESDzun3/+SevWrQGoXbs2f/31FyYmJum+bn2sNBYHI6VSybARozh89BhxcXH07tGZNq2a4FKlIiADTRyqRJVuYvbj44qTuTpLUyoDNNy9c4c9+w6xdfteEhIS8GjYgN+nTiJ/vnxo1GrUOX6edPHx3EhCQgJ79+5l3bp1XLlyhZiYGADkcjlOTk7cuHEjxf2sX7+enj176tqcIAiMnTCJnXv2cWb+LIoULJi9Rd9Iho+fP03HTGTEkIGM/Hm4/gy0HMHXP2LDNTY2RqlUsmDBAoYPH54hERMEgYSEBIoWLUpISAh///03P/zwaeUb8ZhPnjyhXLlyKBQK7O3tuXfvHtbW1lmSwjhphaK+ffuydu1aAN3bCXx5wBEEgT59+rB27dqvauTi/Vi1ei0zZ89DMDCgc6fWjB01FLmJNagVJCQkZHhQkUi0pfSQmoA6lj9mL2b7jr28eBHCrBnTGNC/D5Bj7X+OjytaKZVKrl27pqsB/SFJwjA3Nzc8PT3p168fNjY2DB06lCVLlnyyz1OnTlG3bt1PUo7Hxiop4VCO+j+4sGnSBPhnjibbIZGAzJCWv07g/pMnPPa/p9e+m+PD1zOCIBAWFoaDgwNKpZK5c+dmWOxBKxiGhoZcunQJQ0NDqlWr9knFLPH/x48fT/HixVEoFEydOpXg4GCsra2zLIVx0rw8a9as4cqVK5QuXZoxY8ZQrFgxrl27pouGSQlBEIiIiGDdunU8f/48w9Ew4nZ9+g1ixqy5lC/nwF9/bmXK5DHIDA2IV4YTHx//VW8QGg3ExycQr4xApUpk3JgRHD64jebNGvPHzNl069GHN2/e5kT0pEDSqKh3796xZ88e2rRpg6WlJe7u7vzxxx9oNBo6duzI/v37+fDhA5cuXWLs2LG6yfK7d+8m26eRkRFHjx79ROxB24eMjeX07dmDS3fvc/ziZZAnr0+cbTA1Yd3+Q/g8fMjwoUMA9Np3cyx8PSIIAhqNBicnJ/z8/HR+RH2M0IIgcOfOHSpUqICtrS1BQUEYGRkB2kLP5cqVIzQ0lFy5cnH37l3s7e2Bb2dhJrXeJk+ezNSpUwEYNmwYCxcuTPHcBEHg6dOnFCtWjCZNmvDXX39lKIIoMjKS7r36cf7CJQb1787vU8aCBOKVcXq4stSRyWQgNWDuvKUsXLKK/PlsOX38MOZ6XEvxPfLxoBcQEMDOnTvx9vZO5pYpXbo0HTp0wMPDg2rVqiXbJulalfDwcEqUKEFoaKju+8DAQIoVK/bZdSRKpZI69T2IiIrCb9M67RfZ6bkYGhKniMGhZ3/KliqB94F9aDRq1HpM7Zwj+HriYzeOKPb6zHopCAK7d++mQ4cOmJmZ8e7dO4YOHcqaNWsAWLFiBQMGDACyjytBnEAODQ2lW7duHD58GNCWW+zSpQvw/3MVBIFTp05Rv359AHbu3EmHDh3SfC2CIPDsWRDtOnbm7bt3bF6/FHf3uiTGR2VZkRaJRIKhkTWXLpyi76BR2me2Ywtly5bJNs8ks/lY4N+8ecOpU6fw8vLizz//1H1uZWWFh4cH7dq10837iHwuZPnly5c6g8be3h4fHx9y586dJnfh7ds+tOvcjYqlSrJnyiRt2cPsUMBHEEiIi6PR2ImEvH3L4QN7KV68uN7bTI7g6wHRjVOjRg38/f2ThV7qe3WmIAgsX76cwYMHY2xsTGxsLC4uLhw4cAA7O7ssS6ucXkQRWL9+PT/99BMKhYKmTZuyZs0a8ufPj0qlSnZtoB08FQpFmt6QxLxA7nUaEquMZdH8abi7uxOvjM70a0sJmdyYu3fu0bpDb+zy2bJp4zqKFin8rxX9pG90SqWSkJAQtm/fjpeXVzL3S9GiRenevTstW7akePHimJmZAakL/McIgsChQ4do3rw5JUqU4OzZs9jb23/xvibN9Lpx0xYmT59BX8+mTOjbExSxGb1s/SCRgEzGiAWL2XX6DGuWLaZpk8aZ0la+s6Vn2Q/RjVO9enUePHiQzI2jT+FN2qHEghCxsbGULl2a69evA9nHqk8JsdhKr169aNGiBcOHD2fr1q3Y2dnpBkiAt2/f6raJjY1l5syZjBkz5rP7Fu/LgEFDeRESwvpVC3B3r0u8MiLzLugLxCtjKe9ciVXLZtN/0Eg6/tiVvy+f/6SG8ffKxxOuAKdPn2b9+vWcPXuWkJAQ3ed16tShV69euLm5UaxYsWTbZORenD59GlNTU10/SGkfKc2dHDhwgP379+Pj44PcQGD9kWPIDARGDxkI70P5JqvnpFKwsWHMjDnsPnOWHp1/zDSxhxwL/6sQxd7Y2Ji4uDgWLVrE0KFD9f6wxMZ7/PhxGjVqBEDLli2xsrJi48aN9OzZk/Xr1383QiJez9WrV+nYsSNBQUEUKVKEs2fPMmvWLFasWJHs9w8fPqRkyZKf9c+OHjuB1WvWs3rZHNp3aE+8MirTryMtyORWHD92mD4Df6Z1yxYsnD8n276FfY6UBN7Hx4dz586xcuVKHjx4oPvcwcGBBg0a0LdvXxwdHZNt87VtVBAEpkyZwqRJk5BIJDpD4uNze/78OXfv3sXb25uDBw/y6tWrZN8nJMTzy+hxbNqxk2Ht2zKlVw+tpZ2V4ZoyGWplLD8tXcmuU2cY2r8vkyaOy9T2kSP4GUQQBEJDQ6levToBAQG6OPvMEPvQ0FCdRWxsbMyyZcvo2bMnoLWezp49y7p16+jVq9d3I/rw/4m02bNnM3nyZKRSKdbW1skm4wDc3d05e/Zsih1BEAQuXrpMrz4DqONenVUrl5IQF5mlgioIAoKhIdoXZikg+ecvjnhlLDK5Mb//PoeVazezbNF8WrYb5almAAAgAElEQVRs/t08p+RpKD5w69Yt9u7dy9atW4mK0g6qMpmMmjVr0qFDB1q2bEmuXLl022X2ug+R0NBQ/P39OXLkCHv27OHFixepJlpzc3Pj0qVLJKpUTJj4G1t37aa+S2Vm9+tN3vz5tamUMxtTE4KeBjF0yXJuP3zI8MGD+HnEUNBoUGXinEKO4GcAcWGQg4MDAQEBLF68mJ9++knvE7QAly9fpnr16gBUrlyZCxcuYGxsjFqtRiqV6uLtw8PDOXToEJ6ent+NmMD/r/P58+d4eHjg7++f4u8OHjxIs2bNPilULZVKqehSFQ1w5rgXlhYWJCZmbkk7rcAbAXIA4pVhnD93iXu+vkSERxARFcX7d+9p7NGQH7t0QhUfhyAzopJLXczMTTl+xBu5XJ4tn9PHQhoeHs727dv5888/OXnypO5zc3NzunbtSvPmzalZsyYmJia67zJ7zYe4CO78+fMcPHiQa9eu8ffffyer1PY59uzZQ9u2bXX/nr9wMfMWLSVvrlysGTUcl0qVQKnMHGtfJgOZjNMXL/LT4hVExUQzddJ4enTrmjVrZXIEP32Ibhy5XE58fLxO7PWdqiA0NJR27dpx5swZjI2N8fLyokmTJkByq0nsoOKElLe3N02bNs2WYvIx4rm/fv2aTp06cfbs2c/+PmlWUXH7lavWMnnqNDZvWEqjhg0zdZJWkEoRZFaEh4Vw7PhJ5s1fzPXrt1L9/c8//8S8efOIV0Yhkxtz/vwluvYYQucf2/PHtKnZ4hl9XLZSoVBw4cIFDhw4kMyKB2jYsCEtW7akU6dOWFlZJdtPVl6LIAgEBwczatQoDhw4QGw6LPICBQrw4sULnXEmkYBUKnDbx4eRo8dx8/59mlarxq8d21O+YnmttZ+Q+HX+falUm6lTJuPK9RvM2rmbszdvU9utKrNn/kGpkiWy7P7lCH46EASB9+/fU6NGDQICAnQraPX1sEQB3L9/P61atQK0VaYOHTqk81emtt39+/epXLky8fHxBAUFUahQoWwhKKkhDpxTp05l2rRpabLKR44cyZw5c3QRPRGRkTTxbEn+/HnZuUXr99dnzHJSZHIj1Ikqfhk5ltVrNqD4J7LDLn9+KlVypm4dd6pWrYKtbV6diFpYWGBubqaz2mRyc3r2HsjJ0+cJDLiHTGb0zZ5RUpdLWFgYf/75J7t37+bChQvE/5Mz3tzcnC5dutC2bVsqVqyItbW1bvtv3bbE84+JieHMmTOMHj061bfDpMyYMYMxY8akuAZEo9EwZep0/jzkTUyMggZVKtO7sQcVihfFwMwMEhO1f2kRf+1SbBAE4qOiuBbwkNXeR7h49y65rK35sX07rQuHrL2X36Xgp2f1or7cLGKDKFu2LAEBASxZsoQhQ4boNeeLRqOhYcOGulfnv/76K0WrPrXzu3DhArVq1cLa2prg4GBMTEy+ecdMCUEQuHbtGh4eHsmWz6eF58+f6wqu7PHax4BBQ9mxdSUNG9YjXpk5vleZ3IrHD+9Rq3YjXr16A8CI4UPo1LEtVVyrATIgQfunUv2zYl+DSpU8I6rMyIjbt+/QuFlnxo8bxU+DB2XJ80lpUtPPz49Vq1Zx8uRJ/Pz8dJ+XLVuW/v374+7uToUKFZJtkx0nm5Nqwa1bt9ixYwdz585N9fevXr0iX758Kd73/ycdfMqa9RtYs24jMrkcp2JFqV+5IvUrVqCSoyOYmUCiShu/r9GgzdGgTXiGVAqGBvAhgqu+vpzyucPJG7fwfxaEFA1DBvanR/eu2OXPnyUunI/5LgT/4wb74cMHgkNCePPmLa9evSY8IhylMg4DQYqpqRl58uQmX7582Nnlp3ChQsn2lZEOJvrs5XI5CQkJOrHXR2fV5XxZtUq3aKpXr16sW6ddCZieTiYKqaurK4ULF+bZs2fAt7fGkiIIAvPnz2fWrFm6bKKpTa6lhIODA76+vgA0bNKct2/e4HPzb+KVYZlyvjJ5Lg5776NpM23N3ym/jWPS5KmAAJoYEtKRokFsxz92G8TrN+848tcBjOXyTOn0HxtFgYGBXL58WRc2KWJvb68Lm6xTp06ybbKjwKeGeL3nzp2jdu3aKf6mS5cubNmyJU3GE2hdiJu2bOXo8ZNcvX6DGIUClQZKFypISXs78tvYYGNhjiCVkpCYSFhUNCHv3hMQHMzTV6+QaMDK3Ixqrj/QtLEHHTu00x3jm62Az86Cn1Tonzx9yslTZzh56hTBz18QGRmJMi4OA0MZhoYypP9Y4ImJCSTExSGRaDA3t8DCwoJq1arRsH5dKleuhLm5OZD2Gy66capXr87Dhw/15sYRr+3Ro0f06dOH8+fPU6hQIdauXUuDBg3SdY4fn++0adOYOHEiLi4u2S5GP2kGQ6VSiUKhIC4ujsTERPz8/Hj79i1Pnz7lwYMHvH37lsDAQJRKJfHx8SgU2oRn69atw712HZo2b03fXj/yy8/DMsV3L5Mb433Im2bN25M7tw0Xzx+ndNmKJMZHZFikZXJjtm3fzcjRUzjw5x5+qFJZ7yUsAaKjowkMDGT//v1s3LhRN/gDODs707ZtW3788Ufs7OyQy7WTz9+TwCdF7Etz5sxh9OjRFC1aFE9Pz0+Sq/n6+uLg4JDm+y3eT7VazavXr7l924frN29z2+cOkZGRxMbF6XIySaVSZIaGGMvlWFlaUrlSRX6oUhnn8uWwtbUFMn8yOy1kS8FP2nAPef/Fjp27uXzlMnGxsVhb21ClVn1KODhjX7AoltY2yI1NEQwN0ajVxCtjUcRE8/7NS5489MXP5xp3b1xGKhhiX7AgzT096dO7B/ny5QM+L4SiZV+2bFkePnzI0qVLGTx48Fd3DNGCEGPoATp16sT27duBr+t4YuPv2bMnGzdupHfv3qxduzZbNDaRpOmG04JKpSIyMpJ3797x/Plzcllb43PPl5mz5rJ722oqVHAkPl6/ERUGBgbExMRgaW2PRqPh1o2LVKzsSrwy8qv2K2ZuLFyiMiOGDWbC+LEZFvyPJ1wB9u7di5eXF0eOHCEi4v8Lz1q3bk2bNm2oV6+eToBEsosxkBHEviRmzzQxMSEmJoYnT55QvHhx3e8sLCyIiIjI0LWm5BJTJSYSHhmJQhGLRq1CKkgxNTHFytIKiTT5M8lOA2m2E3zxAXrt3cdvU6cR9v49RYqXomaDZtRu0obS5QqQEA+qRG3eI7Ua0Pw/06kkiStN0E6MExMNl04c58KJg/x94SRKpZL6DRowY/rUVH1potgbGRmRmJjIsmXLGDTo63yuSQuBd+jQgYCAAEqVKsXu3bu/WEQ8PYhC0Lt3b9avX0+PHj3YsGFDtmp46SElYfNs0YaY6GhOHduNSqX/65LJrRk8aADLV6xm3tw/+PmXMcQr0zffkPq+c1G/URNUKg1nTh5J83NJSXguX77MsWPH2LlzJw8fPtR97uLiQrNmzWjfvj1lypRJts33LPBJEVctN2vWjCNHjujWa4C2YEqJEiV0K7c3bNhAjx499Pg2JUEi+TTra3bvY9lG8MXGfN/Xl7HjJ+F77x7FyzjSrGMfXGs1xMLKkPh4SMyAISeRgEwOGjUEPX6G9651nD/hjYEgpV/fvgwZnDzhmOjGcXNz49GjR3pZQSs2ziVLljBixAggcwuTiIOLi4sLN2/eZNq0aYwfP/5f0dkFQcCuYHFatWjMsqWL9J5CQSqVYCAzInfugiiVcURHR5MQ90FvHVkmN2fS5N/56/BJ9u/bTcGCBdJU1Aa081fnzp1j165dHD16lPDwcO0+ZTI8PT3p1KkT7u7u5MmTR7fNv+GZf4wgCLx+/Zr69evj6+tL//79WbFihc4FI5VK8fDw4NixY1haWhIUFISlpeW/8l6kh2yRS0fn4ti0hd+nTQcN9Bo6Hs9O3ZHJtLUKFDEZ379GA3GxgAQKFivCsN9+p2WXAcwZN5CZM2Zw0NubfV67MDM1BbSjtCj2omWfUXdI0oVFTZs25f79+9ja2uLt7Y2Li0umuVrE0MUbN25QuHBhJkyYQIMGDVIsnpLVSKXSr7qfb9+9JyExgfLlHdFGx+gXA5mMyxcvExoaxvRpkwGVnq22RJzLO7Bt515CXoZQsGCBZN9+POH6/v171qxZg7e3N5cvX9Z9nitXLsaPH0/Dhg1xdXXVpcuG7G9pfg2CIBAdHY2dnR0ajYbx48czbdo0IPngVrNmTY4dO0aLFi1yxP4fvnkBFLFx9xswmFGjRlHVvSGbjt6kXe/uqFUQq9Bj9lKN9g1BEQ12Be1Ztf8go/9YSvCLF1T5oRp/X9NOcBoZGfHo0SOWL1+uc+NkpPOI1zZhwgQKFy7M/fv3mTRpEq9fv8bFxSXTC5OIDfzBgwdYWVnh6urKxYsXv2lRDqlUysuXL3VFMDJS/PvRw0eAhDKlSgCZ0Ynl7N/vDUDt2jUAPefR16goYJcfgJAQbY6XpPfj3bt3HD16lI4dOyKRSMiTJw/jxo3j7t271K9fn507dxIfH09oaCjTpk2jVq1aGBkZ6YrLZ7S9fg8IgsC5c+cwNzdHo9Fw4MABpk2bprvupHTs2BGAiRMnfotTzZZ8UwtfFJ427Ttxx8eH9j0GMmjcZBITIPrr5sa+SEICRHyAes2bUaSkI1NHdOOnYSOIi1WQmJjIokWLGDhwYIasAtHnfO3aNVq3bk1ISAgVK1bEy8uLYsWKZan1pa32Y4yPjw9FihShXr163Lx5Eycnp29i8Yj35ZdffmHhwoXUqFGDXLlyAWmPYnjzVhsLb2ubB40qMwZMgYBHj7XHyJsXjZ7vk0atwcLSHGO5nMh/VrI+e/aMI0eOsGbNGnx8fHTtw97enlatWtGzZ09KliypizKDf6erJjVEl++6devo00dbPjK1cp+gvTfFixfHzc2NEiVKZJuAhW/NN7PwRbHv0bsfVy5fonO/nxkyYTLxyoz56TOEBpQKKF62GLPXHcDUwooYRSy//jqWoUOHZtiql0gkjBo1CldXV0JCQpg+fTq3bt3SVeTJautLpVJRuHBhvL29iY+Pp1y5crpzzWrUajWtWrWiWLFitGjRgoIFC1K/fn28vb11RcLFv9QIj4jA0MAAc1OTTLuXb96+xcjICJvcNnoXC41Gg4mJMRYWFuzcsZMSJUpQtGhRBg0axO3bt3Fzc2Pp0qUEBAQQHBzMkiVLqFRJG1Kc1Ir/ryCWQ5wzZw59+vTB2NgYX1/fNLknd+zYAfCvfeNJL99E8MVIg569+nL0sDfjZq7gxwF9iY35NsVnlArIk8+GOesPUbJseQ79dYQbN26ly9UgipS3tzempqbMnTuXhg0b8v79e8aN06Y8/ZadVKVS6dI0ANjY2KBQKLJc9MWOJ64mVigUnDp1imbNmiGRSKhZsya7du3i/fv3qbp9EuITMDQ0RJNOV1B6CHkRgrmZKVbWKa/K/Do0GAgCMkNDbty8iUajYciQIZw9exaNRsPFixcZPHgwpUqV0rWb/5rIi4jts1WrVowePRp3d3cUCkWa4ulVKlW2TzGS1XwTwZdIJPw2dRrnz5+nY++hNGjZlOiob1N/QCQ+DgxlEsbPW4dUKmX4zyMJDg5JkyAKgkBUVBRDhgyhWbNmKJVKFixYwLFjx7Cxsck2PlWVSoWnpydr164lLCwMT09PgE9C/bLiPADd4JOUixcv0rFjRwoXLkzNmjU5ePAgYWFhSKVS3bMwNJSRkJiYqfc0OOQlllaWQGYMiBISVSqiYqIZOHAAgYGBLFmyBHd3908EPju0m2+F+LxdXV3Zv38/DRo00IVdplXEc8Q+OVku+IIg8CAggBUrVuJSow59fxlDTDT/D6T/hiQmQK48Zvzy+2Jev3nN8J9Hffb3ovV59uxZSpUqxbJly6hatSqPHz9m+PDh39yqTwmVSkXv3r3p1q0bZ86coUuXLulaBKXP8/D09NTVtf0YhULBxYsXP3H7ANjY5CIhIR5lrDJTz7ugfYEv/ygDSCQSYpVxxMXFY2dnB5Aj8B8hRuI4OTlx7do1evTowfHjx4EcEf8aslTwBUFAoVDQu88AihUvycjpy0lI0MbHZxdiY6BGw+r0GDyG8+fOsGXr9k+sfNHXHB4eTpcuXahTpw6hoaGsW7eOK1euULRo0WzdedVqNZs2baJt27Zs27aNUaNGIZVKM83SF63zj/9AW8y8fPnyn90+qdunUKFCGMoMMBAMeB78Eomg33M2NDTkRVAAAMVLFgX03zglUglhYeHExsZilz+/3vf/vZM0EsfX15cdO3awYcMGNBpNjth/JVkepbN67TqePQ3k9yVbkculxOs54k0fRIVDw1aduXz6MMtXrMSziQfWuXLpYtsB9u3bR/fu3YmOjsbT05N169aRN2/e76JBigPRtm3bePnyJXPnziVfvnz88ssvX73vlFxgarWa6OhoFAoFoaGh+Pr64u/vz61bt3j06BFPnjxJ075/++03Jk+aRFh4OImqRJ48eUa9eiknysooEsGYrdu0E31tWrdE7yGZ2oPw+vUb0Eiwtc2r//1/xwiCwIkTJ2jYsCEAhw8fpnHjxv/qdQVZSZYJviAIhIWFsWXrdko7OlO7aS0+vM+qo6cPtRpMzQS6DhrNsK5N2bh5GyOG/6QTsy5durBt2zYA1q9fr8uH8z2IvYhKpUImk3Hp0iXkcjkjR46kSJEitGnTJtXrSGlp/8eo1Wp8fX3x8fHRCfuzZ894/vy5blXox5QvX567d++muk9TU1P27dunE4Fc1tYYGBhw7/4DtKmJFWm65i+hvbZEdu3eB0AVl0qoMyVkzIB79/2xsDDDzi7Hwof/t63NmzfTvXt3AK5fv65br5KDfshSC3/ZilVEhEcwbs66TI+z/1qUsVCtbiVca9Zl27bt9Ovbm127dtK7d28AXX4a+L6EPiniG0tkZCQFCxakbdu23Llz57MulqioKIKCgnj58iUvXrzAx8eH+/fv6zJdfoxEIqFo0aJUqFCBIkWKULFiRRwdHXFwcCD/P+6MiRMnpir4w4cPZ8GCBbrzBa3x4FbVFZ+790lQhn/Vyt2kGMisWLxoLnfu3GPUL8PIndcuk4qhG3LqzEVs89pSuFCh/3yMuGhIde/enc2bN2Nubk5ERMRni/7kkDGyJJeOOHqXcShPibJOzFq7HaWSbDFR+zlkRnD94kUW/TaCQgXs8fY+gKWlJTt27KBx48bA9yv2kNz98v79e4oXL05iYiLXrl0jd+7cBAQE4Ovry4MHD7hx4waBgYF8+PBBVxFJRCaTYWZmhpOTE05OTpQtW5ZKlSpRoEABTE1NMTc3RyaTpXgOCQkJODk5JUv8BZA3b17WrVuHp6fnJwuyBEFgybIVLFuxip1bV1Kh/Ndny5TJLbhy6TxuNeoBoNHEoUpQoNLzwi5BEFAoYylRphoD+/fht0n/jvxGGUVMk92+fXsOHTpEnTp1OHjwIGZmZv/p+5JZZImFL5VKuXDxMh8+hNKgeUeQkCaxl0jBSA5SCSQoISGF529kDIIUVAkQF//p919DQjxUcK2BmYUlhkZGdO/WjaXLlmFmZpbtfYopZZj8mDdv3uDv78/t27d5+vQpefLkITAwECcnpxR/nytXLlxdXalUqRIODg4UKlSIwoULY2dnh6WlZarHEQX74/slCAKBgYGfiH2jRo3Yu3cvpqamqXb6GjWqs3jJCi5cvEqFCpXIaE4dbUFyS16FPNaJ/eqViwCp3sUeQDCUc2z/XyQmJODh0UDv+/+eEA2O6tWrc+vWLXr27Mn69euB79uQys5kmUtn565d2BcoTJWa9dM0USuRgvLVHfYdv4RKraFKm8GUsoa4JP3a1BIubFpKYJiAjWMDGtcsoVfR12jA1AzcPVqxdtEMQl48xdDQMFs0xrT401+9ekVQUBAhISEEBgZy+/ZtfH19uX//foqDVZ48eahUqRK3bmkLc2/dupWKFStSokSJVC10kYwOgEmLVJiZmbF9+3aaNWsGpN7p1Wo1FZ3L41C2FKvXbuOnIUORSCRpPr5UKsXAwACkpqgSIpk44VemTZ8NwNYta+ncpXemVNCSSiUkxinYunM/lStXoKKzc7Y2GjITMcpNrJM7efJkfvvtt2xVt+HfSKYLvlQqJTo6Gn9/f0qXq4SpmQmJadBLjQYs8tpwe/kQDj/XYHssgv1e45BLQKUBmQm88J5Gv8ETAUtmn+6BJBP6TrwSXGrUY/OKOeza7UWXzp30f5AvkJq1Hh0dTUxMDJGRkdy7dw8/Pz/8/f25fv06b968ITLy04kSMzMzihQpQuXKlXFwcKBs2bI4OztjbW2tc7/MnDmTsWPHsm3bNjp37gzo3+ISBysvLy9Aa9Vv2rQJW1vbLx5LFMkO7dsx7OdRnDx1jPr13In/qFRi0kIrgiCA1BCQkRgfwePHgezYuYf5C5YSHh6BhYU53gf2ULN27Uwrl2ggk3H971tcv3GLcb+O0iU8+68hrl3x8PAAYPXq1fTt2zfbvzX/G8h0H74gCAQ9e04N99r0+3kibXv0THOqY0EO8Y+O0rBqYyKA7lsCmdKiGGFKMI72o20pR3wTocXiB8zrU5pw/dSnSIZUCooYBSO6NqGqayUWL5ifabnrP4foS7958yYBAQE8f/6cJ0+eEBaWsjiVKlUKR0dHKleuTMmSJbG3t6dQoULY29unKd5+2LBhLF68mK5du7J582a9d0ZBEDh9+jT16tVj/vz5uhoB6Sk9GRUdTd36HhQtXJDdO1aTkPD/1bcyuQUggDqamJhYgoNDuHz5CpcuX+Pva9e5f///hbtHjBjCrJlTMZSZEK/8ijzcX0Amt6R7z74cPnqS18FPdTUS/iuIb6WbNm2iR48eAFy9ehVXV9ccsc8issSlc9PnNqrEBBwru5GQDpeLSgmmTh7sWTqchkMWsqlrW+o8u0V9WxjTohq+iVCi60b+6FmayJQj/r4ajQZMzEwoUqI0Tx4HEhsbizwdhafTUs4vJiaGly9fEhQURHBwMP7+/ty8eRNfX19ev36d4jYFChTAycmJwoUL4+DgQKVKlShbtiwFCxb8wvV8efGKRCJh0aJFBAcHs2XLFkxNTVmxYoXexSkkJISgoKAM5TtRqVSYm5kxdMgghv08it1eh2jfvh3xyihkcjMmTZrA77/PSnHb4sWK0q5tK1q28OTHLl0BA1QJkZks9sYcPuLN+YtXGT3y5/+c2ItGjVhvuUCBAty/fz8nT30WkyUW/uy581m4YAEHrz1HECTpTpBmnQcmVTdh2/VYinXfyRzXC7QZtAzManMi+Ay2SojPxDZjbArLpv/O1dPeHD9+BJt/FmGlRErWemJiIlFRUcTExPDq1St8fHzw8/PDz88PHx8fbUH2j9wRBgYGWFpaUqRIEapUqUKZMmVwdHSkTJkyyOVyTE1NMTY2/uRY+rKURDeSm5sbV65cYfPmzXTt2lWvnVM8Rkb3KVqMrm7uyGQG7N25jty5rZEaGHJg/yF27txDsWJFyZMnD/Z2+XBycsTa2gpLSwuMTa0ANYnxsZnuM5ZKpRjIzHCrUR9BMODggT1YW1n/Z4RO7BMDBgxg1apVVKxYkRMnTujyTOWQdWSJhf/ixQsMDQ3JlUeSocVWEREwZu0Bzjs35Mmm3nT10lpiP632oigQ/lGbEWRgZg5GRtrUVwnx2pQJitgMRoJqIF+Bgrx7/z5ZLHhKJF1JevPmTZ48eUJwcDBPnjwhIeHTSBJLS0sqV66Mo6MjFStW1EW9FC5cGCsrq9RPKZXIF32hVqsRBIGjR49iZ2dHt27dyJMnDx4eHnrrpF8rtOK1T50ykQEDf+K36XNZvmQp8cowWrRsRouW7dEWSFFr/9SJqNVq1Gp1JsXXp4yBzJiZM+fg9+Aha1cu+0+KffXq1bl8+TKOjo66oID/yj3ITmSJ4L979x6bvPkynA1THQ+SEg1YvnkSzbpNRRED1cZd5NdONrx+mfy3MjlEPjrN8IG9OHc7CADzErXo/9tiejZzRhmdftFXqyGXTV7UahWPHj3i7Zs3PH78mMDAQHx9fbl16xb3799PsQGbm5tToEABGjZsqPOrV6hQAQcHhxQt9OTH/bZ+TZVKhYWFBdHR0UgkEho3bszRo0dp1KhRtumsKpWKRg3q079vL2bOWUDlCuXp3bs7CXEKNBrll3eQycjkVuz12sW8RSsZPLAvbdu2+s9EoQiCQFBQEOXLlycyMpJff/2VmTNnAjli/63IEsFXKBTI5cZftdBKqoY3QUG6fwedPUJQVHVM/onaAa1lH/HgIO2rtyDKoQMzvf6kTO44dk7pxtwulXi9zp/JrUsRkc6V+BqN1gdrbWVNk8ZNifxowsDU1BR7e3ucnJyoWLEiDg4OODg4kC9fPkxMTDAxMdGGAX7E99DoxdW4Pj4+uLi44OHhwcuXL8mfP3+2Ov8Rw4Zy954vv89YQN68uWnWrGmm+uTTgkxuyt9XLzJt1mIcy5Zm+u9TgP9GMQ5BELh58yZ169YlMjKSjRs30r1795ywy29Mlvjwm7dqw7vQcLYeP0V4BiLepEYgebqHBlXa85682Bf8QMiLBDymX2PtiCq8/ic6x8Qa1jWRM+9sRe5qriALh3g1WFvB1OoS1txrzr3wA8S/Td/YIzOCv8+eZc74wZQsWRzn8uUoX748dnZ2OvdLSoIu8m9o5GIoXZ06dbC1tSU4OBgDA4NsI/raSdBEKv9QHUWskjXL5+BeuwHxykwI3UoDMrkZwS+CqO/RDqlUivfBfRQrWiTb3K/MQpxX8fPzw9HREUA3//Ot31hzyKL0yGZm5sTGRJOh1OVSyG0Ggxq15z1Q/Y/jXDq4A2vg6Hg3/nwGJkban6qioshbexJDVszCKByiFdo4+nAF1OnUDmJe8M4g/RctkYAiJpqIyAjWrl3L9OnT6dChAzVr1qR48eK6iIvU/r53sQetpV+7dhEWvGoAACAASURBVG3OnTvHmzdvKFOmDPBtyiSmhPZNxIDLF85QtEhhWrTtwd69u5HJzRD0nEL5c0ilEmRyc06fPk2dBm0wMpL9Z8ReTLE9b948HB0dEQSBe/fu6Sb7c8T+25MlPSFP7tx8CH2nTamQTuQmcOR3T66EgUnh7vzW15nEMm0YPagqkMjYLoN5Ew8GUohLNKfJsHEM6lCLqCRuGyMDuHLoMJiXwDYx/RnOpVIID3uLXG6sE+9/m6CnBZVKRa1atZgwYQKBgYG4ubkB2Uv0TUxM2LJpHXVr1+LnUZOYO28RUon0iyuF9YFMZoiBzJi58xYweOhYcuWywvvA3v+E2ItlKHv37s3IkSPJly8f7969w8nJ6V9/7d8TWSL4BQsUICEhgdC3KtJTY0MwBuW9A4ya/hcgYciaBRQxgIhIaD5qKxXNIfrWcqatvYJ5Lu02cQptNI6I3BreHpnNkpMxNJr4M0Yx6Z9KkEjgzctgctvYIDfKfOHIzqjVan7//Xc6d+7MlStXGDhwIJD1ZRJTQ6VSkTdPHvbu2U6lShWZt2A5jVt05v37MGRyaww/43rLKIIgIJNb8/r1W/oPHMH0GQtxLl+O40cOUbBgwX+94CXNdrl+/XpcXFx4+fIl1tb/nWik74Us6aWOjg5ogMd+dxHS2t+kYKQMZlj7lkQC1Yecomc1ayIVoEmEBOvibPBeCcCpSXXYcCoaU6Mk20vAxBKizq+mRutfKdZ2NXOHVCUynfN4EgnEKuDl86cULFwIUzPT//SrqXjtW7dupWvXrqxcuZIBAwZ8kzKJqaFSqZBIpPzptZMli+YT9DyYcpXrMHrMGF4Ev0Qmt0YmM/yqQUoikSCTGSKTmxMZGcX4CROoUr0xJ09fYNaM39m5ffN/YlGRIAjExcVRrlw5Nm/eTL9+/bh+/XpOauNsSpYIvpOjA4aGMgLu3yKtBpahHC6tnox/tCVGxZoxbXod4pOEVCbGgMkP/RnWtjLmFsYsnj2LOOP/e41MzeDO1slU8+hP8faL2bi0L5rI9LtzJFJQxMTw4tljihYpgpHM6D/jwkkNsSNv3rwZZ2dnVq1axbx587KNlQ/oXG1t27TizIkjeDZpxIFDx2jasguTJk/G1y+AxMREZHIzZEayNJ27RCJBZiRDJjcFCTx89IQpU/+gQZMObNu5j4b163L40D769e2dLesZ6xvhf+2dd1RUV9eHn7l3Gl0MKiDGbrCgJootRmOsifqKvbc3BoPGGtSYYjQaYyxfYmygxpBobKivsXdRo4iCBbDEhhELGhBRgYFp3x/jjFhjAaScZy2Wa13vPXPOnXN/d88+++wty1y7do0SJUoQGxvLkCFDCA4OBvJHBFphJMejdCRJIjUtjXZ+HXHzLMvn0+dhMPBMfhWVPTg5WkIybyXB43rqUATs1JZY/eRbYFaAqwdsG9uegClrKd9lLut/D0Cf9Pj0yv/aBzWcPXmaoT1bMn3adLp36ywm8z1kWcZkMlGyZEkSEhI4duwYNWrUyHP3x+pyiIyKYs68Baxduw57B3vequlDo3fq0/idetR6qwayygXLxDRxf4IqsNhFCiCDyEOR/HnwEOHhhwk/GIkuI5O6vrUY99Xn+NauBRQOsZNlmcjISHx9fQEIDg7G39+/QESkFWRypQCKLMt88eXXrFn7B/NCwyhStAg58kwowN4B1o1+m8+CD9B7bgyTA6qRlHA/Vv95vTFOLjBr4rcs/2UO8X+fR6u1KxQP9LNirZjl4eFBWloaBw4coH79+nnyHlmF/2ZSEstWrGLX7jAijxwlLS0Ns9mMl6cnpUt7UayYG04ODpgVZtJ1Om5cTyTu73iuXLkGmLG3s6NKlco0fe9devXohru7O1A4hN4adrlw4UI++ugjAC5cuEDZsmULxfjzO7km+BGHDvOfdn6MmvQTH3TsSHr2lCF9ALUjnPltBJ2G/Mjgldf59D/FuXsHTFnE3mR8dtGXJNBnGvi0b2sc7DRs2bROTOrHIMsyZ8+epVKlStjZ2XHkyBG8vb3z7L2yCr/BYCAh4TrHjkdz/Hg0x2NiuHnTUtHLYDDYztVo1BQtWpRKFStSx7c2b71V07KAr9UChUPo4f59mzdvHoMGDcLV1ZU9e/bg4+NTaO5BfieXShyCJMlU8alJ6XJvMD1kBRm657e2n4oEzsaLtK1clr/uKKjR6F0MKbdtYg8GUm56Mf/IBkoa7lv8T0OjhQO7djHlswAmTfyGHt27ion9BGRZZu3atbRv3x64v7ibl++XJClQKB713WdkZKDT6VAoFKhUqsemwChsrgur2A8ePJi5c+fi7u7OkSNH8tyOa8HTyZXUClZhHzFsKN9Pncr+HTtp0LQpGdmY6kShgFv/XMHng06UN2oxZGRicimW5QwDLi7uKJ/jJWPvAL/OmkylSm/Qo3tXANsmK8GDGI1G/Pz8WLNmDR06dMDT05O4uLg8XeTDZDJjSa52H0v0jRqNxhLyldNJ6vIDVrEvV64ccXFxtGzZki1btgB5+4UueJRcsfDBMmluJifTtl17NPbOBK1ex52U7P0MhWyJznnaxsq7t57NuldrIGr/Ab4bPYC09HRqvVmDr776Ch8fH9smHjHZH0WWZdtP/latWrF58+ZCZw0XJGRZ5saNGzRt2pTY2FiGDRvGjz/+CIj5nx/JtTg6o9FIUVdXenbvTuzRw+zetBMHp+z9DLMR7qZASvKT/55F7CUJjAb4be4USnh44uLsyB9//EHt2rUpV64co0aNIj093VIA+96fwILJZCIgIIDOnTuzZcsW+vfvn6di9AXPjizLJCQk4OPjQ2xsLNOmTRNin8/J1cBps9nMoICBNGjQkNnfjuHvc1dR5cGNq3YOEDxtPNFHDjF50kRiY2IwGAyMGTMGWZaZPn069vb2lCtXjrlz55KcnCzE/x5Wa37lypW0adOGkJAQvvjiiyfW5RXkPaw1gHfu3ImHhwc3btxg9erVBAYGFor9BQWZXBV868/6yZMncjvlFkFTv8Rs5sWSquUQ9g4QvutPdm9ajZ9fexq+Xd+WInjKlCmcO3eOI0eO0KlTJ65evcrgwYMpWrQobdu2ZefOnWRmZhZ64bf6u1etWkWdOnWYPHkyP/30U57amCV4PNawy1mzZtGsWTNcXFw4dOgQHTp0EAnQCgC55sPPiizLzJ4zj+kz/o923f/LyG/Gkng9t3vxKBo7uJWUhn+7ejg5u7Bz+2YcHR0fsGiyWqq3b99m8+bNfPPNN5w8aSmK7e7uTq9evRg6dOgD9WULo1VkfempVCoMBgPr16+nTZs2hfJe5Aesc3v06NFMmzbNtsfC3t5efGcFhFci+FYrInD0Z/waEsLAkV/TMyCA9HQwv6K1PTt7+Pv8Zb4M6IJOl87aNauoWKH8Uyd6Vis+IyODoKAgfvzxRy5evAhYCo0PHz6cTp06Ubp0adu5henhkWUZnU6Hl5cXSUlJnD59mjfeeKNQ3YP8gHUud+3alZUrV1KvXj3Cw8OBwjVfCzqvRPDBMsEyMzMZNGQ4O7Zvp6f/CHoGDCIjnecucv6y2DnA+VMXmPb5IK5c/pvlS5fgW7vWc030rJt5Tp48SVBQEPPnz7e10bJlS4YMGcK7776Lg4MDUHgeJFmWOXHiBNWqVcPBwYFTp04ViiyS+QVZlklKSqJ169ZERETQo0cPQkJCUKlU4jsqYLwywYf7ItnWrwPHjh6jlV83Rn03mYx0yMzI+c9XSODoDGdiLjJ2YCfS09KYPXsmrVo0f+GJntXlk5aWxubNm5k9ezZhYWEA2Nvb07dvXwIDAylXrpztuoL+YMmyzL59+2jUqBGvv/46cXFxSJJU4Med17EaXs7OzmRkZPDJJ58wa9YsoODPycLIKxV8uC/6Eyd9x4KFCyhZqgyjvp1DtVqVSb2TM9a+QmGJszeZIGTmNEJ/ncPrpcuycEEQlb2zz93w8MLtrFmzWLhwIdHR0QCo1WomT55M+/btC4X4y7JMSEgI/fv3x9nZmZQUy0aMgjrevI4sy4SHh9sK2axYsYIuXboU+o1mBZlXLvhw3yreum07Iz4dhSwrafpBB3oN/gw7e4mMjOzz7VuF/q+YGOZP/ZIL5/6iYcO3CZ43Gzu7nEuMllX8rS6fJUuWkJxsqblaq1YtJkyYwDvvvIOzszNQMIVQlmUmTZrEV199xXvvvceOHTtE7vRcxrqGFhoaSpcuXQDYu3cv77zzjvgeCjh5QvDh/iSMu3iRL8dNYOuWLVR8ozJtuvTjPz36oNFChg4MBksCtGdvF2QlqFQW903E7hjW/B7EwbDtqLUaxn42mn59egO5I7APx6OvWbOGoKAgtm/fDoBWq6Vjx458/fXXVKxY0XZeQXkQreMPCAggKCgIf39/goODC8z48jpWwyM4OJiPP/4YgEOHDuHr6yu+g0JAnhF8K9YJeeToMb4eP5Fjx46C2UQLv268/V4byr1RlZKlXTEawGC8l/3SlCV7ucKyU1aWQamyvCTOnTpD9OH97Fi/nDMnYyjpVYoO7dvz5RefAbyyn7APi39wcDA///wzhw8fBkCj0TBs2DD8/f0pX7687bz8/mBaX+6tW7dm06ZNjBw5khkzZrzScVl3Az9pc5jZbLb95Vesz1a3bt1YsWIFderUYf/+/SiVynw/pwTPRp4TfCvW4hrhByMI+W0xUYcjSblzB9eibnh4leaNqjXxKlsRj5JlcCjiikatxmQykZp6h1tJ/xAfd5a4v04Qd/YkNxKugtmIW7Hi9O7VE792bSlezJJYLa9M9KziHx8fzw8//MDixYtJTEwEoHbt2gwaNIjOnTvj6OgI5J2+vwhW8alduzZRUVEsXbqU7t27/+uYJEnKtrw81hcPWAQ9NTUVnS4DvV5v64csS6hUKjRaLfZ2drZ+57f8QNZ+N27cmL1799KoUSP27NkD5O95JHg+8qzgw4MieCk+nsioI+wJ28uOXbtI/CfRVhD9YZvMjCVDpywrqVGjOi2aN+PtBvWpXbuWrb28OsmzihDAvn37+O6779i8eTMASqWSdu3aMWLECN5++23beXl1PE9DlmVu3ryJp6cnGRkZ7Nixg6ZNmz5xLLIsc/fuXRwcHF5YbB/+VbVj524ORhzizJmzxF++TGJiErdv3yZdZ0nlqtFocHF2ws3NDQ93d7y936BB/bo0b/reA1vE8/L9l2UZg8FAw4YNiYiIoGvXrixfvhzI2/0WZD95WvCz8nDEi8lk4uy581y+fJmUlNvodDokScLRyRG3116jQvkKuLkVfeCa/GaVPSxOq1ev5rvvviMqKsp2bOTIkfTq1Ys333zTdiw/PcTWdNPKe8WOd+3aRZMmTR4ZgyzLxMfHExgYyIoVK557jFnv5YaNm1kZupqdu3aTkamnmFtRypUrQ5nSpShR3I1ibm44OjmgMEOaLp0b1xNJuH6Dq9euc/L0WW7evIk+M5NWrVrQo3tXmr7XBPt7OfPz2r23liKsU6cOZrOZefPm2Xz3ea2vgpwn3wj+wzxLMq6CNKGzWv6XLl1i6dKlTJ8+naSkJACqVKnC8OHD6dixI0WLWl50+SW8TpZloqKi8PX1xWw2k5iYyGuvvfbA9yfLMrNmzSIwMBDdPev7WV/esixj0BsIXbOGX0IWc/bsOYoXd+OtmtXp3KktVbwr4eTsaNkQp1Bx7zdilhYkQI8uLZ07d1OJPXGa7Tv2sGPXPv5JTKR4sWJ8Muhj2vu1w9HRIc/Mu6x7H0CEXQryseAXVh52+YSFhfHLL7/w22+/2Y516tSJwYMH8+6779qO5fWHXJZlduzYQfPmzSlVqhSXLl0CyOJLl3n99deJj49/5mLp1l+FsbEnGDL8U06eOk1JTw9GBw6m2XuNcCtWCkjDZDBgMv37rz9rhSxZpQTsSE66yq6w/cya+zPRMSeoWaMGn44YSusPWj3Q99zGOkdWrlxJ166Wwj15udawIPcQgp+PeVj8161bx9y5c9m6davt2LBhw+jZsye+vr62Y3lV/GVZZsuWLbz//vt4e3tz6tQp2/+dO3fOFqY6bdo0AgMD/zXPkV5v4NNRY/htyVKqVa1M4IgA/Pw6AHqM+oyXFj9ZlpFVasCODetXM3POIg5HHqVL5/ZMnjgBNze3XBdY60tuyJAhzJ49G7VazZ07d1Cr1ULsBULwCwpZxf/y5cssW7aMoKAgLly4AEDp0qUZM2YMfn5+eHh4AHlT+GVZJjAwkBkzZjwQSTJgwAB+/vlnwLJJLTIy8qmLu/Hxl+n734+4dCme1h80Y/qUr1FpNOgzMnJkzGqtPbdv3SRo/mLmzQ+hbJnSzPxhBj4+VXNNaC0vOT39+vVj6dKl1KlTh02bNj3iHhMUXoTgFzAetvrDw8NZtGgRCxcutB1r2LAh48ePp2nTprZjeUX8rWsz1ljx4cOH88MPP+Di4sLt27dt55nN5seKmCzLRMfE0LN3f9LS0pk6ZRwdO7THpE/HkMOiZ7H4Xdiwfg3DAschSQrW/28V3tmYruNpnw3QoEEDwsPD6dKlCytWrAAK1lqW4OUQgl+AeVj8N27cyJw5c2whngDdu3dnxIgRecrlYxWvPn36sHjxYurVq8fBgwcfOCciIoI6deo8srC7K2wPA/wDcHZyYsvGZbi7l0KfcYfcHI5a68SVS3H0+e9Qzpw/z8L582jZvFmOpu1IS0vD2dkZo9HIqFGjmDp1ar6LShPkPELwCxFWIU1JSeGXX35h7ty5nD17FgBPT0/8/f0ZOnQorq6uQO4L/8MvqNTUVDw9PR+w7K2MHj2a77///oFF3XPnL9Dy/baULOnB/LnT8a7sTaYuLdf6nxW1Rs2t5Nu09utJamo684NmU8e3draLvizLHD58mObNm5OSksLMmTMZOnToK39pC/ImQvALGQ+LamxsLLNmzWL+/Pm2Y40bN2bgwIF0797ddiwnrcWseywOHjzIggULOHr0KEePHn3iNXXr1uXgwYOYTCYkSUKfmUnTFh+QePMmK3+fTzUfHzJ1qTnS32dFrdVy4Xwc7Tr2xWg0cfTwATRabbaIvvV7/N///keHDh0AbBvXhNgLnoQQ/ELMw+J/+PBhvv32W/744w/bsc6dO+Pv70+zZs1sx7JTUGRZZvfu3XTo0IFbt24917XXr1+nePHiAPT770ds2rqd3VtXU7WaD5m6u9nSv+fBui/E+q/ZbEalsSf6+HE6dRtAhQrl2LR+7Uu/PK0vyBkzZhAYGEjx4sU5fvw47u7uwl8veCqiqnQhxrrwaf3z9fVl7dq1JCYmsmTJEry9vQkNDaV58+aULFmScePGcf78eSRJQpblbClKbjQaqVWrFps2baJr167Y3dux+iwcOXIEgN8W/07Y3n0MHtifqtVq5prYKxQK1BoNaq0jam0RVCqVLdVzRoYlJ48hM43qNaoz4evRHDlynHnB8/91w+DTsIr98OHDCQwMpGrVqsTExAixFzwTwsIXPEJWF8uJEycIDQ1lwoQJtmP169dn4MCB9O3b13bsZazWrLumU1JSCAkJYcqUKSQkJDz1ug8//JCFCxdSqXJ1ypUtzZYNSzEYjDm6UClJEkq1BrADzEQe3seRqGPs2x/Oxbi/SU1NJS1dx82kJIYP/4TPv/gcfUYqKk0RmrX4gOs3EtmxdSMlShR/boG2fi+tWrVi69atlC1b1hZ2K8Re8CwIC1/wCFaL32QyUbVqVcaPH4/ZbCYqKoo+ffoQHh5Ov379UCgUtG/fns2bN6NQKCxhiS9g+SsUCjZu3MiiRYtwcXFh2LBhXLt2jbi4OLp37/5Eq3/Lli1Mm/EjmZmZTPrmM5DkHBN7BaDW2qFUq9mwbgNdOndAoZDwrdOYgQHDWLJkOUePRXMzORkw4+rqeq+QzT33lymV+XOnYTDoGTP2i+f+fFmWuXbtGp6enmzdupWAgAAh9oLnRlj4gmfGamHevn2bTZs2MWPGDCIjIwFLVsmRI0fy4Ycf2nL3P6vVL8syMTExvPXWW2zbts2WPM36eUlJSaxbt45vvvmGixcv3r9OqaRy1eo0bFCPebOnoNcbcmSxUqFQoNLYExt9jNZtO3PpUjwAJYoXo337djRr2hhf31o4Ojqi1WosLz1ZQkKBMcv41VpHRo35iiW/h3I08sAzu2FkWSY2NpZGjRqRnJzMnDlzGDRoECDEXvB8CMEXPDdZXT6nT59m2bJl/PDDD9y5cwewJHILDAykf//+D1z3tJ2xiYmJeHl5kZGRQVxcHGXKlHkg5NLKvn37mD9/PkuWLAFA6+BCxIHtVK9ehUxdRraOE6xi78qY0SOYOu1HAJo3e4+xYz+lyXvvAVpAh9mof2Ax+3HFUtRqFWfOXqBl627079+HcV+MfaZ8QHFxcbaax/Pnz+ejjz4SkTiCF0IIvuClyCrGMTExzJw5k9DQUFvsfIMGDRg/fjyNGjVCo9EAj1r+Vh9+iRIluHHjBgBnzpyhYsWKDwjiw1FF3lWqcTslhatX4snU3cz2sVnFvk2bVmzcuJUSJYoTER5G6bKVgbTnTtNgaU/NwIBAIqOi2b1zC85OTo8Vfes9WbBgAf7+/oAliqp27eyP5RcUHoQPX/BSZI3y8fHxYeHChSQkJLBq1SoaN27MgQMHaNGiBcWKFWPAgAH89ddfADZ/P9xPc+zg4GBrt2HDhsTFxT3wQsmaTiEi4jBIKiaM/wLIfsseQKVx4Ltvx7Nx41beerMGZ08fo3TZsmTqbpKp0z23hW05X6Jd25ZcvnyFw4ciH3ueLMsoFAqGDh2Kv78/Li4uXLlyRYi94KURgi/INqzCb2dnR8eOHQkLC+Py5ct8//33qNVqfv75Z7y9valWrRpTpkwBHhT+ChUq2Nq6ceOGLTvmw8VvAHbv2YMsSbzTsB5mY2a2j0WlUnE94TKffzkBd48ShIfvwqmI80vv3NVn6Pjgg5YYTUa2bNv+yP9bx/rxxx8za9YsqlWrxvXr1/H09BRiL3hphOALsp2sVn/JkiUZPXo0iYmJXLt2jYEDB3Lt2jXGjh2LQqGgbt26Nn98zZo1H2mndOnSpKenPyL6W7bu4HUvT7wre2MwGLJ9DArZiU6dewGwa8cG1Br7bFkjsFj5djRq2ICIQ4cwmow2N5Usy5jNZnx9fQkODqZPnz7ExMSg0WiE2AuyBSH4ghwlq/i7u7sTFBTElStX2LNnD40aNeLQoUP07t0bFxcXNm7c+Mj1ly5dom7duuj1epurA+DU6dNU86kMqLJ98VKpVHLt6jn274+gYsXyVK5SO5tz8mRSx7cmKSl3uHjxki2kNTk5GU9PTyIjI+nfvz+//vorICJxBNmHEHxBrpHV5WPNdX/16lWCg4MpVaoUJ0+efOx1MTExtupdkiQRHx+PyWSiWhVvQJ/t/ZSUag7sP4jZbOKjAf1y4DMMVK1SiTt373Lt2lXAktOoRIkSJCQkMG3aNBYtWvTEFNACwYsiBF+Q62S1+j08PPD396dKlSpPvebAgQM0btwYgIt/XwIUVKpYDszZ784BFeHhhwBo1bI5mLN5UdhkwsPdHVmS0GXoWbNmDT4+Puj1ek6cOEFgYCAmk0mkNhZkO0LwBa8Mq1++d+/ehIaG/uv5e/fuJSAggNu37yBJEsWKuWHOEVGUib9yGQUKihUrhsmYvZ9hNptwcnbE1bUICxYsoGPHjjg7OxMZGUmVKlUwGo0ixl6QIyhfdQcEhROr2Ddp0oSwsLBnvi4oKIjDUUdxLeKCVqvOscImly5dwc5ei4uLc7Zb2mYz2Gu12NvbsWHDBhwdHIiOjqZ06dLChSPIUYTgC3Idq9hn3Wj1pPPKli2Lo6Mjbm5ueHh44OPjw8nTZ/hzfwQKXjzr5L9xMe4iDvb22DkUJVP3fGmb/x0zkqRAr9dT88232L93NyAWZwU5jxB8Qa5iTQLm5+dHSkoKZcuWpXz58lSoUAEvLy8qVKiAt7c3rq6uaDQanJ2dUalUKJX3p+qyFaFs2bbzgTw12U3C9Rt4v1GJnPF6KtAbjKSlpdOrVw9AiL0gdxCCL8hVrDlmVq9ejZeX1zNfY80dI8sydlotBoOBu3dTkaScs/JLlvTMkXYVCgVpqWlkZuof2F0sEOQ0QvAFuYrJZMLDwwOFQvHCOfRLlfJCgYK/L12mStXsDc1Uq9WcO3McgAoVygHZ/ytCIUkk3UwmPT0drxx6qQgEj0NE6QhyHZPJZMu3/yJUKF8Os9nE2XMXgEfTLrwUkh0hvy4FoHu3zoAue9sHUEhcunwFWanE3b1E9rcvEDwBIfiCfIXRaMTFxQUHBweOHz8BaLKtbUmSwJzO8hWhqJQqqlevilGf/Ru7QEl09ClcXJzx8iyZA+0LBI9HCL4gX9K48Tuc+usct5OvIcvZM42V6iJ8NW4i58/HMWHC57i+llN1YlVs3R6GV0kP3N1LiAVbQa4hBF+QL2nybiNu/PMPsSdPIStffilKrXVl3doVTJo0FYVCwdjPx2PIzP5i6EqlkoSrF7hy9SqNGzXK9vYFgqchBF+QL2lQrx4atYatO/aAQvvC7ahUKtTaohyJ2k+79t0A2LZlLZCByZT9u7okpZa16zejVMq836pFtrcvEDwNIfiCfIfRaKRSpYpU9/Hht8WhgPGZwzOtmSnVWi1qbVGuX79Bzx5dqFX7HQDC9++kWYu2ZOpSs73fkiSRnppC6KoNNKhbj4oVKmA2i3w5gtxDCL4g39K/by/SdeksXx6KUv2glS9JksV616jvibs9aq0LKo2Wu3dT2f/nfvr17YlHyQosXRZKubJlOHcmmnoN6pOpS86R/irVWsL2HuDEqdN06twBWZZz5FeEQPAkRE1bQb5EkiQMej2t2vihUJjZ/MdSFJICk8mEJEnodDquXU0gLT2d1LRU4uOv8uefB4iJPUFU5FHuplos+KrVKjN2zKf0iNGCaAAABGxJREFU7NUbMJCpy4EwTO7Vs1Wrad+pH9Gxpzj/V+wL70MQCF4UsfFKkC8xmUyo1Gq++nIs7Tt2ZXbQz4wYPpJM3S2UagemTvyOiZOmPnKdp6cHderUpnmzdxk+Yghau9eADPQZqTmaoVKlcWTxb0s4ciyGiePH2cYgEOQmwsIX5GtkWaZd+y5cuhzP8sXzeKOSpS7u/v3h7D9wkDJlXqdYsWK4l3CnSBFnnJwccXQuAsgY9Wm5EhKpUipJS9fhU6sJlSpWYOMfq5FkWQi+INcRgi/I18iyTHR0DB269KB82dfZunkj+oxkVCoVSGosqRFMcK+giMlkwmwykVuTXpIklGotQ4aNYe26zaxbs5I336wpYu8FrwSxaCvI1xiNRqpX92HK5Akcjz7JuPHjUGkcMBj0ZOrukqlLI1OnIzMzE4PBYBH8XOqbQqFAqXZg4rfTWPJ7KGNGjRRiL3ilCB++IN9jMpno1KED4eERLApZhkeJ4gQEfJQjoZXPg0rjREhICCG/reA/bT/gk0EfCzeO4JUiXDqCAoG1qErL99ty6q+/+PKz4fj7D0KfkZzr5QIlSYFS7cy2bZvo++Ewar1Zk3VrVyFJkrDuBa8U4dIRFAisQrp183qavNuY0Z9PYsrUKag0GtRqda71Q6mUUart+H7qNHr2+YTGDd/mj/+FCrEX5AmES0dQYDAajciyTPCcn3Cwt2fOvEWcPXueCeNG4VWqDJm67M+NkxW1VoMxU49/wHC2bQujRfMm/L44xNY3geBVIyx8QYHCaDSitbNj7uyZDPr4I7Zu3U0bv97s3rULtbYIaq0WhSL7qmQpFJaiKWptUcLC9tOkRSdWrV5Pj25dhdgL8hzChy8okCgUCiRJ4tjxaD77/CsiDkXSoH4dRgwdQLNmTQEw6jNspRNfpG1ZpQEUxEQfZ+wX33Io6jhlS7/OwgVz8alWTeykFeQ5hOALCjTWxdwfZ85m2YpQ/vnnH96s6UO3rn40adQAN7eilnh9k94Wtvk4JElCqZRBUmM2ZnL7zh127f6T5SvXEnUkGicnR3r26Ib/gA9xdnYSVr0gTyIEX1DgkSQJhUJBYmISoatW88NPs7l58xYlirlRv74vvr41qVXThxo+lVFpX3tsG4aMZE6dOU9ERBSHI49y5FgsF+Iu4lqkCD17dGPI4I9xc3MDhAtHkHcRgi8oNFitfYADB8JZt2ET4QcjOHvuPHq9AQCtVoPba6/h5OSAGUhPT+d6wg10GZmApYBJpYrlqVLZm25du9C4UUNbm0LoBXkdIfiCQofVBw9w9+5dbiYnExMTy8lTp4mPj+eff5K4c9cS0WNvb0fx4sV53cuL8hXK8WbNmhQv5oajoyPAC60BCASvCiH4gkKN1d3zXJjNGMVirCAfIuLwBYUaEUUjKEyIOHyBQCAoJAjBFwgEgkKCEHyBQCAoJAjBFwgEgkKCEHyBQCAoJAjBFwgEgkKCEHyBQCAoJAjBFwgEgkKCEHyBQCAoJPw/N9wfRu4Dw+wAAAAASUVORK5CYII=)"
      ],
      "metadata": {
        "id": "MvLVpeMFqznM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `torch.nn` module defines a series of useful classes like linear networks layers, activation functions, loss functions etc. A full list can be found at https://pytorch.org/docs/stable/nn.html."
      ],
      "metadata": {
        "id": "2-izSE3uyksq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "SODk32VPxMMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create a basic network, we can use the `nn.Sequential` class."
      ],
      "metadata": {
        "id": "SAszJtUjxgP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# in this example the activation functions are included in the layers object and\n",
        "# the forward method just calls it\n",
        "\n",
        "class SimpleClassifier(nn.Module):\n",
        "  '''\n",
        "    Simple Neural Network\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Linear(2, 4),\n",
        "      nn.Tanh(),\n",
        "      nn.Linear(4, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''Forward pass'''\n",
        "    return self.layers(x)\n",
        "\n",
        "model = SimpleClassifier()\n",
        "# Printing a module shows all its submodules\n",
        "print(model)"
      ],
      "metadata": {
        "id": "0FBb1c4kz4U6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0609d90d-56df-4ff0-9c46-cf8c04f85f5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleClassifier(\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=2, out_features=4, bias=True)\n",
            "    (1): Tanh()\n",
            "    (2): Linear(in_features=4, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are further alternatives, like `nn.ModuleList` and `nn.ModuleDict`, that allow you to have different data structures of network modules."
      ],
      "metadata": {
        "id": "M6c203tVR8GG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally to `torch.nn`, there is also `torch.nn.functional`. It contains functions that are used in network layers.\n",
        "\n",
        "This is in contrast to `torch.nn` which defines them as `nn.Modules`, and `torch.nn` actually uses a lot of functionalities from `torch.nn.functional`.\n",
        "\n",
        "Hence, the functional package is useful in many situations, and so we import it as well here."
      ],
      "metadata": {
        "id": "gz_J1Juhw_pb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "wvnJw3v_x0_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To build a custom network, we inherit from a class called *torch.nn.Network* and fill out the \\__init\\__ and forward methods:"
      ],
      "metadata": {
        "id": "jrwBpFPoxGAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, num_inputs, num_hidden, num_outputs):\n",
        "        super().__init__()\n",
        "        # Initialize the modules we need to build the network\n",
        "        self.linear1 = nn.Linear(num_inputs, num_hidden)\n",
        "        self.act_fn = nn.Tanh()\n",
        "        self.linear2 = nn.Linear(num_hidden, num_outputs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Perform the calculation of the model to determine the prediction\n",
        "        x = self.linear1(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "DKoNCSWTq5no"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The forward function is where the computation of the module is taken place, and is executed when you call the module (nn = MyModule(); nn(x)).\n",
        "\n",
        "In the init function, we usually create the parameters of the module or defining other modules that are used in the forward function.\n",
        "\n",
        "The backward calculation is done automatically, but could be overwritten as well if wanted."
      ],
      "metadata": {
        "id": "3QR2ZLC4yKIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleClassifier(num_inputs=2, num_hidden=4, num_outputs=1)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "# Printing a module shows all its submodules\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPKrLibHryH0",
        "outputId": "314f7284-ffa8-49e3-9d88-782da1c47abb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleClassifier(\n",
            "  (linear1): Linear(in_features=2, out_features=4, bias=True)\n",
            "  (act_fn): Tanh()\n",
            "  (linear2): Linear(in_features=4, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing the model lists all submodules it contains. To access network's parameters we use the **parameters()** method, which returns a python generator."
      ],
      "metadata": {
        "id": "WQEopwSRSnaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(f\"Parameter {name}, shape {param.shape}\")"
      ],
      "metadata": {
        "id": "37leAu9AsHLD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c432342-08aa-49d7-e26b-9e377af5c714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter linear1.weight, shape torch.Size([4, 2])\n",
            "Parameter linear1.bias, shape torch.Size([4])\n",
            "Parameter linear2.weight, shape torch.Size([1, 4])\n",
            "Parameter linear2.bias, shape torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each linear layer has a weight matrix of the shape [output, input], and a bias of the shape [output]."
      ],
      "metadata": {
        "id": "Zh4yXCzcyp1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Datasets and data loaders"
      ],
      "metadata": {
        "id": "pxJChYeUoHLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch has developed standard conventions of interacting with training data that make it fairly consistent to work with, whether you’re working with images, text, or audio. Those convetions include *datasets* and *data loaders*.\n",
        "\n",
        "The dataset class provides an uniform interface to access the training/test data, while the data loader makes sure to efficiently load and stack the data points from the dataset into batches during training.\n",
        "\n",
        "The dataset class summarizes the basic functionality of a dataset in a natural way. To define a dataset in PyTorch, we simply specify two functions: __getitem__, and __len__. The get-item function has to return the -th data point in the dataset, while the len function returns the size of the dataset."
      ],
      "metadata": {
        "id": "106XH8NzIEB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to give a better idea how to define a custom set\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# define your own dataset class by inheriting from PyTorch Dataset \n",
        "class XORDataset(Dataset):\n",
        "\n",
        "    def __init__(self, size, std=0.1):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            size - Number of data points we want to generate\n",
        "            std - Standard deviation of the noise (see generate_continuous_xor function)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.std = std\n",
        "        self.generate_continuous_xor()\n",
        "\n",
        "    def generate_continuous_xor(self):\n",
        "        # Each data point in the XOR dataset has two variables, x and y, that can be either 0 or 1\n",
        "        # The label is their XOR combination, i.e. 1 if only x or only y is 1 while the other is 0.\n",
        "        # If x=y, the label is 0.\n",
        "        data = torch.randint(low=0, high=2, size=(self.size, 2), dtype=torch.float32)\n",
        "        label = (data.sum(dim=1) == 1).to(torch.long)\n",
        "        # To make it slightly more challenging, we add a bit of gaussian noise to the data points.\n",
        "        data += self.std * torch.randn(data.shape)\n",
        "\n",
        "        self.data = data\n",
        "        self.label = label\n",
        "\n",
        "    def __len__(self):\n",
        "        # Number of data point we have. Alternatively self.data.shape[0], or self.label.shape[0]\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return the idx-th data point of the dataset\n",
        "        # If we have multiple things to return (data point and label), we can return them as tuple\n",
        "        data_point = self.data[idx]\n",
        "        data_label = self.label[idx]\n",
        "        return data_point, data_label"
      ],
      "metadata": {
        "id": "pptP4-56p04-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s try to create such a dataset and inspect it:"
      ],
      "metadata": {
        "id": "5MZmsUoX4Xcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = XORDataset(size=2500)\n",
        "print(\"Size of dataset:\", len(train_dataset))\n",
        "print(\"Data point 0:\", train_dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZTJTkVK4YO9",
        "outputId": "b4197dc6-8283-4906-da59-824a5cf762b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of dataset: 2500\n",
            "Data point 0: (tensor([0.8647, 1.1596]), tensor(0))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To better relate to the dataset, we visualize the samples below."
      ],
      "metadata": {
        "id": "MJJYhmSG4eKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def visualize_samples(data, label):\n",
        "    if isinstance(data, torch.Tensor):\n",
        "        data = data.cpu().numpy()\n",
        "    if isinstance(label, torch.Tensor):\n",
        "        label = label.cpu().numpy()\n",
        "    data_0 = data[label == 0]\n",
        "    data_1 = data[label == 1]\n",
        "\n",
        "    plt.figure(figsize=(4,4))\n",
        "    plt.scatter(data_0[:,0], data_0[:,1], edgecolor=\"#333\", label=\"Class 0\")\n",
        "    plt.scatter(data_1[:,0], data_1[:,1], edgecolor=\"#333\", label=\"Class 1\")\n",
        "    plt.title(\"Dataset samples\")\n",
        "    plt.ylabel(r\"$x_2$\")\n",
        "    plt.xlabel(r\"$x_1$\")\n",
        "    plt.legend()"
      ],
      "metadata": {
        "id": "Z88_e4gZ4e70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_samples(train_dataset.data, train_dataset.label)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "pzzgM0f74mbA",
        "outputId": "04451eb0-102b-4b7e-be1a-ddeb79258e21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The class `torch.utils.data.DataLoader` represents a Python iterable over a dataset with support for automatic batching, multi-process data loading and many more features.\n",
        "\n",
        "The data loader communicates with the dataset using the function __getitem__, and stacks its outputs as tensors over the first dimension to form a batch. In contrast to the dataset class, we usually don’t have to define our own data loader class, but can just create an object of it with the dataset as input.\n",
        "\n",
        "Additionally, we can configure our data loader with the following input arguments (only a selection, see full list at https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader):\n",
        "\n",
        "- batch_size: Number of samples to stack per batch\n",
        "\n",
        "- shuffle: If True, the data is returned in a random order. This is important during training for introducing stochasticity.\n",
        "\n",
        "- num_workers: Number of subprocesses to use for data loading. The default, 0, means that the data will be loaded in the main process which can slow down training for datasets where loading a data point takes a considerable amount of time (e.g. large images). More workers are recommended for those, but can cause issues on Windows computers. For tiny datasets as ours, 0 workers are usually faster.\n",
        "\n",
        "- pin_memory: If True, the data loader will copy Tensors into CUDA pinned memory before returning them. This can save some time for large data points on GPUs. Usually a good practice to use for a training set, but not necessarily for validation and test to save memory on the GPU.\n",
        "\n",
        "- drop_last: If True, the last batch is dropped in case it is smaller than the specified batch size. This occurs when the dataset size is not a multiple of the batch size. Only potentially helpful during training to keep a consistent batch size."
      ],
      "metadata": {
        "id": "tTNYdo7SqUNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a DataLoader for the training data\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)"
      ],
      "metadata": {
        "id": "Oh4Sw1LVwnTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# next(iter(...)) catches the first batch of the data loader\n",
        "# If shuffle is True, this will return a different batch every time we run this cell\n",
        "# For iterating over the whole dataset, we can simple use \"for batch in data_loader: ...\"\n",
        "data_inputs, data_labels = next(iter(train_data_loader))\n",
        "\n",
        "# The shape of the outputs are [batch_size, d_1,...,d_N] where d_1,...,d_N are the\n",
        "# dimensions of the data point returned from the dataset class\n",
        "print(\"Data inputs\", data_inputs.shape, \"\\n\", data_inputs)\n",
        "print(\"Data labels\", data_labels.shape, \"\\n\", data_labels)"
      ],
      "metadata": {
        "id": "3DJv5HZO6Ifq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same procedure has to be performed for the *validation* and *test* data sets."
      ],
      "metadata": {
        "id": "EDdl0uBYx5Pc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loop"
      ],
      "metadata": {
        "id": "s4Lb_Tm44aYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After defining the model and the dataset, it is time to prepare the optimization of the model. During training, we will perform the following steps:\n",
        "\n",
        "- Get a batch from the data loader\n",
        "- Obtain the predictions from the model for the batch\n",
        "- Calculate the loss based on the difference between predictions and labels\n",
        "- Backpropagation: calculate the gradients for every parameter with respect to the loss\n",
        "- Update the parameters of the model in the direction of the gradients"
      ],
      "metadata": {
        "id": "kBVjYer36SS6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch  provides a list of predefined loss functions which we can use (see https://pytorch.org/docs/stable/nn.html#loss-functions)."
      ],
      "metadata": {
        "id": "LBuUY9jS6jns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for binary classification\n",
        "loss_module = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "5XlldR3Z63Zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train the network, we need an optimizer. For updating the parameters, PyTorch provides the package `torch.optim` that has most popular optimizers implemented. "
      ],
      "metadata": {
        "id": "ZRmN57l-3EMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize optimizer (before a training loop)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "lEUfq3mw3JTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The optimizer provides two useful functions: optimizer.step(), and optimizer.zero_grad().\n",
        "\n",
        "The step function updates the parameters based on the gradients as explained above. The function optimizer.zero_grad() sets the gradients of all parameters to zero. \n",
        "\n",
        "If we call the backward function on the loss while the parameter gradients are non-zero from the previous batch, the new gradients would actually be added to the previous ones instead of overwriting them.\n",
        "\n",
        "This is done because a parameter might occur multiple times in a computation graph, and we need to sum the gradients in this case instead of replacing them. Hence, remember to call optimizer.zero_grad() before calculating the gradients of a batch."
      ],
      "metadata": {
        "id": "IkWttFFF7VG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can write a small training function. Remember our five steps: load a batch, obtain the predictions, calculate the loss, backpropagate, and update. Additionally, we have to push all data and model parameters to the device of our choice (GPU if available)."
      ],
      "metadata": {
        "id": "lnnCTSWg1oX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition, we set our model to training mode. This is done by calling model.train(). There exist certain modules that need to perform a different forward step during training than during testing (e.g. BatchNorm and Dropout), and we can switch between them using model.train() and model.eval()."
      ],
      "metadata": {
        "id": "HmfEAM359MTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Progress bar\n",
        "from tqdm.notebook import tqdm\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def train_model(model, optimizer, data_loader, loss_module, num_epochs=100):\n",
        "    # Set model to train mode\n",
        "    model.train()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        for data_inputs, data_labels in data_loader:\n",
        "\n",
        "            ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n",
        "            data_inputs = data_inputs.to(device)\n",
        "            data_labels = data_labels.to(device)\n",
        "\n",
        "            ## Step 2: Run the model on the input data\n",
        "            preds = model(data_inputs)\n",
        "            preds = preds.squeeze(dim=1) # Output is [Batch size, 1], but we want [Batch size]\n",
        "\n",
        "            ## Step 3: Calculate the loss\n",
        "            loss = loss_module(preds, data_labels.float())\n",
        "\n",
        "            ## Step 4: Perform backpropagation\n",
        "            # Before calculating the gradients, we need to ensure that they are all zero.\n",
        "            # The gradients would not be overwritten, but actually added to the existing ones.\n",
        "            optimizer.zero_grad()\n",
        "            # Perform backpropagation\n",
        "            loss.backward()\n",
        "\n",
        "            ## Step 5: Update the parameters\n",
        "            optimizer.step()"
      ],
      "metadata": {
        "id": "DYhpp-Ku1xzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To perform the backward pass, we just execute loss.backward() which updates gradients in all differentiable tensors in the graph."
      ],
      "metadata": {
        "id": "2oQFK2otUXSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, optimizer, train_data_loader, loss_module)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350,
          "referenced_widgets": [
            "2ad446cb2a9940eaba5c21cf8ee07e79",
            "053903b76f7e4cd1becf0f18a5d2eb1f",
            "6452972c24884da199a37166f39a40d0",
            "43184973c86846babec4e264640098e0",
            "17462a9249c043058f31f9ae429082ab",
            "2bf61653db854a5184a0405ddfce6c72",
            "6c878b76530e454489efdba6fed3ed69",
            "e07ee736341c4fd580d968d9c69bb3b3",
            "f32d1163d5114e44876fc689e31f50c3",
            "c405faadf9634776aecbb8e6ea4c214e",
            "a4c8d1201f904830830d68b514f47aee"
          ]
        },
        "id": "f3-SjHMd9av9",
        "outputId": "61d6dccb-6248-4f9b-b690-11cf9d935f97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ad446cb2a9940eaba5c21cf8ee07e79"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-3f78c27e2599>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-1cbd8f14e16f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, data_loader, loss_module, num_epochs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m## Step 2: Run the model on the input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Output is [Batch size, 1], but we want [Batch size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-11bcf26c8a25>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Perform the calculation of the model to determine the prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (12288x64 and 2x4)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving and loading"
      ],
      "metadata": {
        "id": "Sq6zK1yaVXZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# saving a model\n",
        "torch.save(model, \"our_model.tar\")\n",
        "\n",
        "# loading a model\n",
        "model = torch.load(\"our_model.tar\")"
      ],
      "metadata": {
        "id": "OMq56BAvAw2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This stores both the parameters and the structure of the model to a file. This might be a problem if you change the structure of the model at a later point. For this reason, it’s more common to save a model’s state_dict instead.\n",
        "\n",
        "This is a standard Python dict that contains the maps of each layer’s parameters in the model."
      ],
      "metadata": {
        "id": "cRNx1QEHBGwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_dict = model.state_dict()\n",
        "print(state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQ0AXnBXVnRd",
        "outputId": "2b583672-9497-46d6-9936-29c4a6c24d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('linear1.weight', tensor([[-0.6446,  1.0468],\n",
            "        [ 2.6138,  3.0301],\n",
            "        [-1.1551, -1.1321],\n",
            "        [ 0.5989,  0.1297]])), ('linear1.bias', tensor([ 0.6682, -1.1887,  1.7417, -0.1935])), ('linear2.weight', tensor([[-0.7609,  2.1014,  1.8715, -1.0693]])), ('linear2.bias', tensor([-1.1348]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(state_dict(), \"our_model.tar\")"
      ],
      "metadata": {
        "id": "JtajsgJFBNzh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "outputId": "2da85fc6-418f-4586-b3d7-fb0f59496cf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-38e908f8ec65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"our_model.tar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'collections.OrderedDict' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The benefit here is that if you extend the model in some fashion, you can supply a strict=False parameter to load_state_dict that assigns parameters to layers in the model that do exist in the state_dict, but does not fail if the loaded state_dict has layers missing or added from the model’s current structure.\n",
        "\n",
        "Models can be saved to a disk during a training run and reloaded at another point so that training can continue where you left off."
      ],
      "metadata": {
        "id": "iVyDE4-EBkWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load a model from a state dict, we use the function torch.load to load the state dict from the disk, and the module function load_state_dict to overwrite our parameters with the new values:"
      ],
      "metadata": {
        "id": "N6lGLlxJ-kp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load state dict from the disk (make sure it is the same name as above)\n",
        "state_dict = torch.load(\"our_model.tar\")\n",
        "\n",
        "# Create a new model and load the state\n",
        "new_model = SimpleClassifier(num_inputs=2, num_hidden=4, num_outputs=1)\n",
        "new_model.load_state_dict(state_dict)\n",
        "\n",
        "# Verify that the parameters are the same\n",
        "print(\"Original model\\n\", model.state_dict())\n",
        "print(\"\\nLoaded model\\n\", new_model.state_dict())"
      ],
      "metadata": {
        "id": "1D6ow5Ja-ma0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A detailed tutorial can be found at https://pytorch.org/tutorials/beginner/saving_loading_models.html"
      ],
      "metadata": {
        "id": "cuSQDNHm_AZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "s3ht_4gk_J6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have trained a model, it is time to evaluate it on a held-out test set. As our dataset consist of randomly generated data points, we need to first create a test set with a corresponding data loader."
      ],
      "metadata": {
        "id": "_4No5uVM_MZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = XORDataset(size=500)\n",
        "# drop_last -> Don't drop the last batch although it is smaller than 128\n",
        "test_data_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, drop_last=False)"
      ],
      "metadata": {
        "id": "_JBXquYB_QE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When evaluating the model, we don’t need to keep track of the computation graph as we don’t intend to calculate the gradients. This reduces the required memory and speed up the model. In PyTorch, we can deactivate the computation graph using with torch.no_grad(): .... Remember to additionally set the model to eval mode."
      ],
      "metadata": {
        "id": "gZMtyL___XtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, data_loader):\n",
        "    model.eval() # Set model to eval mode\n",
        "    true_preds, num_preds = 0., 0.\n",
        "\n",
        "    with torch.no_grad(): # Deactivate gradients for the following code\n",
        "        for data_inputs, data_labels in data_loader:\n",
        "\n",
        "            # Determine prediction of model on dev set\n",
        "            #data_inputs, data_labels = data_inputs.to(device), data_labels.to(device)\n",
        "            preds = model(data_inputs)\n",
        "            preds = preds.squeeze(dim=1)\n",
        "            preds = torch.sigmoid(preds) # Sigmoid to map predictions between 0 and 1\n",
        "            pred_labels = (preds >= 0.5).long() # Binarize predictions to 0 and 1\n",
        "\n",
        "            # Keep records of predictions for the accuracy metric (true_preds=TP+TN, num_preds=TP+TN+FP+FN)\n",
        "            true_preds += (pred_labels == data_labels).sum()\n",
        "            num_preds += data_labels.shape[0]\n",
        "\n",
        "    acc = true_preds / num_preds\n",
        "    print(f\"Accuracy of the model: {100.0*acc:4.2f}%\")"
      ],
      "metadata": {
        "id": "yAjYPkNt_YRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model(model, test_data_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkmSbmtj_kER",
        "outputId": "8262c3b7-7b08-443f-ac9e-0328f6d85794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model: 99.80%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize what our model has learned, we can perform a prediction for every data point in a range of [-0.5, 1.5], and visualize the predicted class as in the sample figure at the beginning of this section. This shows where the model has created decision boundaries."
      ],
      "metadata": {
        "id": "jKRNxlCw_2wJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.colors import to_rgba\n",
        "\n",
        "@torch.no_grad() # Decorator, same effect as \"with torch.no_grad(): ...\" over the whole function.\n",
        "def visualize_classification(model, data, label):\n",
        "    if isinstance(data, torch.Tensor):\n",
        "        data = data.cpu().numpy()\n",
        "    if isinstance(label, torch.Tensor):\n",
        "        label = label.cpu().numpy()\n",
        "    data_0 = data[label == 0]\n",
        "    data_1 = data[label == 1]\n",
        "\n",
        "    fig = plt.figure(figsize=(4,4), dpi=500)\n",
        "    plt.scatter(data_0[:,0], data_0[:,1], edgecolor=\"#333\", label=\"Class 0\")\n",
        "    plt.scatter(data_1[:,0], data_1[:,1], edgecolor=\"#333\", label=\"Class 1\")\n",
        "    plt.title(\"Dataset samples\")\n",
        "    plt.ylabel(r\"$x_2$\")\n",
        "    plt.xlabel(r\"$x_1$\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Let's make use of a lot of operations we have learned above\n",
        "    model.to(device)\n",
        "    c0 = torch.Tensor(to_rgba(\"C0\")).to(device)\n",
        "    c1 = torch.Tensor(to_rgba(\"C1\")).to(device)\n",
        "    x1 = torch.arange(-0.5, 1.5, step=0.01, device=device)\n",
        "    x2 = torch.arange(-0.5, 1.5, step=0.01, device=device)\n",
        "    xx1, xx2 = torch.meshgrid(x1, x2)  # Meshgrid function as in numpy\n",
        "    model_inputs = torch.stack([xx1, xx2], dim=-1)\n",
        "    preds = model(model_inputs)\n",
        "    preds = torch.sigmoid(preds)\n",
        "    output_image = (1 - preds) * c0[None,None] + preds * c1[None,None]  # Specifying \"None\" in a dimension creates a new one\n",
        "    output_image = output_image.cpu().numpy()  # Convert to numpy array. This only works for tensors on CPU, hence first push to CPU\n",
        "    plt.imshow(output_image, origin='lower', extent=(-0.5, 1.5, -0.5, 1.5))\n",
        "    plt.grid(False)\n",
        "    return fig\n",
        "\n",
        "_ = visualize_classification(model, dataset.data, dataset.label)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Awbad9gFACv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment logging with Tensorboard"
      ],
      "metadata": {
        "id": "kwmbSyOovlfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find official tutorial at https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html"
      ],
      "metadata": {
        "id": "On1x0lA5AvB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TensorBoard is a logging and visualization tool that is a popular choice for training deep learning models. Although initially published for TensorFlow, TensorBoard is also integrated in PyTorch allowing us to easily use it. First, let’s import it below."
      ],
      "metadata": {
        "id": "LNt4sjlBvvh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tensorboard logger from PyTorch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Load tensorboard extension for Jupyter Notebook, only need to start TB in the notebook\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "kR80vE7sXJhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last line is required if you want to run TensorBoard directly in the Jupyter Notebook. Otherwise, you can start TensorBoard from the terminal."
      ],
      "metadata": {
        "id": "n4kQ8pDaXStd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start the logging process by creating a new object, `writer = SummaryWriter(...)`, where we specify the directory in which the logging file should be saved.\n",
        "\n",
        "With this object, we can log different aspects of our model by calling functions of the style `writer.add_...`. \n",
        "\n",
        "For example, we can visualize the computation graph with the function `writer.add_graph`, or add a scalar value like the loss with `writer.add_scalar`.\n",
        "\n",
        "Let’s adapt the training loop with adding a TensorBoard logger below."
      ],
      "metadata": {
        "id": "zwIyRvM5XYc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_with_logger(model, optimizer, data_loader, loss_module, val_dataset, num_epochs=100, logging_dir='runs/our_experiment'):\n",
        "    # Create TensorBoard logger\n",
        "    writer = SummaryWriter(logging_dir)\n",
        "    model_plotted = False\n",
        "\n",
        "    # Set model to train mode\n",
        "    model.train()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        epoch_loss = 0.0\n",
        "        for data_inputs, data_labels in data_loader:\n",
        "\n",
        "            ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n",
        "            data_inputs = data_inputs.to(device)\n",
        "            data_labels = data_labels.to(device)\n",
        "\n",
        "            # For the very first batch, we visualize the computation graph in TensorBoard\n",
        "            if not model_plotted:\n",
        "                writer.add_graph(model, data_inputs)\n",
        "                model_plotted = True\n",
        "\n",
        "            ## Step 2: Run the model on the input data\n",
        "            preds = model(data_inputs)\n",
        "            preds = preds.squeeze(dim=1) # Output is [Batch size, 1], but we want [Batch size]\n",
        "\n",
        "            ## Step 3: Calculate the loss\n",
        "            loss = loss_module(preds, data_labels.float())\n",
        "\n",
        "            ## Step 4: Perform backpropagation\n",
        "            # Before calculating the gradients, we need to ensure that they are all zero.\n",
        "            # The gradients would not be overwritten, but actually added to the existing ones.\n",
        "            optimizer.zero_grad()\n",
        "            # Perform backpropagation\n",
        "            loss.backward()\n",
        "\n",
        "            ## Step 5: Update the parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            ## Step 6: Take the running average of the loss\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # Add average loss to TensorBoard\n",
        "        epoch_loss /= len(data_loader)\n",
        "        writer.add_scalar('training_loss',\n",
        "                          epoch_loss,\n",
        "                          global_step = epoch + 1)\n",
        "\n",
        "        # Visualize prediction and add figure to TensorBoard\n",
        "        # Since matplotlib figures can be slow in rendering, we only do it every 10th epoch\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            fig = visualize_classification(model, val_dataset.data, val_dataset.label)\n",
        "            writer.add_figure('predictions',\n",
        "                              fig,\n",
        "                              global_step = epoch + 1)\n",
        "\n",
        "    writer.close()"
      ],
      "metadata": {
        "id": "16ZO1eeKXtW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s use this method to train a model as before, with a new model and optimizer."
      ],
      "metadata": {
        "id": "kLkwRTaPBBXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleClassifier(num_inputs=2, num_hidden=4, num_outputs=1).to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "train_model_with_logger(model, optimizer, train_data_loader, loss_module, val_dataset=dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d8f8236f31ef404db00cfd9cc33cf5be",
            "43ccd0a4d3b64662892b98c72915601f",
            "e9d7e34efe5d4fa4b7a3089be534d228",
            "2a08d3341ffa4ebe9604a7390ec40cab",
            "89c8d98e9d0546a8a99a640ba0f7d36a",
            "5b109dcd3af546c08c5a6f0c3bb9b982",
            "b5498f5609ac4ae5ab8438e177738522",
            "5edd651cab504470bf6076f12aa24f04",
            "a9b9804e055c42a1b156b00c66ce7617",
            "e0c235fb107d4d98a4d2a42289f6c311",
            "2acda8a104784d149d23ab78a8573123"
          ]
        },
        "id": "i5SiUHzeBCqo",
        "outputId": "7d048369-fa6c-4204-b41d-65db3b6855a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8f8236f31ef404db00cfd9cc33cf5be"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TensorBoard file in the folder runs/our_experiment now contains a loss curve, the computation graph of our network, and a visualization of the learned predictions over number of epochs. To start the TensorBoard visualizer, simply run the following statement:"
      ],
      "metadata": {
        "id": "ux1bAJbUBXpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir runs/our_experiment"
      ],
      "metadata": {
        "id": "qMhLW7HTBbRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Examples - Vanilla PyTorch"
      ],
      "metadata": {
        "id": "GulMyhDsR2rd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP for image classification"
      ],
      "metadata": {
        "id": "pQWENpp-Pn07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we build a simple classifier that can tell the difference between cats and dogs.\n",
        "\n",
        "Download the data from here https://www.kaggle.com/competitions/dogs-vs-cats/data\n",
        "\n",
        "We split the images of both classes into sub-folders for the train, val, and test split. Resulting in 10k training examples per category and 1.25k validation and test examples."
      ],
      "metadata": {
        "id": "S7vTYyNwQcC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykAx5PIno-Vy",
        "outputId": "41651135-2896-4b9d-d718-dacc6e27ce06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp drive/MyDrive/_NN_NLP/PyTorch/datasets/cat_dog.zip . "
      ],
      "metadata": {
        "id": "RvC56Jw6r4Qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip cat_dog.zip"
      ],
      "metadata": {
        "id": "a2NAwVH0Bfnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "XnrU2qvR2yn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exploring the images**\n",
        "\n",
        "TO_DO"
      ],
      "metadata": {
        "id": "EJN0K2EXZlOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Converting raw data into a Dataset**\n",
        "\n",
        "The torchvision package includes a class called ImageFolder that does pretty much everything for us. The default transform is to take image data and turn it into a tensor (the transforms.ToTensor() method). \n",
        "\n",
        "To increase our processing performance, we scale every incoming image to the same resolution of 64 × 64 via the Resize(64) transform. We then convert the images to a tensor, and finally, we normalize the tensor around a specific set of mean and standard deviation points.\n",
        "\n",
        "Keeping the incoming values between 0 and 1 prevents the values from getting too large during the training phase (known as the exploding gradient problem)."
      ],
      "metadata": {
        "id": "BkSI-i4UDEuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating training, validation, and test sets for the image data\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "# see a list of all possible transforms at \n",
        "# https://pytorch.org/vision/stable/transforms.html\n",
        "transforms = transforms.Compose([\n",
        "    transforms.Resize((64, 64)), # here was a bug;\n",
        "    # see https://discuss.pytorch.org/t/runtimeerror-stack-expects-each-tensor-to-be-equal-size-but-got-3-224-224-at-entry-0-and-3-224-336-at-entry-3/87211/10\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225] )\n",
        "    ])\n",
        "\n",
        "train_data_path = \"train\"\n",
        "train_data = torchvision.datasets.ImageFolder(root=train_data_path,\n",
        "                                              transform=transforms)\n",
        "\n",
        "val_data_path = \"val\"\n",
        "val_data = torchvision.datasets.ImageFolder(root=val_data_path,\n",
        "                                            transform=transforms)\n",
        "\n",
        "test_data_path = \"test\"\n",
        "test_data = torchvision.datasets.ImageFolder(root=test_data_path,\n",
        "                                            transform=transforms)"
      ],
      "metadata": {
        "id": "UbY_REywWpWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating DataLoaders\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size=64\n",
        "train_data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_data_loader  = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_data_loader  = DataLoader(test_data, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "o5m7pzZGXtlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Network**\n",
        "\n",
        "The forward() method describes how data flows through the network in both training and making predictions (inference).\n",
        "\n",
        "First, we have to convert the 3D tensor (x (64) * y (64) * three-channels for color information) into a 1D tensor so that it can be fed into the first Linear layer, and we do that using the view()."
      ],
      "metadata": {
        "id": "x9IE-1n5FSY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "import torch.nn as nn\n",
        "\n",
        "# network\n",
        "class SimpleNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(12288, 84) # called Dense in Keras\n",
        "    self.fc2 = nn.Linear(84, 50)\n",
        "    self.fc3 = nn.Linear(50,2)\n",
        "  \n",
        "  def forward(self, x): # bug was here\n",
        "    x = x.view(-1, 12288)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "# initialize the model\n",
        "simplenet = SimpleNet()\n",
        "simplenet.to(device)"
      ],
      "metadata": {
        "id": "1jLgpGbXY3Pv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62df15e0-3207-4740-8ed0-96707a33f2c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleNet(\n",
              "  (fc1): Linear(in_features=12288, out_features=84, bias=True)\n",
              "  (fc2): Linear(in_features=84, out_features=50, bias=True)\n",
              "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From there, you can see that we apply the layers and the activation functions in order, finally returning the softmax output to give us our prediction for that image.\n",
        "\n",
        "The output of the final layer is equal to 2 neurons, matching up with our two classes of cat or fish. In general, you want the data in your layers to be *compressed* as it goes the network.\n",
        "\n",
        "If a layer is going to, say, 50 inputs to 100 outputs, then the network might learn by simply passing the 50 connections to 50 of the 100 outputs and consider its job done.\n",
        "\n",
        "By reducing the size of the output with respect to the input, we force that part of the network to learn a representation of the original input with fewer resources, which hopefully means that it extracts some features of the images that are important to the problem we’re trying to solve.\n",
        "\n"
      ],
      "metadata": {
        "id": "Sq8YcwMAGfNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training loop**"
      ],
      "metadata": {
        "id": "u0ZXOGkdZ1XQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(simplenet.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "Hy1GkTs5Zib7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training loop in a single training function:"
      ],
      "metadata": {
        "id": "1sR0TA_07UAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Progress bar\n",
        "from tqdm.notebook import tqdm\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device=\"cpu\"):\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "    \n",
        "    training_loss = 0.0 # set for the epoch\n",
        "    valid_loss = 0.0\n",
        "    \n",
        "    # start training for the given epoch\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    for inputs, labels in train_loader: # during each batch\n",
        "      \n",
        "      inputs = inputs.to(device) # move to GPU\n",
        "      labels = labels.to(device)\n",
        "      \n",
        "      predictions = model(inputs) # get model predictions for the batch\n",
        "      loss = loss_fn(predictions, labels) # compare model predictions with target labels\n",
        "      \n",
        "      optimizer.zero_grad() # zero the gradient\n",
        "      loss.backward() # compute gradient\n",
        "      optimizer.step() # take an optimization step\n",
        "      \n",
        "      training_loss += loss.item()*inputs.size(0) # accumulating loss over each batch\n",
        "      _, predicted = predictions.max(1)\n",
        "      correct += (predicted == labels).sum() # accumulating TP + TN\n",
        "\n",
        "    # get average training loss for the epoch across all training examples\n",
        "    # by deviding the accumulated loss by the number of all training examples (20k) \n",
        "    training_loss /= len(train_loader.dataset) # len(train_loader) gives number of batches\n",
        "    # calculate training accuracy for all training examples\n",
        "    train_accuracy = correct / len(train_loader.dataset)\n",
        "\n",
        "    # evaluate current model performance once all batches were proccessed\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for inputs, labels in val_loader:\n",
        "      \n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # Deactivate gradients for making a prediction\n",
        "      with torch.no_grad(): # (faster and less memory usage)\n",
        "        predictions = model(inputs)\n",
        "      loss = loss_fn(predictions, labels)\n",
        "      valid_loss += loss.item()*inputs.size(0)\n",
        "        \n",
        "      _, predicted = predictions.max(1)\n",
        "      correct += (predicted == labels).sum()\n",
        "      \n",
        "    # get average validation loss for the epoch across all validation batches\n",
        "    valid_loss /= len(val_loader.dataset)\n",
        "    val_accuracy = correct / len(val_loader.dataset)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1}, train_loss: {training_loss:.2f},\\\n",
        "          val_loss: {valid_loss:.2f}, val_accuracy = {val_accuracy:.2f},\\\n",
        "          train_accuracy = {train_accuracy:.2f}')"
      ],
      "metadata": {
        "id": "aZzASgJTIE0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can start training by calling the function with the required parameters:"
      ],
      "metadata": {
        "id": "oVWI0GjrIshs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# takes about 30 minutes on a CPU\n",
        "train(simplenet, optimizer, torch.nn.CrossEntropyLoss(), train_data_loader, val_data_loader)"
      ],
      "metadata": {
        "id": "g3JJcU2JBk9B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511,
          "referenced_widgets": [
            "0c58cb25edbb4d958826a9a13c96e003",
            "cc1917bf1da04472a28956e3f4db121d",
            "502666c085e34ec1b6b2c83b725c20c4",
            "8a72abd599d244c3a764345dd907545d",
            "6c81461c1e864e62beb634638839fc2f",
            "3ee74950374c439b970ad764f532faf4",
            "84c1e5133d5043de9824db22788fa509",
            "3dded5b149fd4584829746387b6b0d08",
            "f51f0c9db16b4cd38d0fb82cb72ed8b9",
            "a611a7e2b9d847ad827aac12938ca65d",
            "584931f17c7245f9ae2d9c53fc4073a4"
          ]
        },
        "outputId": "b845e76f-ea14-41a8-d3d7-1c44e50abfa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c58cb25edbb4d958826a9a13c96e003"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, train_loss: 0.47,          val_loss: 0.68, val_accuracy = 0.64,          train_accuracy = 0.77\n",
            "Epoch: 1, train_loss: 0.44,          val_loss: 0.72, val_accuracy = 0.65,          train_accuracy = 0.79\n",
            "Epoch: 2, train_loss: 0.41,          val_loss: 0.75, val_accuracy = 0.66,          train_accuracy = 0.81\n",
            "Epoch: 3, train_loss: 0.39,          val_loss: 0.76, val_accuracy = 0.65,          train_accuracy = 0.82\n",
            "Epoch: 4, train_loss: 0.36,          val_loss: 0.83, val_accuracy = 0.64,          train_accuracy = 0.84\n",
            "Epoch: 5, train_loss: 0.33,          val_loss: 0.87, val_accuracy = 0.65,          train_accuracy = 0.85\n",
            "Epoch: 6, train_loss: 0.31,          val_loss: 0.88, val_accuracy = 0.65,          train_accuracy = 0.86\n",
            "Epoch: 7, train_loss: 0.29,          val_loss: 0.97, val_accuracy = 0.65,          train_accuracy = 0.87\n",
            "Epoch: 8, train_loss: 0.27,          val_loss: 1.04, val_accuracy = 0.64,          train_accuracy = 0.88\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-b2aff7165a96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimplenet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-e37d2107b178>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, loss_fn, train_loader, val_loader, epochs, device)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# during each batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# move to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \"\"\"\n\u001b[1;32m    229\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    899\u001b[0m         \"\"\"\n\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"P\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference**\n",
        "\n",
        "We reuse the transform pipeline we made earlier to convert the image into the correct form for our neural network. However, because our network uses batches, it actually expects a 4D tensor, with the first dimension denoting the different images within a batch.\n",
        "\n",
        "We don’t have a batch, but we can create a batch of length 1 by using unsqueeze(0), which adds a new dimension at the front of our tensor.\n",
        "\n",
        "Getting predictions is as simple as passing our batch into the model. We then have to find out the class with the higher probability. \n",
        "\n",
        "PyTorch provides the argmax() function, which returns the index of the highest value of the tensor. We then use that to index into our labels array and print out our prediction."
      ],
      "metadata": {
        "id": "3nAE553aI7ZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "labels = ['cat','dog']\n",
        "\n",
        "# apply same transformations as to the training examples\n",
        "# img = Image.open(\"./test/fish/ninsilver.jpg\")\n",
        "img = Image.open(\"./test/cat/cat.11278.jpg\") \n",
        "img = transforms(img).to(device)\n",
        "img = torch.unsqueeze(img, 0)"
      ],
      "metadata": {
        "id": "OyCB9H4ES6Pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set the model in evaluation mode\n",
        "simplenet.eval()\n",
        "prediction = F.softmax(simplenet(img), dim=1) # don't have to apply softmax first\n",
        "prediction = prediction.argmax()\n",
        "print(labels[prediction]) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-aTtMMTzlUw",
        "outputId": "7dd99685-a129-42db-fcec-2f3c0f873b1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN on images"
      ],
      "metadata": {
        "id": "sGCGjWAYMkkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The network**\n",
        "\n",
        "Here we use the same dataset, but apply a CNN to train on it. Following implementation is inspired by ideas put forward in https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf"
      ],
      "metadata": {
        "id": "L_QuJkukzzdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(CNNNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=11, stride=4, padding=2), # 2D convolutional layer\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(in_channels=64, out_channels=192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(in_channels=192, out_channels=384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2), # max pooling layer\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6)) # average pooling layer\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(), # dropout layer \n",
        "            nn.Linear(in_features=(256 * 6 * 6), out_features=4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(in_features=4096, out_features=4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=4096, out_features=num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1) # flattening out the matrix\n",
        "        x = self.classifier(x) # classifier consists only of fully-connected layers\n",
        "        return x\n",
        "\n",
        "cnnnet = CNNNet()\n",
        "cnnnet.to(device)"
      ],
      "metadata": {
        "id": "k-Q42DbwWDxc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13de6f10-f33a-4336-f6a7-a9f7952dfda3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNNNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU()\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU()\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU()\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, the Dropout layers in our example CNN network are initialized with 0.5, meaning that 50% of the input tensor is randomly zeroed out. If you want to change that to 20%, add the p parameter to the initialization call: Dropout(p=0.2)."
      ],
      "metadata": {
        "id": "yQDDAzLaYU3G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finding optimal learning rate**"
      ],
      "metadata": {
        "id": "FySt3M_4bhfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_lr(model, loss_fn, optimizer, train_loader, init_value=1e-8, final_value=10.0, device=\"cpu\"):\n",
        "    number_in_epoch = len(train_loader) - 1\n",
        "    update_step = (final_value / init_value) ** (1 / number_in_epoch)\n",
        "    lr = init_value\n",
        "    optimizer.param_groups[0][\"lr\"] = lr\n",
        "    best_loss = 0.0\n",
        "    batch_num = 0\n",
        "    losses = []\n",
        "    log_lrs = []\n",
        "    for data in train_loader:\n",
        "        batch_num += 1\n",
        "        inputs, targets = data\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        # Crash out if loss explodes\n",
        "\n",
        "        if batch_num > 1 and loss > 4 * best_loss:\n",
        "            if(len(log_lrs) > 20):\n",
        "                return log_lrs[10:-5], losses[10:-5]\n",
        "            else:\n",
        "                return log_lrs, losses\n",
        "\n",
        "        # Record the best loss\n",
        "\n",
        "        if loss < best_loss or batch_num == 1:\n",
        "            best_loss = loss\n",
        "\n",
        "        # Store the values\n",
        "        losses.append(loss.item())\n",
        "        log_lrs.append((lr))\n",
        "\n",
        "        # Do the backward pass and optimize\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the lr for the next step and store\n",
        "\n",
        "        lr *= update_step\n",
        "        optimizer.param_groups[0][\"lr\"] = lr\n",
        "    if(len(log_lrs) > 20):\n",
        "        return log_lrs[10:-5], losses[10:-5]\n",
        "    else:\n",
        "        return log_lrs, losses"
      ],
      "metadata": {
        "id": "ASOVyHPHzPdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logs,losses = find_lr(audio_net, nn.CrossEntropyLoss(), optimizer, train_loader, device=device)\n",
        "plt.plot(logs,losses)"
      ],
      "metadata": {
        "id": "2jnKRnQ2zWdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "1nxV_0t2aAvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#optimizer = torch.optim.SGD(cnnnet.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer = torch.optim.Adam(cnnnet.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "9UgUG4c4WXOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# takes about 3 hours on a CPU\n",
        "train(cnnnet, optimizer, torch.nn.CrossEntropyLoss(), train_data_loader, val_data_loader)"
      ],
      "metadata": {
        "id": "dN6FQmeZWb6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458,
          "referenced_widgets": [
            "41ca62de845e4a6e97fc18addfb4785b",
            "841ad15f068e4f6aa05b6db04446147d",
            "8ce4b0fdc8c34b57b63f34a1a6425ee8",
            "1434f96a632e411f95efb18a22949dcd",
            "3005a789363445f0b88dccb584a5fa09",
            "9d972a22ac1a42d8af0babf4a753b1cb",
            "9280137bde34443780539267b6c7dddc",
            "3e24b8065302461a823f63ee4e520714",
            "21c9ddc0e696412981bfe9bb5e15bab2",
            "81d9a85df32b4468a410d8bfa28267b1",
            "b9072a5c5ece49ffbe300ba2ae702900"
          ]
        },
        "outputId": "323cd2b4-e8e6-4c89-9c03-7acae315bcae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41ca62de845e4a6e97fc18addfb4785b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, train_loss: 0.69,          val_loss: 0.68, val_accuracy = 0.63,          train_accuracy = 0.53\n",
            "Epoch: 1, train_loss: 0.66,          val_loss: 0.64, val_accuracy = 0.64,          train_accuracy = 0.61\n",
            "Epoch: 2, train_loss: 0.65,          val_loss: 0.66, val_accuracy = 0.63,          train_accuracy = 0.62\n",
            "Epoch: 3, train_loss: 0.62,          val_loss: 0.62, val_accuracy = 0.66,          train_accuracy = 0.66\n",
            "Epoch: 4, train_loss: 0.61,          val_loss: 0.60, val_accuracy = 0.68,          train_accuracy = 0.67\n",
            "Epoch: 5, train_loss: 0.60,          val_loss: 0.59, val_accuracy = 0.69,          train_accuracy = 0.68\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-a7f0c9186033>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnnnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-e37d2107b178>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, loss_fn, train_loader, val_loader, epochs, device)\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m       \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# get model predictions for the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# compare model predictions with target labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-9cbad2e4fc5e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# flattening out the matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# classifier consists only of fully-connected layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature extractors**\n",
        "\n",
        "TO_DO: Following are some tricks to analyze the layers of an image CNN."
      ],
      "metadata": {
        "id": "541a72XHZQMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN on audio spectrograms"
      ],
      "metadata": {
        "id": "TxsHX6fagkGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following is an example of audio classification using custom datasets and k-fold validation."
      ],
      "metadata": {
        "id": "uIQHAlPv9Kgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "HYCCBC8sDnnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Environmental Sound Classification (ESC) dataset is a collection of field recordings, each of which is 5 seconds long and assigned to one of 50 classes (e.g., a dog barking, snoring, a knock on a door). Find more infos at https://github.com/karolpiczak/ESC-50"
      ],
      "metadata": {
        "id": "IjZmIKWi11AJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading and unzipping the ESC data set\n",
        "!wget https://github.com/karoldvl/ESC-50/archive/master.zip\n",
        "!unzip master.zip\n",
        "!ls ./ESC-50-master/meta"
      ],
      "metadata": {
        "id": "rVyFpwkf2Fx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the WAV files are stored in the audio directory with filenames like: `1-100032-A-0.wav`\n",
        "\n",
        "We care about the final number in the filename, because that tells us what class this sound clip has been assigned to.\n",
        "\n"
      ],
      "metadata": {
        "id": "2aOPX8_c2TsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get numbers of examples per sound category\n",
        "import glob\n",
        "from collections import Counter\n",
        "\n",
        "esc50_list = [f.split(\"-\")[-1].replace(\".wav\",\"\") for f in glob.glob(\"ESC-50-master/audio/*.wav\")]\n",
        "Counter(esc50_list)"
      ],
      "metadata": {
        "id": "AZfmByohB70F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a3aa63e-49ac-4167-a18e-f20b1820dff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'45': 40,\n",
              "         '46': 40,\n",
              "         '24': 40,\n",
              "         '1': 40,\n",
              "         '49': 40,\n",
              "         '10': 40,\n",
              "         '25': 40,\n",
              "         '36': 40,\n",
              "         '13': 40,\n",
              "         '33': 40,\n",
              "         '47': 40,\n",
              "         '17': 40,\n",
              "         '18': 40,\n",
              "         '44': 40,\n",
              "         '7': 40,\n",
              "         '22': 40,\n",
              "         '37': 40,\n",
              "         '23': 40,\n",
              "         '20': 40,\n",
              "         '4': 40,\n",
              "         '48': 40,\n",
              "         '19': 40,\n",
              "         '43': 40,\n",
              "         '6': 40,\n",
              "         '0': 40,\n",
              "         '41': 40,\n",
              "         '3': 40,\n",
              "         '42': 40,\n",
              "         '35': 40,\n",
              "         '15': 40,\n",
              "         '29': 40,\n",
              "         '5': 40,\n",
              "         '26': 40,\n",
              "         '34': 40,\n",
              "         '2': 40,\n",
              "         '12': 40,\n",
              "         '28': 40,\n",
              "         '40': 40,\n",
              "         '21': 40,\n",
              "         '27': 40,\n",
              "         '14': 40,\n",
              "         '8': 40,\n",
              "         '11': 40,\n",
              "         '32': 40,\n",
              "         '30': 40,\n",
              "         '31': 40,\n",
              "         '38': 40,\n",
              "         '16': 40,\n",
              "         '39': 40,\n",
              "         '9': 40})"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, we can get this and a lot of additional information from the meta data supplied with the data set:"
      ],
      "metadata": {
        "id": "Of-t5EA9r_FC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load meta data of the training corpus\n",
        "meta_df = pd.read_csv('./ESC-50-master/meta/esc50.csv')\n",
        "#meta_df.head()\n",
        "meta_df.loc[meta_df['category'] == 'clock_alarm']['target']\n",
        "#import IPython; IPython.embed(); exit(1)"
      ],
      "metadata": {
        "id": "ccQrdTzVuSiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a single audio file\n",
        "audio_dir = './ESC-50-master/audio/'\n",
        "file_list = os.listdir(audio_dir)\n",
        "current_file = file_list[0]\n",
        "waveform, sample_rate = torchaudio.load(os.path.join(audio_dir, current_file))\n",
        "print(current_file)\n",
        "\n",
        "# find the category of the current_file\n",
        "#meta_df.loc[meta_df['fold'] == 5]\n",
        "category = meta_df.loc[meta_df['filename'] == current_file]['category'].to_string().split()[1]\n",
        "\n",
        "print(category)"
      ],
      "metadata": {
        "id": "-HGE4EWSCSnd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c6c0f5c-6056-4722-fcf4-7e98d8633ad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5-199284-B-45.wav\n",
            "train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can get some simple info for the audio file and listen to it:"
      ],
      "metadata": {
        "id": "FVrEXoxIpOue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython.display as ipd\n",
        "\n",
        "# get number of channels and number of samples\n",
        "print(\"Shape of waveform: {}\".format(waveform.size()))\n",
        "print(\"Sample rate of waveform: {}\".format(sample_rate))\n",
        "\n",
        "plt.figure()\n",
        "plt.title(category)\n",
        "plt.plot(waveform.t().numpy())\n",
        "\n",
        "\n",
        "# playback interface\n",
        "ipd.Audio(waveform.numpy(), rate=sample_rate)"
      ],
      "metadata": {
        "id": "g8RY6gxXCYxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Custom datasets and audio transformations**\n",
        "\n",
        "Since there is no equivalent functionality of `ImageFolder` for audio files, we have to write our own custom dataset and apply desired transformations to the audio files."
      ],
      "metadata": {
        "id": "xlzQ26Shwa4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although there have been attempts to classify audio based on the raw waveform (e.g., “Very Deep Convolutional Networks For Raw Waveforms\" (https://arxiv.org/abs/1610.00087), in acoustic modeling, mostly we work in the frequency domain which means that we have to convert the audio signal to spectrograms.\n",
        "\n",
        "Let's see first which transforms can be made to an audio file:"
      ],
      "metadata": {
        "id": "_WrGxYDgF5z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert audio files to mono with Torch if required\n",
        "waveform_mono = torch.mean(waveform, dim=0).unsqueeze(0)\n",
        "waveform_mono.size()"
      ],
      "metadata": {
        "id": "IkHcTneDCp3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can compute different types of spectrograms:"
      ],
      "metadata": {
        "id": "TXRkFhpB6y-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute linear and mel-spectrograms\n",
        "# converting from power to loudness (raw spectrum to dB) gives better resolution of the spectrogram\n",
        "# personally I don't see a huge advantage to convert from raw frequency to pitch (linear frequency vs mel-transformed frequency spacing)\n",
        "spectrogram = torchaudio.transforms.Spectrogram()(waveform)\n",
        "spectrogram_mel = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate)(waveform)\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(spectrogram.log2()[0,:,:].numpy()) #cmap='gray'\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(spectrogram_mel.log2()[0,:,:].numpy()) #cmap='gray'\n",
        "\n",
        "# for librosa implementation see https://medium.com/@hasithsura/audio-classification-d37a82d6715\n",
        "\n",
        "# save spectogramms (linear, mel, MFCC) as grey-scale images\n",
        "# (actually, there is no need to save them and then load again)\n",
        "# fig = plt.figure()\n",
        "# plt.imsave(f'/content/ESC-50-spectrogramms/{label_dir}/spec_img{i}.png', spectrogram[0].log2()[0,:,:].numpy(), cmap='gray')"
      ],
      "metadata": {
        "id": "-pPk2uSxCuDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or spectral features:"
      ],
      "metadata": {
        "id": "hdy4NhO_68tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MFCC are obtained by applying DCT to the mel-transformed spectrum\n",
        "# the only advantage I can think of is data compression (check with spectrogram.size())\n",
        "# also it's not entirely clear how those algorithms are implemented\n",
        "# why are they giving different results?\n",
        "# see https://arxiv.org/abs/1709.01922 for empirical results why raw frequency is sufficient\n",
        "\n",
        "# implementation consistent with librosa\n",
        "# applying DCT to the mel-fransformed frequency spectrogramm\n",
        "spectrogram_mfcc = torchaudio.transforms.MFCC(sample_rate=sample_rate)(waveform)\n",
        "\n",
        "plt.figure()\n",
        "fig1 = plt.gcf()\n",
        "plt.imshow(spectrogram_mfcc.log2()[0,:,:].numpy()) #\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(spectrogram_mfcc.log2()[0,:,:].numpy())\n",
        "plt.draw()\n",
        "\n",
        "# implementation consistent with kaldi\n",
        "# gives you DCT slice\n",
        "n_fft = 400.0\n",
        "frame_length = n_fft / sample_rate * 1000.0\n",
        "frame_shift = frame_length / 2.0\n",
        "\n",
        "params = {\n",
        "    \"channel\": 0,\n",
        "    \"dither\": 0.0,\n",
        "    \"window_type\": \"hanning\",\n",
        "    \"frame_length\": frame_length,\n",
        "    \"frame_shift\": frame_shift,\n",
        "    \"remove_dc_offset\": False,\n",
        "    \"round_to_power_of_two\": False,\n",
        "    \"sample_frequency\": sample_rate,\n",
        "}\n",
        "mfcc = torchaudio.compliance.kaldi.mfcc(waveform, **params)\n",
        "plt.figure()\n",
        "plt.plot(mfcc[0,:].numpy())\n",
        "plt.draw()"
      ],
      "metadata": {
        "id": "kYkTF-l0CypP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this, we can write the custom dataset:"
      ],
      "metadata": {
        "id": "CSsM_rxt7JnE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oonkkesJldGT"
      },
      "source": [
        "class dataset(Dataset):\n",
        "\n",
        "  def __init__(self, audio_dir, meta_df, fold_num):\n",
        "    \n",
        "    self.features = []\n",
        "    self.categories = []\n",
        "    self.targets = []\n",
        "\n",
        "    audio_files =  meta_df.loc[meta_df['fold'] == fold_num]['filename'].to_list()\n",
        "\n",
        "    for i, file in tqdm(enumerate(audio_files)):\n",
        "      \n",
        "      waveform, sample_rate = torchaudio.load(os.path.join(audio_dir, file))\n",
        "      # print(sample_rate)\n",
        "      # print(waveform.shape[1]/sample_rate)\n",
        "\n",
        "      # convert to mono if file is in stereo\n",
        "      if waveform.size()[0] > 1:\n",
        "        waveform = torch.mean(waveform, dim=0).unsqueeze(0)\n",
        "      \n",
        "      # resample when sample rate too high\n",
        "      if sample_rate > 22050:\n",
        "        waveform = torchaudio.transforms.Resample(sample_rate, 22050)(waveform)\n",
        "      \n",
        "      # add padding/length normalization\n",
        "      \n",
        "      spectrogram = torchaudio.transforms.Spectrogram()(waveform)\n",
        "      self.features.append(spectrogram)\n",
        "      self.categories.append(meta_df.loc[meta_df['filename'] == file]['category'].to_string().split()[1])\n",
        "      self.targets.append(meta_df.loc[meta_df['filename'] == file]['target'])\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.features)\n",
        "  def __getitem__(self, idx):\n",
        "    return self.features[idx], self.categories[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create datasets and dataloaders for training, validation, and test\n",
        "path_to_ESC50 = ''\n",
        "\n",
        "train_data_path = \"train\"\n",
        "val_data_path = \"val\"\n",
        "test_data_path = \"test\"\n",
        "\n",
        "train_esc50_raw = ESC50raw(os.path.join(path_to_ESC50, train_data_path))\n",
        "valid_esc50_raw = ESC50raw(os.path.join(path_to_ESC50, val_data_path))\n",
        "test_esc50_raw  = ESC50raw(os.path.join(path_to_ESC50, test_data_path))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_esc50, batch_size = 64, shuffle = True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_esc50, batch_size = 64, shuffle = True)\n",
        "test_loader  = torch.utils.data.DataLoader(test_esc50, batch_size = 64, shuffle = True)"
      ],
      "metadata": {
        "id": "Y4kgGc4LE-OM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training loop with k-fold cross-validation**\n",
        "\n",
        "We have to write a different training loop to take advantage of the five folds. More infos on how to implement k-fold validation can be found here https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-k-fold-cross-validation-with-pytorch.md"
      ],
      "metadata": {
        "id": "_XNuvKPNYnqM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPCCRh9ktSSv",
        "outputId": "b7c738ce-f1e9-4b20-c9d0-7ca1eaf7ffc4"
      },
      "source": [
        "# load all folds into a dictionary\n",
        "audio_dir = './ESC-50-master/audio/'\n",
        "meta_df = pd.read_csv('./ESC-50-master/meta/esc50.csv')\n",
        "\n",
        "folds = {}\n",
        "for fold_num in meta_df['fold'].unique().tolist():\n",
        "    folds['fold{}'.format(fold_num)] = dataset(audio_dir, meta_df, fold_num)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400it [00:05, 77.50it/s]\n",
            "400it [00:04, 86.30it/s]\n",
            "400it [00:04, 80.10it/s]\n",
            "400it [00:04, 86.60it/s]\n",
            "400it [00:04, 86.19it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPjT34IgAyHG",
        "outputId": "8f15eb60-ad17-4592-e059-bfbe71b2d40f"
      },
      "source": [
        "list(folds.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fold1', 'fold2', 'fold3', 'fold4', 'fold5']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqS9dg7Z46FA",
        "outputId": "1e0ff217-f202-4669-8dfd-8136e88f002a"
      },
      "source": [
        "# training loop\n",
        "\n",
        "# configuration options\n",
        "num_epochs = 1\n",
        "batch_size = 16\n",
        "learning_rate = 1e-4\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# aggregate fold results\n",
        "results = {}\n",
        "\n",
        "# set fixed random number seed\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# re-compile list of all folds\n",
        "fold_list = list(folds.keys())\n",
        "  \n",
        "# Start print\n",
        "print('--------------------------------')\n",
        "\n",
        "# K-fold cross validation model evaluation\n",
        "for i, fold in enumerate(fold_list):\n",
        "  \n",
        "  print(f'FOLD {i+1}')\n",
        "  print('--------------------------------')\n",
        "\n",
        "  # concatinate training set\n",
        "  fold_list.remove(fold)\n",
        "  train_datasets = []\n",
        "  for train_fold in fold_list:\n",
        "    train_datasets.append(folds[train_fold])\n",
        "  train_data = ConcatDataset(train_datasets)\n",
        "\n",
        "  # test set\n",
        "  test_data = folds[fold]\n",
        "\n",
        "  # data loaders for training and testing data in this fold\n",
        "  train_loader = DataLoader(train_data, batch_size=batch_size, num_workers=4, shuffle=True)\n",
        "  test_loader = DataLoader(test_data, batch_size=batch_size, num_workers=4, shuffle=True)\n",
        "  # pin_memory=True, drop_last=True)\n",
        "  \n",
        "  # init the neural network\n",
        "  network = SimpleConvNet()\n",
        "  network.apply(reset_weights)\n",
        "  \n",
        "  # initialize optimizer\n",
        "  optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n",
        "  \n",
        "  # training loop for defined number of epochs\n",
        "  for epoch in range(0, num_epochs):\n",
        "\n",
        "    print(f'Starting epoch {epoch+1}')\n",
        "\n",
        "    # set current loss value\n",
        "    current_loss = 0.0\n",
        "\n",
        "    # iterate over the DataLoader for training data\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "      \n",
        "      inputs, targets = data\n",
        "      \n",
        "      # zero the gradients\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      # Perform forward pass\n",
        "      outputs = network(inputs)\n",
        "      \n",
        "      # Compute loss\n",
        "      import IPython; IPython.embed(); exit(1)\n",
        "      loss = loss_function(outputs, targets)\n",
        "\n",
        "      \n",
        "      # Perform backward pass\n",
        "      loss.backward()\n",
        "      \n",
        "      # Perform optimization\n",
        "      optimizer.step()\n",
        "      \n",
        "      # Print statistics\n",
        "      current_loss += loss.item()\n",
        "      if i % 500 == 499:\n",
        "          print('Loss after mini-batch %5d: %.3f' %\n",
        "                (i + 1, current_loss / 500))\n",
        "          current_loss = 0.0\n",
        "          \n",
        "  # Process is complete.\n",
        "  print('Training process has finished. Saving trained model.')\n",
        "\n",
        "  # Print about testing\n",
        "  print('Starting testing')\n",
        "  \n",
        "  # Saving the model\n",
        "  save_path = f'./model-fold-{fold}.pth'\n",
        "  torch.save(network.state_dict(), save_path)\n",
        "\n",
        "  # Evaluationfor this fold\n",
        "  correct, total = 0, 0\n",
        "  with torch.no_grad():\n",
        "\n",
        "    # Iterate over the test data and generate predictions\n",
        "    for i, data in enumerate(test_loader, 0):\n",
        "\n",
        "      # Get inputs\n",
        "      inputs, targets = data\n",
        "\n",
        "      # Generate outputs\n",
        "      outputs = network(inputs)\n",
        "\n",
        "      # Set total and correct\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += targets.size(0)\n",
        "      correct += (predicted == targets).sum().item()\n",
        "\n",
        "    # Print accuracy\n",
        "    print('Accuracy for fold %d: %d %%' % (fold, 100.0 * correct / total))\n",
        "    print('--------------------------------')\n",
        "    results[fold] = 100.0 * (correct / total)\n",
        "  \n",
        "# Print fold results\n",
        "print(f'K-FOLD CROSS VALIDATION RESULTS FOR {len(folds.keys())} FOLDS')\n",
        "print('--------------------------------')\n",
        "sum = 0.0\n",
        "for key, value in results.items():\n",
        "  print(f'Fold {key}: {value} %')\n",
        "  sum += value\n",
        "print(f'Average: {sum/len(results.items())} %')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------\n",
            "FOLD 1\n",
            "--------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Reset trainable parameters of layer = Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
            "Reset trainable parameters of layer = Linear(in_features=1094500, out_features=50, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=50, out_features=20, bias=True)\n",
            "Reset trainable parameters of layer = Linear(in_features=20, out_features=10, bias=True)\n",
            "Starting epoch 1\n",
            "Python 3.7.10 (default, May  3 2021, 02:48:31) \n",
            "Type \"copyright\", \"credits\" or \"license\" for more information.\n",
            "\n",
            "IPython 5.5.0 -- An enhanced Interactive Python.\n",
            "?         -> Introduction and overview of IPython's features.\n",
            "%quickref -> Quick reference.\n",
            "help      -> Python's own help system.\n",
            "object?   -> Details about 'object', use 'object??' for extra details.\n",
            "\n",
            "In [1]: targets\n",
            "Out[1]: \n",
            "('crying_baby',\n",
            " 'hand_saw',\n",
            " 'airplane',\n",
            " 'siren',\n",
            " 'sheep',\n",
            " 'clock_tick',\n",
            " 'wind',\n",
            " 'sneezing',\n",
            " 'helicopter',\n",
            " 'pig',\n",
            " 'snoring',\n",
            " 'church_bells',\n",
            " 'frog',\n",
            " 'door_wood_creaks',\n",
            " 'siren',\n",
            " 'dog')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2jLYtDKmt0g"
      },
      "source": [
        "def reset_weights(m):\n",
        "  '''\n",
        "    Resetting model weights to avoid\n",
        "    weight leakage across different folds.\n",
        "  '''\n",
        "  for layer in m.children():\n",
        "   if hasattr(layer, 'reset_parameters'):\n",
        "    print(f'Reset trainable parameters of layer = {layer}')\n",
        "    layer.reset_parameters()\n",
        "\n",
        "class SimpleConvNet(nn.Module):\n",
        "  '''\n",
        "    Simple Convolutional Neural Network\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Conv2d(1, 10, kernel_size=3),\n",
        "      nn.ReLU(),\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(1094500, 50),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(50, 20),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(20, 10)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''Forward pass'''\n",
        "    return self.layers(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTMs on speech"
      ],
      "metadata": {
        "id": "DCDUjPHDVloC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Example: https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch/"
      ],
      "metadata": {
        "id": "qZV_5txmRStW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-trained models from PyTorch Hub"
      ],
      "metadata": {
        "id": "9GoLcs2rRiFt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen from examples, code written in vanilla PyTorch gets messy very quickly. Also, a lot of overhead time is spent with massaging the raw data into a right format to pass it to the dataset class, and debugging errors related to typos in paths, module incompatibilities, or slightly wrong object dimensions of training examples.\n",
        "\n",
        "In practice it is therefore advisable to use PyTorch Lightning for code organization and PyTorch or Hugging Face Hub for getting already pre-packed datasets, or for that matter, pre-trained models.\n",
        "\n",
        "It's true that at some point you have to be comfortable with compiling your own datasets. However, for 95 percent of applications you can get away using off-the-shelf-solutions."
      ],
      "metadata": {
        "id": "CA4v5Uh6JXVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use specific libraries, e.g., `the torchvision.models`, to load some models:"
      ],
      "metadata": {
        "id": "tRJa3wi8NSC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "alexnet = models.alexnet(num_classes=1000, pretrained=True)"
      ],
      "metadata": {
        "id": "1wSFEsCdNdaO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158,
          "referenced_widgets": [
            "e312058d534143f88ad373782dfcf8c0",
            "8b5c1d388fe247e586f8ea7e6ac1acc7",
            "268b70eb9a0d4b9585629ea098b2bccd",
            "bea98cd8f34f46dfa3f05bded41f47c7",
            "890f7094f8f14175b41a7839a2652266",
            "8aac15fea6b3450593f3de0566ce17f7",
            "5c976a6f49b04215a1026a5eb5431869",
            "895135a9ecb7484991c33ae32e7d9433",
            "38e29e1e5a494e61a18d9db76dc32a83",
            "a5d4ee258443403a9f3260641581aab5",
            "c9d76c976755483bac7349c7e02cc124"
          ]
        },
        "outputId": "e7426467-cf8f-475a-a6c6-d256b80c4961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/233M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e312058d534143f88ad373782dfcf8c0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, it's easier to use the Hub (see the model list at https://pytorch.org/hub/research-models):"
      ],
      "metadata": {
        "id": "ZDtBGqzpNgFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "resnet50 = torch.hub.load('pytorch/vision', 'resnet50')\n",
        "print(resnet50)"
      ],
      "metadata": {
        "id": "AZjimZYgM1xf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f6596be-2c92-434e-9f0c-152054097053"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "/root/.cache/torch/hub/pytorch_vision_main/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: \n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -R /root/.cache/torch/hub/pytorch_vision_main"
      ],
      "metadata": {
        "id": "Ppz5OIppHVxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Lightning"
      ],
      "metadata": {
        "id": "2CaTppULPKze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://m.youtube.com/watch?v=Hgg8Xy6IRig"
      ],
      "metadata": {
        "id": "IFZjMmoWzTrh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch Lightning is a wrapper library which allows to re-pack the training loop inside a single module. This doesn't really makes you write less code, but you get out a better structured code.\n",
        "\n",
        "For a gentle introduction see https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09\n",
        "\n",
        "Altough, the author suggests to use LightningModule for model and update fuctions, LightningDataModule for DataLoaders, and the Lightning's trainer function to train the model, there are examples where DataLoaders are put inside the LightningModule, which makes it kind of more slick. See an example below in the section on Hugging Face.\n",
        "\n",
        "Lightning also handles logging into TensorBoard, and saving model checkpoints automatically with minimal code overhead from our side."
      ],
      "metadata": {
        "id": "dXdOBhDvPmXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError:\n",
        "    # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    !pip install --quiet pytorch-lightning\n",
        "    import pytorch_lightning as pl"
      ],
      "metadata": {
        "id": "O8-gU-yqkD5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the seed\n",
        "pl.seed_everything(42)"
      ],
      "metadata": {
        "id": "5we3Su0wkUmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch Lightning, we define `pl.LightningModule`s (inheriting from `torch.nn.Module`) that organize our code into 5 main sections:\n",
        "\n",
        "- Initialization (__init__), where we create all necessary parameters/models\n",
        "\n",
        "- Optimizers (configure_optimizers) where we create the optimizers, learning rate scheduler, etc.\n",
        "\n",
        "- Training loop (training_step) where we only have to define the loss calculation for a single batch (the loop of optimizer.zero_grad(), loss.backward() and optimizer.step(), as well as any logging/saving operation, is done in the background)\n",
        "\n",
        "- Validation loop (validation_step) where similarly to the training, we only have to define what should happen per step\n",
        "\n",
        "- Test loop (test_step) which is the same as validation, only on a test set."
      ],
      "metadata": {
        "id": "YTCJCZjglDvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFARModule(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, model_name, model_hparams, optimizer_name, optimizer_hparams):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            model_name - Name of the model/CNN to run. Used for creating the model (see function below)\n",
        "            model_hparams - Hyperparameters for the model, as dictionary.\n",
        "            optimizer_name - Name of the optimizer to use. Currently supported: Adam, SGD\n",
        "            optimizer_hparams - Hyperparameters for the optimizer, as dictionary. This includes learning rate, weight decay, etc.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Exports the hyperparameters to a YAML file, and create \"self.hparams\" namespace\n",
        "        self.save_hyperparameters()\n",
        "        # Create model\n",
        "        self.model = create_model(model_name, model_hparams)\n",
        "        # Create loss module\n",
        "        self.loss_module = nn.CrossEntropyLoss()\n",
        "        # Example input for visualizing the graph in Tensorboard\n",
        "        self.example_input_array = torch.zeros((1, 3, 32, 32), dtype=torch.float32)\n",
        "\n",
        "    def forward(self, imgs):\n",
        "        # Forward function that is run when visualizing the graph\n",
        "        return self.model(imgs)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # We will support Adam or SGD as optimizers.\n",
        "        if self.hparams.optimizer_name == \"Adam\":\n",
        "            # AdamW is Adam with a correct implementation of weight decay (see here for details: https://arxiv.org/pdf/1711.05101.pdf)\n",
        "            optimizer = optim.AdamW(\n",
        "                self.parameters(), **self.hparams.optimizer_hparams)\n",
        "        elif self.hparams.optimizer_name == \"SGD\":\n",
        "            optimizer = optim.SGD(self.parameters(), **self.hparams.optimizer_hparams)\n",
        "        else:\n",
        "            assert False, f\"Unknown optimizer: \\\"{self.hparams.optimizer_name}\\\"\"\n",
        "\n",
        "        # We will reduce the learning rate by 0.1 after 100 and 150 epochs\n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(\n",
        "            optimizer, milestones=[100, 150], gamma=0.1)\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # \"batch\" is the output of the training data loader.\n",
        "        imgs, labels = batch\n",
        "        preds = self.model(imgs)\n",
        "        loss = self.loss_module(preds, labels)\n",
        "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
        "\n",
        "        # Logs the accuracy per epoch to tensorboard (weighted average over batches)\n",
        "        self.log('train_acc', acc, on_step=False, on_epoch=True)\n",
        "        self.log('train_loss', loss)\n",
        "        return loss  # Return tensor to call \".backward\" on\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        imgs, labels = batch\n",
        "        preds = self.model(imgs).argmax(dim=-1)\n",
        "        acc = (labels == preds).float().mean()\n",
        "        # By default logs it per epoch (weighted average over batches)\n",
        "        self.log('val_acc', acc)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        imgs, labels = batch\n",
        "        preds = self.model(imgs).argmax(dim=-1)\n",
        "        acc = (labels == preds).float().mean()\n",
        "        # By default logs it per epoch (weighted average over batches), and returns it afterwards\n",
        "        self.log('test_acc', acc)"
      ],
      "metadata": {
        "id": "MSEKfEsQljgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another important part of PyTorch Lightning is the concept of callbacks. Callbacks are self-contained functions that contain the non-essential logic of your Lightning Module. They are usually called after finishing a training epoch, but can also influence other parts of your training loop.\n",
        "\n",
        "For instance, we will use the following two pre-defined callbacks: `LearningRateMonitor` and `ModelCheckpoint`. The learning rate monitor adds the current learning rate to our TensorBoard, which helps to verify that our learning rate scheduler works correctly.\n",
        "\n",
        "The model checkpoint callback allows you to customize the saving routine of your checkpoints. For instance, how many checkpoints to keep, when to save, which metric to look out for, etc. We import them below:"
      ],
      "metadata": {
        "id": "luliO7-pl3LK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint"
      ],
      "metadata": {
        "id": "FCREMi01mD9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To allow running multiple different models with the same Lightning module, we define a function below that maps a model name to the model class. At this stage, the dictionary `model_dict` is empty, but we will fill it throughout the notebook with our new models."
      ],
      "metadata": {
        "id": "Xd_3BkWpnVfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_dict = {}\n",
        "\n",
        "def create_model(model_name, model_hparams):\n",
        "    if model_name in model_dict:\n",
        "        return model_dict[model_name](**model_hparams)\n",
        "    else:\n",
        "        assert False, f\"Unknown model name \\\"{model_name}\\\". Available models are: {str(model_dict.keys())}\""
      ],
      "metadata": {
        "id": "-rbGU4Wynb5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, to use the activation function as another hyperparameter in our model, we define a “name to function” dict below:"
      ],
      "metadata": {
        "id": "Bh4D91WgniCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "act_fn_by_name = {\n",
        "    \"tanh\": nn.Tanh,\n",
        "    \"relu\": nn.ReLU,\n",
        "    \"leakyrelu\": nn.LeakyReLU,\n",
        "    \"gelu\": nn.GELU\n",
        "}"
      ],
      "metadata": {
        "id": "7PVID_bQnkwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we pass the classes or objects directly as an argument to the Lightning module, we couldn’t take advantage of PyTorch Lightning’s automatically hyperparameter saving and loading.\n",
        "\n",
        "Besides the `Lightning module`, the second most important module in PyTorch Lightning is the `Trainer`. The trainer is responsible to execute the training steps defined in the Lightning module and completes the framework. Similar to the Lightning module, you can override any key part that you don’t want to be automated, but the default settings are often the best practice to do. The most important functions we use below are:\n",
        "\n",
        "- `trainer.fit`: Takes as input a lightning module, a training dataset, and an (optional) validation dataset. This function trains the given module on the training dataset with occasional validation (default once per epoch, can be changed)\n",
        "\n",
        "- `trainer.test`: Takes as input a model and a dataset on which we want to test. It returns the test metric on the dataset.\n",
        "\n",
        "For training and testing, we don’t have to worry about things like setting the model to eval mode (model.eval()) as this is all done automatically. See below how we define a training function for our models:"
      ],
      "metadata": {
        "id": "10VPoNNKn3FA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model_name, save_name=None, **kwargs):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        model_name - Name of the model you want to run. Is used to look up the class in \"model_dict\"\n",
        "        save_name (optional) - If specified, this name will be used for creating the checkpoint and logging directory.\n",
        "    \"\"\"\n",
        "    if save_name is None:\n",
        "        save_name = model_name\n",
        "\n",
        "    # Create a PyTorch Lightning trainer with the generation callback\n",
        "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, save_name),                          # Where to save models\n",
        "                         gpus=1 if str(device)==\"cuda:0\" else 0,                                             # We run on a single GPU (if possible)\n",
        "                         max_epochs=180,                                                                     # How many epochs to train for if no patience is set\n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),  # Save the best checkpoint based on the maximum val_acc recorded. Saves only weights and not optimizer\n",
        "                                    LearningRateMonitor(\"epoch\")],                                           # Log learning rate every epoch\n",
        "                         enable_progress_bar=True)                                                           # Set to False if you do not want a progress bar\n",
        "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
        "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
        "\n",
        "    # Check whether pretrained model exists. If yes, load it and skip training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, save_name + \".ckpt\")\n",
        "    if os.path.isfile(pretrained_filename):\n",
        "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
        "        model = CIFARModule.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n",
        "    else:\n",
        "        pl.seed_everything(42) # To be reproducable\n",
        "        model = CIFARModule(model_name=model_name, **kwargs)\n",
        "        trainer.fit(model, train_loader, val_loader)\n",
        "        model = CIFARModule.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
        "\n",
        "    # Test best model on validation and test set\n",
        "    val_result = trainer.test(model, val_loader, verbose=False)\n",
        "    test_result = trainer.test(model, test_loader, verbose=False)\n",
        "    result = {\"test\": test_result[0][\"test_acc\"], \"val\": val_result[0][\"test_acc\"]}\n",
        "\n",
        "    return model, result"
      ],
      "metadata": {
        "id": "s-Q-FfUwoTwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Examples - PyTorch Lightning"
      ],
      "metadata": {
        "id": "GDnLGzAzvS7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet (2015, Microsoft)\n",
        "\n",
        "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial5/Inception_ResNet_DenseNet.html"
      ],
      "metadata": {
        "id": "T9AGSWuRvbD1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformers\n",
        "\n",
        "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html"
      ],
      "metadata": {
        "id": "F9SsTkqFSmuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Autoencoders\n",
        "\n",
        "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial9/AE_CIFAR10.html"
      ],
      "metadata": {
        "id": "Bjk1lWSzTByK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face with PyTorch\n",
        "\n"
      ],
      "metadata": {
        "id": "1NI78k428mml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging Face transformers is a Python library which allows to use pre-trained large language models and fine-tune them on your own data set using its Trainer API (see https://huggingface.co/course/chapter3/1?fw=pt).\n",
        "\n",
        "Following this approach, tuning/training a PyTorch model becomes as easy as using Keras' model.fit(). See the following example:"
      ],
      "metadata": {
        "id": "tM-eclpnSOC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning\n",
        "!pip install tensorboard\n",
        "!pip install nlp\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "Ko2w1CY_-gvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "import nlp\n",
        "import transformers"
      ],
      "metadata": {
        "id": "tHSmBOV4-uMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBSentimentClassifier(pl.LightningModule):\n",
        "    \n",
        "    # initilize the model and model loss\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # load a re-trained BERT model from HF transfomers\n",
        "        self.model = transformers.BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "        # cross-entropy loss from PyTorch\n",
        "        self.loss = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    def prepare_data(self):\n",
        "        \n",
        "        # load BERT tokenizer from HF transformers\n",
        "        tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # convert the text field to token ids and add to the data set items\n",
        "        # text sequences also get normalized here\n",
        "        def _tokenize(x):\n",
        "            x['token_ids'] = tokenizer.batch_encode_plus(\n",
        "                    x['text'], \n",
        "                    max_length=32,\n",
        "                    truncation=True, \n",
        "                    padding=True)\n",
        "            return x\n",
        "\n",
        "        # load IMDB data set from HF nlp and split it\n",
        "        def _prepare_ds(split):\n",
        "            ds = nlp.load_dataset('imdb', split='train[:10%]')\n",
        "            ds = ds.map(_tokenize, batched=True)\n",
        "            ds.set_format(type='torch', columns=['token_ids', 'label'])\n",
        "            return ds\n",
        "\n",
        "        self.train_ds, self.test_ds = map(_prepare_ds, ('train', 'test'))\n",
        "\n",
        "    def forward(self, token_ids):\n",
        "        mask = (token_ids != 0).float()\n",
        "        logits, = self.model(token_ids, mask)\n",
        "        return logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        logits = self.forward(batch['token_ids'])\n",
        "        loss = self.loss(logits, batch['label']).mean()\n",
        "        return {'loss': loss, 'log': {'train_loss': loss}}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        logits = self.forward(batch['token_ids'])\n",
        "        loss = self.loss(logits, batch['label'])\n",
        "        acc = (logits.argmax(-1) == batch['label']).float()\n",
        "        return {'loss': loss, 'acc': acc}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        loss = torch.cat([o['loss'] for o in outputs], 0).mean()\n",
        "        acc = torch.cat([o['acc'] for o in outputs], 0).mean()\n",
        "        out = {'val_loss': loss, 'val_acc': acc}\n",
        "        return {**out, 'log': out}\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "                self.train_ds,\n",
        "                batch_size=8,\n",
        "                drop_last=True,\n",
        "                shuffle=True,\n",
        "                )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "                self.test_ds,\n",
        "                batch_size=8,\n",
        "                drop_last=False,\n",
        "                shuffle=True,\n",
        "                )\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.SGD(\n",
        "            self.parameters(),\n",
        "            lr=1e-2,\n",
        "            momentum=0.9,\n",
        "        )\n",
        "    \n",
        "def main(_):\n",
        "    model = IMDBSentimentClassifier()\n",
        "    trainer = pl.Trainer(\n",
        "        default_root_dir='root/logs',\n",
        "        gpus=(1 if torch.cuda.is_available() else 0),\n",
        "        max_epochs=10,\n",
        "        logger=pl.loggers.TensorBoardLogger('root/logs/', name='imdb', version=0),\n",
        "    )\n",
        "    trainer.fit(model)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main(_)"
      ],
      "metadata": {
        "id": "3HMXSEXd-xp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, in case you need any additional customizations for your training you can still utilize all of the underlying PyTorch functionality and implement your own training loop (see https://huggingface.co/course/chapter3/4?fw=pt)."
      ],
      "metadata": {
        "id": "rKMrIKe9_tA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using AWS Sagemaker"
      ],
      "metadata": {
        "id": "WhNvJaLMwqzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- https://julsimon.medium.com/training-with-pytorch-on-amazon-sagemaker-58fca8c69987\n",
        "- https://docs.aws.amazon.com/sagemaker/latest/dg/pytorch.html"
      ],
      "metadata": {
        "id": "kFCzJc1Gwurl"
      }
    }
  ]
}